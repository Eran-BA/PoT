# PoH Decision Flowchart: Should You Use It?

**Quick decision tree to determine if PoH is right for your NLP task.**

---

## ğŸ¯ The 5-Question Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Does your task involve POINTERS or     â”‚
â”‚  STRUCTURED OUTPUT?                     â”‚
â”‚  (e.g., each token points to another)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   YES   â”‚ (continue)
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
              v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Is baseline accuracy < 90%?            â”‚
â”‚  (i.e., is this a HARD task?)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   YES   â”‚ (continue)
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
              v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Does the task require LONG-RANGE       â”‚
â”‚  DEPENDENCIES (>10 tokens)?             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   YES   â”‚ (continue)
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
              v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Would ITERATIVE REFINEMENT help?       â”‚
â”‚  (decisions depend on other decisions)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   YES   â”‚ (continue)
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
              v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Can you afford 8-12Ã— compute?          â”‚
â”‚  (iterations, not real-time)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â”‚   YES   â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
              v
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  âœ… USE PoH!   â”‚
     â”‚  Expected:     â”‚
     â”‚  +10-30% gain  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ANY "NO" ANSWER?
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âŒ USE BASELINE    â”‚
â”‚  PoH won't help     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Task-Specific Quick Reference

### Dependency Parsing
- **Pointers?** âœ… Yes (each word â†’ head)
- **Hard?** âœ… Yes (~80-92% UAS)
- **Long-range?** âœ… Yes (cross-sentence)
- **Iterative?** âœ… Yes (attachment ambiguity)
- **Compute OK?** âœ… Yes (offline parsing)
- **VERDICT:** ğŸ† **USE PoH** (+15-30%)

### Coreference Resolution
- **Pointers?** âœ… Yes (mention â†’ antecedent)
- **Hard?** âœ… Yes (~60-75% F1)
- **Long-range?** âœ… Yes (100+ tokens)
- **Iterative?** âœ… Yes (chain reasoning)
- **Compute OK?** âœ… Yes (document-level)
- **VERDICT:** ğŸ† **USE PoH** (+20-40%)

### Semantic Role Labeling
- **Pointers?** âœ… Yes (argument â†’ predicate)
- **Hard?** âœ… Yes (~75-85% F1)
- **Long-range?** âœ… Yes (long-distance args)
- **Iterative?** âœ… Yes (frame disambiguation)
- **Compute OK?** âœ… Yes (offline)
- **VERDICT:** ğŸ† **USE PoH** (+10-25%)

### Relation Extraction
- **Pointers?** âœ… Yes (entity pairs)
- **Hard?** âœ… Yes (~50-65% F1)
- **Long-range?** âœ… Yes (document-level)
- **Iterative?** âœ… Yes (multi-relation)
- **Compute OK?** âœ… Yes (batch)
- **VERDICT:** ğŸ† **USE PoH** (+15-30%)

### Multi-Hop QA
- **Pointers?** âš ï¸ Partial (passage â†’ passage)
- **Hard?** âœ… Yes (~55-70% EM)
- **Long-range?** âœ… Yes (multi-passage)
- **Iterative?** âœ… Yes (hop-by-hop)
- **Compute OK?** âœ… Yes (QA systems)
- **VERDICT:** âœ… **USE PoH** (+8-15%)

### Named Entity Recognition
- **Pointers?** âŒ No (sequence tagging)
- **Hard?** âŒ No (~92-95% F1)
- **Long-range?** âŒ No (local context)
- **Iterative?** âš ï¸ Rare (nested only)
- **Compute OK?** âš ï¸ Maybe
- **VERDICT:** âš ï¸ **BASELINE** (PoH for nested NER only)

### Sentiment Analysis
- **Pointers?** âŒ No (classification)
- **Hard?** âŒ No (~90-95% acc)
- **Long-range?** âŒ No (pooling works)
- **Iterative?** âŒ No (single decision)
- **Compute OK?** âŒ No (real-time)
- **VERDICT:** âŒ **BASELINE** (PoH is overkill)

### Machine Translation
- **Pointers?** âŒ No (generation)
- **Hard?** âš ï¸ Depends (language pair)
- **Long-range?** âœ… Yes (long sentences)
- **Iterative?** âš ï¸ Partial (decoder layers)
- **Compute OK?** âš ï¸ Maybe (inference time)
- **VERDICT:** âš ï¸ **BASELINE** (PoH for encoder only, low-resource)

### Text Classification
- **Pointers?** âŒ No (label output)
- **Hard?** âŒ No (~85-95% acc)
- **Long-range?** âš ï¸ Sometimes (long docs)
- **Iterative?** âŒ No (pooling sufficient)
- **Compute OK?** âŒ No (high throughput)
- **VERDICT:** âŒ **BASELINE** (PoH offers nothing)

---

## ğŸ¨ Visual Task Map

```
                    STRUCTURED OUTPUT
                            â†‘
                            â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  PoH    â”‚ Baselineâ”‚
                  â”‚  ZONE   â”‚  ZONE   â”‚
           HIGH   â”‚    ğŸ†   â”‚    âš ï¸   â”‚
        DIFFICULTYâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚    âš ï¸   â”‚    âŒ   â”‚
           LOW    â”‚Marginal â”‚ Overkillâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TASK PLACEMENT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ† PoH WINS (Top-Right Quadrant)      â”‚
â”‚ â€¢ Dependency Parsing                   â”‚
â”‚ â€¢ Coreference Resolution               â”‚
â”‚ â€¢ Semantic Role Labeling               â”‚
â”‚ â€¢ Relation Extraction                  â”‚
â”‚ â€¢ Multi-Hop QA                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ MARGINAL (Left Quadrants)          â”‚
â”‚ â€¢ NER (nested only)                    â”‚
â”‚ â€¢ Constituency Parsing                 â”‚
â”‚ â€¢ Event Extraction                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ BASELINE BETTER (Bottom Quadrants)  â”‚
â”‚ â€¢ Sentiment Analysis                   â”‚
â”‚ â€¢ Text Classification                  â”‚
â”‚ â€¢ Standard NER                         â”‚
â”‚ â€¢ Paraphrase Detection                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Deep Dive: When Iterative Refinement Helps

### âœ… HELPS (Use PoH):

**1. Ambiguity Resolution**
```
"I saw the man with the telescope."
Iteration 1: telescope â†’ saw? man? (ambiguous)
Iteration 2: Consider "with" semantics
Iteration 3: Resolve based on verb frame
```

**2. Constraint Satisfaction**
```
Dependency parsing: Must form a tree
Iteration 1: Local attachments (may have cycles)
Iteration 2: Fix cycles
Iteration 3: Ensure single root
```

**3. Multi-Hop Reasoning**
```
Q: "Where was Obama's wife born?"
Hop 1: Obama â†’ wife â†’ Michelle Obama
Hop 2: Michelle Obama â†’ birthplace â†’ Chicago
```

**4. Transitive Dependencies**
```
Coreference: Alice ... she ... her ... the woman
Chain must be consistent transitively
```

### âŒ DOESN'T HELP (Use Baseline):

**1. Independent Decisions**
```
Sentiment: Each word has sentiment score
No cross-token dependencies â†’ no iteration benefit
```

**2. Local Context**
```
POS Tagging: Mostly trigram features
Longer context rarely helps
```

**3. Single Global Decision**
```
Topic Classification: Entire document â†’ 1 label
Pooling + linear is optimal
```

---

## ğŸ’° Cost-Benefit Analysis

### When PoH is Worth the Cost:

| Scenario | Compute Cost | Accuracy Gain | ROI | Decision |
|----------|-------------|---------------|-----|----------|
| Hard task, high value | 12Ã— | +20% | 1.67% per iter | âœ… Worth it |
| Medium task, offline | 8Ã— | +10% | 1.25% per iter | âœ… Worth it |
| Easy task, research | 4Ã— | +5% | 1.25% per iter | âš ï¸ Marginal |
| Any task, real-time | 12Ã— | +15% | N/A | âŒ Too slow |
| Easy task, production | 8Ã— | +2% | 0.25% per iter | âŒ Not worth it |

**Rule of Thumb:** 
- ROI > 1.0% per iteration â†’ Use PoH
- ROI < 0.5% per iteration â†’ Use baseline
- Real-time constraints â†’ Use baseline

---

## ğŸš€ Recommended First Experiments

If you want to prove PoH value, run these in order:

### 1. **Coreference on GAP** (Highest Expected Gain)
- **Dataset:** Google's Gender-Ambiguous Pronouns
- **Baseline:** ~65-70% accuracy
- **PoH Expected:** ~75-85% accuracy (+10-20%)
- **Why:** Hardest pronoun resolution task, needs multi-hop
- **Impact:** ğŸ”¥ğŸ”¥ğŸ”¥ Very high (active research, interpretable)

### 2. **Dependency Parsing on UD** (Your Current Work)
- **Dataset:** Universal Dependencies (pick morphologically rich language)
- **Baseline:** ~75-88% UAS (language-dependent)
- **PoH Expected:** ~85-95% UAS (+8-15%)
- **Why:** Natural pointer task, structured output
- **Impact:** ğŸ”¥ğŸ”¥ High (multilingual, practical)

### 3. **DocRED Relation Extraction** (Long-Range)
- **Dataset:** Document-level RE with evidence
- **Baseline:** ~50-60% F1
- **PoH Expected:** ~60-70% F1 (+10-20%)
- **Why:** Long documents, multiple relations, partial evidence
- **Impact:** ğŸ”¥ğŸ”¥ğŸ”¥ Very high (document-level is hot)

### 4. **HotpotQA Multi-Hop** (Multi-Hop Reasoning)
- **Dataset:** 2-hop question answering
- **Baseline:** ~60% EM
- **PoH Expected:** ~65-70% EM (+8-15%)
- **Why:** Explicit multi-hop, can visualize reasoning paths
- **Impact:** ğŸ”¥ğŸ”¥ğŸ”¥ Very high (interpretability + performance)

### 5. **AMR Parsing** (Structured Semantics)
- **Dataset:** Abstract Meaning Representation
- **Baseline:** ~75 Smatch
- **PoH Expected:** ~78-82 Smatch (+4-7%)
- **Why:** Graph structure, semantic relations
- **Impact:** ğŸ”¥ Moderate-high (niche but respected)

---

## ğŸ“Š Expected Results Summary

| Task | Difficulty | PoH Gain | Best Iters | Compute Cost | Overall |
|------|-----------|----------|------------|--------------|---------|
| Coreference (GAP) | ğŸ”´ Very Hard | +15-25% | 12-16 | High | ğŸ†ğŸ†ğŸ† |
| DocRED (RE) | ğŸ”´ Very Hard | +10-20% | 10-12 | High | ğŸ†ğŸ†ğŸ† |
| Dependency Parsing | ğŸŸ¡ Hard | +10-18% | 8-12 | Medium | ğŸ†ğŸ† |
| HotpotQA | ğŸŸ¡ Hard | +8-15% | 8-10 | Medium | ğŸ†ğŸ† |
| SRL | ğŸŸ¡ Hard | +10-15% | 8-10 | Medium | ğŸ†ğŸ† |
| AMR Parsing | ğŸŸ¡ Hard | +4-10% | 6-8 | Medium | ğŸ† |
| NER (nested) | ğŸŸ¢ Medium | +3-8% | 4-6 | Low | âš ï¸ |
| Constituency | ğŸŸ¢ Medium | +2-5% | 4-6 | Low | âš ï¸ |
| Sentiment | ğŸŸ¢ Easy | -2-+2% | N/A | N/A | âŒ |
| Text Classification | ğŸŸ¢ Easy | -5-0% | N/A | N/A | âŒ |

---

## ğŸ¯ Final Recommendation

**To find where YOUR PoH shines:**

1. **Start with Coreference (GAP)** - Highest expected gain, clear win
2. **Validate on Dependency Parsing** - Your current expertise, natural fit
3. **Scale to DocRED** - Document-level, long-range dependencies
4. **Prove multi-hop with HotpotQA** - Interpretable, impactful

**Avoid:**
- Sentiment, classification, generation tasks
- Tasks with >92% baseline accuracy
- Real-time inference scenarios

**Sweet Spot:**
- Structured output (pointers, graphs)
- Hard tasks (70-85% baseline)
- Long-range dependencies (>10 tokens)
- Offline/batch inference OK

---

**Bottom Line:** PoH is a **specialist**, not a generalist. It shines on **hard structured prediction**, not easy classification. Pick your battles wisely!

Author: Eran Ben Artzy  
Date: 2025

