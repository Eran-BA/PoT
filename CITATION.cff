# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: >-
  Pointer-over-Heads Transformer: Dynamic Multi-Head
  Attention with Adaptive Routing
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - family-names: Ben Artzy
    given-names: Eran
    email: eranb92@gmail.com
repository-code: 'https://github.com/Eran-BA/PoT'
url: 'https://github.com/Eran-BA/PoT'
abstract: >-
  The Pointer-over-Heads Transformer (PoT) is a drop-in modification 
  of multi-head attention that adds dynamic head-wise routing and 
  iterative refinement cycles with full compatibility with PyTorch 
  MultiheadAttention. A two-timescale hierarchical controller produces 
  per-token routing weights over attention heads, enabling the model 
  to compose different attention patterns across refinement steps.
  On the Sudoku-Extreme benchmark, PoT achieves 80% grid accuracy—a 
  +25 percentage point improvement over the HRM baseline (55%)—and 
  92% cell accuracy, outperforming Tiny Recursive Models (87%).
keywords:
  - transformer
  - attention mechanism
  - iterative refinement
  - adaptive computation
  - dynamic routing
  - multi-head attention
  - deep learning
  - pytorch
  - sudoku
  - reasoning
license: Apache-2.0
version: 2.0.0
date-released: '2025-12-16'
preferred-citation:
  type: article
  authors:
    - family-names: Ben Artzy
      given-names: Eran
      email: eranb92@gmail.com
  title: >-
    BERT/GPT with Inner-Thinking Cycles: Iterative Refinement 
    via Dynamic Head Routing
  year: 2025
  url: 'https://github.com/Eran-BA/PoT/blob/main/paper.pdf'

