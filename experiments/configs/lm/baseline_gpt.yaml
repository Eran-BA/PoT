model:
  type: baseline_gpt
  vocab_size: 32000
  d_model: 512
  n_heads: 8
  d_ff: 2048
  depth: 6
  dropout: 0.1
  max_seq_len: 512
  pos_encoding: absolute

train:
  batch_size: 16
  lr: 3e-4
  weight_decay: 0.01
  grad_clip: 1.0
  warmup_steps: 500
  max_steps: 2000
  eval_interval: 200
  seed: 42

data:
  dataset: synthetic_lm
  max_tokens: 50000
  seq_len: 128

save_dir: experiments/results/lm/baseline

