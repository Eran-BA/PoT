# Parameter Scaling Experiment Summary

## üéØ Objective

Test how **PoH-HRM** compares to **Baseline Transformer** across different model sizes on maze-solving tasks.

**Central Question:** Does the advantage of dynamic attention head routing scale with model capacity?

---

## üìä Experimental Design

### Model Sizes Tested

| Size   | d_model | n_heads | d_ff  | depth | Target Params |
|--------|---------|---------|-------|-------|---------------|
| Tiny   | 128     | 4       | 512   | 2     | ~1M           |
| Small  | 256     | 4       | 1024  | 3     | ~3M           |
| Medium | 512     | 8       | 2048  | 4     | ~10M          |
| Large  | 768     | 12      | 3072  | 6     | ~30M          |
| XL     | 1024    | 16      | 4096  | 8     | ~100M         |

### Architectures Compared

1. **Baseline Transformer**
   - Standard multi-head attention
   - Stacked transformer encoder layers
   - No dynamic routing

2. **PoH-HRM** 
   - Pointer-over-Heads dynamic head routing
   - Hierarchical Reasoning Module (f_L fast, f_H slow)
   - Iterative refinement (R=4 iterations)
   - HRM outer loop period (T=4)

### Task: Maze Solving

- **Grid sizes:** 16√ó16 (default, configurable)
- **Generation:** Using `maze-dataset` library with DFS algorithm
- **Difficulty:** Minimum path length filter (40% of grid size)
- **Training:** 1000 samples, 50 epochs per model
- **Testing:** 100 samples

### Metrics

1. **Accuracy:** % of mazes where model finds any valid path to goal
2. **Optimality:** % of mazes where path is ‚â§5% longer than shortest path

---

## üöÄ How to Run

### Local (with GPU)

```bash
# Quick test with default settings
./RUN_PARAMETER_SCALING.sh

# Custom configuration
python experiments/parameter_scaling_benchmark.py \
    --maze-size 16 \
    --train 1000 \
    --test 100 \
    --epochs 50 \
    --R 4 --T 4 \
    --seed 42
```

### Google Colab (Recommended)

1. Open: `notebooks/Parameter_Scaling_Benchmark_Colab.ipynb`
2. Runtime ‚Üí Change runtime type ‚Üí **A100 GPU**
3. Run all cells
4. Wait ~2-4 hours for completion
5. Download results JSON and plots

---

## üìà Expected Patterns

### Hypothesis 1: U-Shaped Advantage

- **Small models (1-3M):** Baseline may win
  - PoH controller overhead hurts when capacity is limited
  - Dynamic routing costs aren't justified
  
- **Medium models (10M):** PoH-HRM starts winning
  - Sufficient capacity for controller to be effective
  - Dynamic routing enables better credit assignment
  
- **Large models (30-100M):** PoH-HRM maintains or increases advantage
  - More heads ‚Üí more routing flexibility
  - Hierarchical control becomes more valuable

### Hypothesis 2: Linear Scaling

- Both models improve with size
- PoH-HRM maintains constant relative advantage
- No saturation up to 100M params

### Hypothesis 3: Early Saturation

- Both models plateau at medium scale
- Task is too easy for large models
- Need harder mazes (larger grid, longer paths)

---

## üìÅ Output Files

### Results JSON

Location: `experiments/results/parameter_scaling/scaling_results_maze16.json`

Structure:
```json
{
  "config": {
    "maze_size": 16,
    "n_train": 1000,
    "n_test": 100,
    "epochs": 50,
    "R": 4,
    "T": 4
  },
  "results": [
    {
      "size": "tiny",
      "baseline_params": 1234567,
      "baseline_acc": 45.0,
      "baseline_opt": 30.0,
      "poh_params": 1456789,
      "poh_acc": 52.0,
      "poh_opt": 38.0,
      "poh_advantage_acc": 7.0,
      "poh_advantage_opt": 8.0
    },
    ...
  ]
}
```

### Visualizations

Generated by: `python experiments/plot_parameter_scaling.py <results_json>`

1. **scaling_plot_maze16.png** - 4-panel figure:
   - Top-left: Accuracy vs. Parameters (log scale)
   - Top-right: Optimality vs. Parameters (log scale)
   - Bottom-left: PoH Accuracy Advantage (bar chart)
   - Bottom-right: PoH Optimality Advantage (bar chart)

2. **scaling_summary_maze16.txt** - Text summary:
   - Full results table
   - Average PoH advantage
   - Best configurations

---

## üî¨ Analysis Checklist

After running experiments, analyze:

- [ ] **Does PoH-HRM advantage exist at all sizes?**
  - Check bar charts (green = PoH wins)
  
- [ ] **At what size does PoH-HRM start winning?**
  - Look for crossover point in top plots
  
- [ ] **Does advantage grow with size?**
  - Check if PoH advantage bars increase left to right
  
- [ ] **Is there a saturation point?**
  - Look for flattening in top plots
  
- [ ] **Accuracy vs. Optimality trends**
  - Does PoH-HRM help more with finding paths or optimal paths?
  
- [ ] **Parameter efficiency**
  - Compare PoH performance at size N to Baseline at size 2N

---

## üîÑ Follow-up Experiments

### If PoH-HRM wins consistently:

1. **Ablation study:** Test R=1, T=‚àû (no HRM) to isolate routing benefit
2. **Harder tasks:** Increase maze size to 24√ó24, 32√ó32
3. **Other tasks:** Try on sorting, NLI, or sequence modeling
4. **Scaling further:** Test 200M, 500M models if resources allow

### If Baseline wins at some sizes:

1. **Debug:** Check if HRM state is actually being used
2. **Tune:** Try different R, T values for each model size
3. **Architecture:** Test with more heads (e.g., 32, 64 for XL)
4. **Training:** Try longer training (100 epochs) or different LR

### If results are unclear:

1. **Increase samples:** Use 5000 train, 500 test
2. **Multiple seeds:** Run with seeds 42, 43, 44, 45, 46
3. **Harder mazes:** Increase min_path_length to 60% of grid
4. **Bigger mazes:** Test on 20√ó20, 24√ó24 grids

---

## üìù Integration with Paper/README

### For README.md

Add section:
```markdown
## Parameter Scaling Analysis

We tested PoH-HRM vs. Baseline Transformer across 5 model sizes (1M-100M parameters) on maze solving.

**Key finding:** [Your result here]

See full results: `experiments/results/parameter_scaling/`

Run locally: `./RUN_PARAMETER_SCALING.sh`  
Run on Colab: [Parameter_Scaling_Benchmark_Colab.ipynb](notebooks/Parameter_Scaling_Benchmark_Colab.ipynb)
```

### For Paper

- **Section 4.3: Parameter Scaling Experiments**
  - Figure: scaling_plot_maze16.png
  - Table: Results summary
  - Analysis: Compare to hypothesis

- **Discussion**
  - When does PoH-HRM help most?
  - Implications for model design
  - Comparison to Mixture-of-Experts scaling

---

## üõ†Ô∏è Troubleshooting

### OOM (Out of Memory)

- Reduce batch size in `train_model()` (default: 32 ‚Üí 16)
- Skip XL model (comment out in MODEL_CONFIGS)
- Use smaller mazes (--maze-size 12)

### Taking too long

- Reduce epochs (--epochs 30)
- Reduce training data (--train 500)
- Test fewer sizes (edit MODEL_CONFIGS to only include tiny, small, medium)

### Baseline vs PoH params don't match

- This is expected! PoH has extra controller overhead
- Compare PoH at size N to Baseline at size 1.5N for fair comparison

### Both models perform poorly

- Task may be too hard: reduce min_path_length
- Or increase training: more samples or epochs
- Check maze generation quality visually

---

## üìö Related Files

- `experiments/parameter_scaling_benchmark.py` - Main script
- `experiments/plot_parameter_scaling.py` - Visualization
- `experiments/PARAMETER_SCALING_README.md` - Detailed usage guide
- `RUN_PARAMETER_SCALING.sh` - Quick local runner
- `notebooks/Parameter_Scaling_Benchmark_Colab.ipynb` - Colab notebook
- `experiments/maze_ab_proper_generation.py` - Single-size A/B test
- `experiments/maze_hyperparam_search.py` - R, T, n_heads search

---

## ‚úÖ Quick Start Checklist

- [ ] Clone repo and checkout `scaling_parameter_size` branch
- [ ] Install dependencies (`pip install maze-dataset matplotlib`)
- [ ] Run quick test with 12√ó12 mazes, 30 epochs
- [ ] If results look good, run full benchmark (16√ó16, 50 epochs)
- [ ] Generate plots and analyze results
- [ ] Document findings in README/paper
- [ ] Consider follow-up experiments based on results

---

**Good luck with your scaling experiments!** üöÄ

