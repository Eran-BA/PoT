{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ® Sokoban Supervised Learning with PoT\n",
        "\n",
        "This notebook trains a **Pondering over Thoughts (PoT)** model on Sokoban puzzles using **supervised learning** - identical to our Sudoku training pipeline.\n",
        "\n",
        "## What is Sokoban?\n",
        "\n",
        "**Sokoban** (å€‰åº«ç•ª, \"warehouse keeper\") is a classic puzzle game where you push boxes onto target locations.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4b/Sokoban_ani.gif\" width=\"300\" alt=\"Sokoban gameplay animation\">\n",
        "\n",
        "**Rules:**\n",
        "- ğŸ§‘ Player can move in 4 directions (up/down/left/right)\n",
        "- ğŸ“¦ Player can push ONE box at a time (not pull)\n",
        "- ğŸ¯ Goal: Push ALL boxes onto target squares\n",
        "- âš ï¸ Boxes can get stuck in corners (deadlock = game over!)\n",
        "\n",
        "â–¶ï¸ **Watch gameplay:** [Sokoban Tutorial on YouTube](https://www.youtube.com/watch?v=4SjXQ_bHTxU)\n",
        "\n",
        "**Why is it hard for AI?**\n",
        "- PSPACE-complete (exponential state space)\n",
        "- Sparse rewards (only get reward when solved)\n",
        "- Long-horizon planning required\n",
        "- Easy to create unsolvable states\n",
        "\n",
        "## Benchmark Comparison\n",
        "\n",
        "| Method | Simple (6Ã—6, 1 box) | Complex (10Ã—10, 2 boxes) | Notes |\n",
        "|--------|---------------------|--------------------------|-------|\n",
        "| SFT (paper) | ~50% | ~15% | Supervised fine-tuning |\n",
        "| GPT-4 + LangGraph | varies | varies | [Blog](https://blog.gopenai.com/using-llms-and-langgraph-to-tackle-sokoban-puzzles-5f50b43b9515) |\n",
        "| RL (PPO) | ~20% | <5% | Very hard to train |\n",
        "| Random | 25% | 25% | 4 actions |\n",
        "| **PoT (this notebook)** | TBD | TBD | Adaptive depth |\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We use the [Xiaofeng77/sokoban](https://huggingface.co/datasets/Xiaofeng77/sokoban) HuggingFace dataset with:\n",
        "- ~3,000 (board, optimal_action) pairs\n",
        "- On-the-fly augmentation (8x via rotations/flips)\n",
        "- Cross-entropy loss + Q-halt loss (identical to Sudoku)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ”§ Setup (Run First)\n",
        "# @markdown Install dependencies and clone repository\n",
        "\n",
        "!pip install -q torch datasets tqdm wandb gym-sokoban\n",
        "\n",
        "# Clone PoT repository\n",
        "!git clone -q https://github.com/ebenartzy/PoT.git 2>/dev/null || (cd PoT && git pull -q)\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'PoT')\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ“Š Configuration\n",
        "# @markdown ### Model Type\n",
        "MODEL_TYPE = \"hybrid_pot\"  # @param [\"pot\", \"hybrid_pot\", \"baseline\"]\n",
        "CONTROLLER_TYPE = \"transformer\"  # @param [\"transformer\", \"gru\", \"lstm\", \"diffusion\", \"swin\", \"mamba\"]\n",
        "\n",
        "# @markdown ### Architecture\n",
        "D_MODEL = 256  # @param {type:\"slider\", min:64, max:512, step:64}\n",
        "D_FF = 512  # @param {type:\"slider\", min:128, max:2048, step:128}\n",
        "N_HEADS = 4  # @param {type:\"slider\", min:2, max:16, step:2}\n",
        "N_LAYERS = 2  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "DROPOUT = 0.0  # @param {type:\"slider\", min:0.0, max:0.5, step:0.1}\n",
        "\n",
        "# @markdown ### PoT Iteration Parameters\n",
        "R = 4  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "T = 4  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "# @markdown ### Hybrid PoT Parameters (H/L cycles)\n",
        "H_LAYERS = 2  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "L_LAYERS = 2  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "H_CYCLES = 2  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "L_CYCLES = 6  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "# @markdown ### Controller Parameters\n",
        "D_CTRL = 128  # @param {type:\"slider\", min:32, max:256, step:32}\n",
        "MAX_DEPTH = 128  # @param {type:\"slider\", min:32, max:256, step:32}\n",
        "\n",
        "# @markdown ### Feature Injection\n",
        "INJECTION_MODE = \"broadcast\"  # @param [\"none\", \"broadcast\", \"film\", \"depth_token\", \"cross_attn\", \"alpha_gated\"]\n",
        "INJECTION_MEMORY_SIZE = 8  # @param {type:\"slider\", min:4, max:32, step:4}\n",
        "INJECTION_N_HEADS = 4  # @param {type:\"slider\", min:2, max:8, step:2}\n",
        "\n",
        "# @markdown ### ACT (Adaptive Computation Time) Parameters\n",
        "HALT_MAX_STEPS = 4  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "HALT_EXPLORATION_PROB = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "ALLOW_EARLY_HALT_EVAL = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Training Hyperparameters\n",
        "EPOCHS = 100  # @param {type:\"slider\", min:10, max:500, step:10}\n",
        "BATCH_SIZE = 64  # @param {type:\"slider\", min:16, max:256, step:16}\n",
        "LEARNING_RATE = 3e-4  # @param {type:\"number\"}\n",
        "WEIGHT_DECAY = 0.01  # @param {type:\"number\"}\n",
        "GRAD_CLIP = 1.0  # @param {type:\"slider\", min:0.1, max:5.0, step:0.1}\n",
        "WARMUP_STEPS = 100  # @param {type:\"slider\", min:0, max:1000, step:50}\n",
        "LR_MIN_RATIO = 0.1  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
        "BETA1 = 0.9  # @param {type:\"number\"}\n",
        "BETA2 = 0.95  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ### HRM Gradient Style\n",
        "HRM_GRAD_STYLE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Data\n",
        "AUGMENT = True  # @param {type:\"boolean\"}\n",
        "SEED = 42  # @param {type:\"integer\"}\n",
        "N_GENERATED = 0  # @param {type:\"slider\", min:0, max:10000, step:500}\n",
        "GEN_DIFFICULTY = \"simple\"  # @param [\"simple\", \"larger\", \"two_boxes\", \"complex\"]\n",
        "\n",
        "# @markdown ### Logging\n",
        "USE_WANDB = False  # @param {type:\"boolean\"}\n",
        "WANDB_PROJECT = \"sokoban-pot\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Evaluation\n",
        "EVAL_DIFFICULTIES = [\"simple\", \"complex\"]  # Easy (6x6,1box) and Hard (10x10,2boxes)\n",
        "EVAL_SAMPLES = 200  # @param {type:\"slider\", min:50, max:500, step:50}\n",
        "\n",
        "# Build config dict for easy access\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    'model_type': MODEL_TYPE,\n",
        "    'controller_type': CONTROLLER_TYPE,\n",
        "    'd_model': D_MODEL,\n",
        "    'd_ff': D_FF,\n",
        "    'n_heads': N_HEADS,\n",
        "    'n_layers': N_LAYERS,\n",
        "    'dropout': DROPOUT,\n",
        "    # PoT iterations\n",
        "    'R': R,\n",
        "    'T': T,\n",
        "    # Hybrid H/L cycles\n",
        "    'H_layers': H_LAYERS,\n",
        "    'L_layers': L_LAYERS,\n",
        "    'H_cycles': H_CYCLES,\n",
        "    'L_cycles': L_CYCLES,\n",
        "    # Controller\n",
        "    'd_ctrl': D_CTRL,\n",
        "    'max_depth': MAX_DEPTH,\n",
        "    # Feature Injection\n",
        "    'injection_mode': INJECTION_MODE,\n",
        "    'injection_memory_size': INJECTION_MEMORY_SIZE,\n",
        "    'injection_n_heads': INJECTION_N_HEADS,\n",
        "    # ACT\n",
        "    'halt_max_steps': HALT_MAX_STEPS,\n",
        "    'halt_exploration_prob': HALT_EXPLORATION_PROB,\n",
        "    'allow_early_halt_eval': ALLOW_EARLY_HALT_EVAL,\n",
        "    # Training\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'lr': LEARNING_RATE,\n",
        "    'weight_decay': WEIGHT_DECAY,\n",
        "    'grad_clip': GRAD_CLIP,\n",
        "    'warmup_steps': WARMUP_STEPS,\n",
        "    'lr_min_ratio': LR_MIN_RATIO,\n",
        "    'beta1': BETA1,\n",
        "    'beta2': BETA2,\n",
        "    # HRM\n",
        "    'hrm_grad_style': HRM_GRAD_STYLE,\n",
        "    # Data\n",
        "    'augment': AUGMENT,\n",
        "    'seed': SEED,\n",
        "    'n_generated': N_GENERATED,\n",
        "    'gen_difficulty': GEN_DIFFICULTY,\n",
        "}\n",
        "\n",
        "print(f\"Config: {MODEL_TYPE} ({CONTROLLER_TYPE})\")\n",
        "print(f\"  Architecture: d={D_MODEL}, ff={D_FF}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
        "print(f\"  PoT: R={R}, T={T}\")\n",
        "print(f\"  Hybrid: H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "print(f\"  Controller: d_ctrl={D_CTRL}, max_depth={MAX_DEPTH}\")\n",
        "print(f\"  Injection: mode={INJECTION_MODE}\")\n",
        "print(f\"  ACT: halt_max={HALT_MAX_STEPS}\")\n",
        "print(f\"  Training: epochs={EPOCHS}, lr={LEARNING_RATE}, batch={BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ“¥ Load Dataset\n",
        "# @markdown Downloads Sokoban dataset from HuggingFace + optional generated data\n",
        "# @markdown **Important:** Train/Val/Test are kept PURE (no augmentation leakage)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "from src.data.sokoban_hf import SokobanHFDataset, SokobanCombinedDataset\n",
        "\n",
        "# IMPORTANT: Load WITHOUT augmentation first to split cleanly\n",
        "# Then apply augmentation only to training samples\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "if N_GENERATED > 0:\n",
        "    print(f\"  HuggingFace + generating {N_GENERATED} additional {GEN_DIFFICULTY} puzzles...\")\n",
        "    # Load without augmentation for clean split\n",
        "    full_ds_no_aug = SokobanCombinedDataset(\n",
        "        hf_split=\"train\",\n",
        "        n_generated=N_GENERATED,\n",
        "        difficulty=GEN_DIFFICULTY,\n",
        "        augment=False,  # No augmentation for splitting\n",
        "        seed=SEED,\n",
        "    )\n",
        "    # Load with augmentation for training\n",
        "    full_ds_aug = SokobanCombinedDataset(\n",
        "        hf_split=\"train\",\n",
        "        n_generated=N_GENERATED,\n",
        "        difficulty=GEN_DIFFICULTY,\n",
        "        augment=AUGMENT,\n",
        "        seed=SEED,\n",
        "    )\n",
        "else:\n",
        "    # Load without augmentation for clean split\n",
        "    full_ds_no_aug = SokobanHFDataset(split=\"train\", augment=False)\n",
        "    # Load with augmentation for training\n",
        "    full_ds_aug = SokobanHFDataset(split=\"train\", augment=AUGMENT)\n",
        "\n",
        "# Test set: completely separate (from HuggingFace 'test' split)\n",
        "test_ds = SokobanHFDataset(split=\"test\", augment=False)\n",
        "\n",
        "# Split indices (not datasets!) to keep train/val PURE\n",
        "n_total = len(full_ds_no_aug)\n",
        "val_size = min(500, n_total // 5)\n",
        "train_size = n_total - val_size\n",
        "\n",
        "# Deterministic shuffle\n",
        "rng = np.random.default_rng(SEED)\n",
        "indices = rng.permutation(n_total)\n",
        "train_indices = indices[:train_size].tolist()\n",
        "val_indices = indices[train_size:].tolist()\n",
        "\n",
        "# Train: uses augmented dataset (on-the-fly augmentation)\n",
        "# Val: uses non-augmented dataset (PURE - no augmentation)\n",
        "train_subset = Subset(full_ds_aug, train_indices)\n",
        "val_subset = Subset(full_ds_no_aug, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nâœ… Data splits (PURE - no leakage):\")\n",
        "print(f\"   Train: {len(train_subset)} samples (augment={'ON' if AUGMENT else 'OFF'})\")\n",
        "print(f\"   Val:   {len(val_subset)} samples (augment=OFF, pure)\")\n",
        "print(f\"   Test:  {len(test_ds)} samples (augment=OFF, separate HF split)\")\n",
        "print(f\"   Board shape: {full_ds_no_aug.board_shape}\")\n",
        "if N_GENERATED > 0:\n",
        "    print(f\"   +{N_GENERATED} generated {GEN_DIFFICULTY} puzzles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ—ï¸ Create Model\n",
        "\n",
        "from src.pot.models.sokoban_solver import PoTSokobanSolver, HybridPoTSokobanSolver, BaselineSokobanSolver\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "H, W = full_ds_no_aug.board_shape\n",
        "seq_len = H * W\n",
        "\n",
        "# Controller kwargs for advanced controllers\n",
        "controller_kwargs = {\n",
        "    'n_heads': N_HEADS,\n",
        "    'd_ctrl': D_CTRL,\n",
        "    'max_depth': MAX_DEPTH,\n",
        "}\n",
        "\n",
        "# Injection kwargs for cross_attn mode\n",
        "injection_kwargs = {\n",
        "    'memory_size': INJECTION_MEMORY_SIZE,\n",
        "    'n_heads': INJECTION_N_HEADS,\n",
        "} if INJECTION_MODE == 'cross_attn' else None\n",
        "\n",
        "if MODEL_TYPE == \"pot\":\n",
        "    model = PoTSokobanSolver(\n",
        "        board_height=H,\n",
        "        board_width=W,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "        R=R,\n",
        "        controller_type=CONTROLLER_TYPE,\n",
        "        controller_kwargs=controller_kwargs,\n",
        "        max_depth=MAX_DEPTH,\n",
        "    )\n",
        "elif MODEL_TYPE == \"hybrid_pot\":\n",
        "    model = HybridPoTSokobanSolver(\n",
        "        board_height=H,\n",
        "        board_width=W,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        H_layers=H_LAYERS,\n",
        "        L_layers=L_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "        H_cycles=H_CYCLES,\n",
        "        L_cycles=L_CYCLES,\n",
        "        T=T,\n",
        "        halt_max_steps=HALT_MAX_STEPS,\n",
        "        halt_exploration_prob=HALT_EXPLORATION_PROB,\n",
        "        allow_early_halt_eval=ALLOW_EARLY_HALT_EVAL,\n",
        "        hrm_grad_style=HRM_GRAD_STYLE,\n",
        "        controller_type=CONTROLLER_TYPE,\n",
        "        controller_kwargs=controller_kwargs,\n",
        "        injection_mode=INJECTION_MODE,\n",
        "        injection_kwargs=injection_kwargs,\n",
        "    )\n",
        "else:\n",
        "    model = BaselineSokobanSolver(\n",
        "        board_height=H,\n",
        "        board_width=W,\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"âœ… {MODEL_TYPE} ({CONTROLLER_TYPE}) model created\")\n",
        "print(f\"   Parameters: {n_params:,}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Board: {H}x{W} = {seq_len} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸš€ Train Model\n",
        "# @markdown Supervised training with cross-entropy + Q-halt loss (identical to Sudoku)\n",
        "\n",
        "from src.training.sokoban_supervised import train_supervised\n",
        "import time\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    run_name = f\"{MODEL_TYPE}-{CONTROLLER_TYPE}-R{R}-H{H_CYCLES}L{L_CYCLES}\"\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=run_name,\n",
        "        config=CONFIG,\n",
        "    )\n",
        "\n",
        "print(f\"Training {MODEL_TYPE} ({CONTROLLER_TYPE})...\")\n",
        "print(f\"  Epochs: {EPOCHS}, LR: {LEARNING_RATE}, Batch: {BATCH_SIZE}\")\n",
        "print(f\"  PoT: R={R}, T={T}, H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results = train_supervised(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    grad_clip=GRAD_CLIP,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    use_pot=(MODEL_TYPE != \"baseline\"),\n",
        "    wandb_log=USE_WANDB,\n",
        "    # Additional optimizer params\n",
        "    betas=(BETA1, BETA2),\n",
        "    lr_min_ratio=LR_MIN_RATIO,\n",
        ")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Training complete!\")\n",
        "print(f\"   Best val accuracy: {results['best_val_acc']:.2%}\")\n",
        "print(f\"   Training time: {train_time / 60:.1f} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ“ˆ Evaluate on HuggingFace Test Set\n",
        "\n",
        "from src.training.sokoban_supervised import evaluate\n",
        "\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "print(f\"\\nğŸ“Š HuggingFace Test Set Results:\")\n",
        "print(f\"   Accuracy: {test_metrics['accuracy']:.2%}\")\n",
        "print(f\"   Loss: {test_metrics['loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ¯ Multi-Difficulty Evaluation\n",
        "# @markdown Evaluate on Simple (6Ã—6, 1 box) and Complex (10Ã—10, 2 boxes)\n",
        "\n",
        "from src.data.sokoban_generator import SokobanGeneratedDataset\n",
        "\n",
        "difficulty_results = {}\n",
        "\n",
        "for difficulty in EVAL_DIFFICULTIES:\n",
        "    print(f\"\\nGenerating {difficulty} test boards...\")\n",
        "    \n",
        "    eval_ds = SokobanGeneratedDataset(\n",
        "        difficulty=difficulty,\n",
        "        n_samples=EVAL_SAMPLES,\n",
        "        seed=1042,\n",
        "        augment=False,\n",
        "    )\n",
        "    \n",
        "    eval_loader = DataLoader(eval_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # Handle different board sizes\n",
        "    eval_h, eval_w = eval_ds.board_shape\n",
        "    if (eval_h, eval_w) != train_ds.board_shape:\n",
        "        print(f\"   âš ï¸ Board size mismatch: train={train_ds.board_shape}, eval={eval_ds.board_shape}\")\n",
        "        print(f\"   Skipping (model trained on fixed size)\")\n",
        "        difficulty_results[difficulty] = {'accuracy': None, 'note': 'size_mismatch'}\n",
        "        continue\n",
        "    \n",
        "    eval_metrics = evaluate(model, eval_loader, device)\n",
        "    difficulty_results[difficulty] = {\n",
        "        'accuracy': eval_metrics['accuracy'],\n",
        "        'loss': eval_metrics['loss'],\n",
        "    }\n",
        "    print(f\"   {difficulty}: {eval_metrics['accuracy']:.2%} accuracy\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"MULTI-DIFFICULTY RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "for diff, res in difficulty_results.items():\n",
        "    if res.get('accuracy') is not None:\n",
        "        print(f\"  {diff}: {res['accuracy']:.2%}\")\n",
        "    else:\n",
        "        print(f\"  {diff}: N/A ({res.get('note', 'error')})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ“‹ Final Comparison Table\n",
        "\n",
        "simple_acc = difficulty_results.get('simple', {}).get('accuracy', 0) or 0\n",
        "complex_acc = difficulty_results.get('complex', {}).get('accuracy', 0) or 0\n",
        "\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    SOKOBAN BENCHMARK COMPARISON                          â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘ Method                   â”‚ Simple (6Ã—6,1) â”‚ Complex (10Ã—10,2) â”‚ Notes    â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘ SFT (paper)              â”‚     ~50%       â”‚      ~15%         â”‚ Baseline â•‘\n",
        "â•‘ GPT-4 + LangGraph        â”‚     varies     â”‚      varies       â”‚ Zero-shotâ•‘\n",
        "â•‘ RL (PPO)                 â”‚     ~20%       â”‚      <5%          â”‚ Hard     â•‘\n",
        "â•‘ Random                   â”‚      25%       â”‚       25%         â”‚ 4 actionsâ•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•£\"\"\")\n",
        "\n",
        "print(f\"â•‘ PoT (this run)           â”‚    {simple_acc:5.1%}      â”‚     {complex_acc:5.1%}        â”‚ R={R}      â•‘\")\n",
        "print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
        "\n",
        "# Store ALL config + results for Optuna/hyperparameter search\n",
        "COLAB_RESULTS = {\n",
        "    # Full configuration (for Optuna)\n",
        "    **CONFIG,\n",
        "    # Results\n",
        "    'best_val_acc': results['best_val_acc'],\n",
        "    'hf_test_acc': test_metrics['accuracy'],\n",
        "    'simple_acc': simple_acc,\n",
        "    'complex_acc': complex_acc,\n",
        "    'train_time_min': train_time / 60,\n",
        "    'n_params': n_params,\n",
        "    'difficulty_results': difficulty_results,\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ“Š Full Results (for Optuna search):\")\n",
        "print(f\"   Model: {MODEL_TYPE} ({CONTROLLER_TYPE})\")\n",
        "print(f\"   Architecture: d={D_MODEL}, ff={D_FF}, heads={N_HEADS}\")\n",
        "print(f\"   PoT: R={R}, T={T}, H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "print(f\"   ACT: halt_max={HALT_MAX_STEPS}, max_depth={MAX_DEPTH}\")\n",
        "print(f\"   Val Acc: {results['best_val_acc']:.2%}\")\n",
        "print(f\"   Test Acc: {test_metrics['accuracy']:.2%}\")\n",
        "print(f\"   Simple Acc: {simple_acc:.2%}\")\n",
        "print(f\"   Complex Acc: {complex_acc:.2%}\")\n",
        "print(f\"   Params: {n_params:,}\")\n",
        "print(f\"   Time: {train_time / 60:.1f} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ğŸ’¾ Save Model & Results (Optional)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Save model checkpoint\n",
        "model_name = f\"sokoban_{MODEL_TYPE}_{CONTROLLER_TYPE}_R{R}_H{H_CYCLES}L{L_CYCLES}\"\n",
        "save_path = f\"checkpoints/{model_name}.pt\"\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'results': COLAB_RESULTS,\n",
        "}, save_path)\n",
        "\n",
        "print(f\"âœ… Model saved to {save_path}\")\n",
        "\n",
        "# Save results JSON (for Optuna aggregation)\n",
        "results_path = f\"checkpoints/{model_name}_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    # Convert non-serializable items\n",
        "    results_to_save = {k: v for k, v in COLAB_RESULTS.items() \n",
        "                       if not isinstance(v, dict) or k == 'difficulty_results'}\n",
        "    json.dump(results_to_save, f, indent=2, default=str)\n",
        "\n",
        "print(f\"âœ… Results saved to {results_path}\")\n",
        "\n",
        "# Print command to download\n",
        "print(f\"\\nğŸ“¥ Download command:\")\n",
        "print(f\"   from google.colab import files\")\n",
        "print(f\"   files.download('{save_path}')\")\n",
        "print(f\"   files.download('{results_path}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Notes\n",
        "\n",
        "### Loss Function (Identical to Sudoku)\n",
        "```python\n",
        "loss = cross_entropy(action_logits, action_label) + 0.5 * bce(q_halt, is_correct)\n",
        "```\n",
        "\n",
        "### Configuration Parameters for Optuna Search\n",
        "\n",
        "| Parameter | Description | Typical Range |\n",
        "|-----------|-------------|---------------|\n",
        "| **Architecture** | | |\n",
        "| `D_MODEL` | Hidden dimension | 64-512 |\n",
        "| `D_FF` | Feedforward dimension | 128-2048 |\n",
        "| `N_HEADS` | Attention heads | 2-16 |\n",
        "| `N_LAYERS` | Transformer layers | 1-8 |\n",
        "| **PoT Iterations** | | |\n",
        "| `R` | Refinement iterations | 1-16 |\n",
        "| `T` | HRM period | 1-8 |\n",
        "| **Hybrid H/L Cycles** | | |\n",
        "| `H_CYCLES` | High-level (slow) cycles | 1-8 |\n",
        "| `L_CYCLES` | Low-level (fast) cycles | 1-16 |\n",
        "| `H_LAYERS` | Layers in H-level | 1-4 |\n",
        "| `L_LAYERS` | Layers in L-level | 1-4 |\n",
        "| **Controller** | | |\n",
        "| `D_CTRL` | Controller hidden dimension | 32-256 |\n",
        "| `MAX_DEPTH` | Max controller depth | 32-256 |\n",
        "| **Feature Injection** | | |\n",
        "| `INJECTION_MODE` | Injection mode | none/broadcast/film/etc |\n",
        "| `INJECTION_MEMORY_SIZE` | Memory size (cross_attn) | 4-32 |\n",
        "| `INJECTION_N_HEADS` | Heads (cross_attn) | 2-8 |\n",
        "| **ACT (Adaptive Computation)** | | |\n",
        "| `HALT_MAX_STEPS` | Max halting steps | 1-16 |\n",
        "| `HALT_EXPLORATION_PROB` | Exploration probability | 0.0-1.0 |\n",
        "| **Controller Types** | | |\n",
        "| `transformer` | CausalDepthTransformerRouter | Default |\n",
        "| `gru` | GRU-based controller | Fast |\n",
        "| `lstm` | LSTM-based controller | |\n",
        "| `diffusion` | Diffusion denoising | DiT-style |\n",
        "| `swin` | Swin Transformer | Vision |\n",
        "| `mamba` | Mamba SSM | State-space |\n",
        "| **Data Generation** | | |\n",
        "| `N_GENERATED` | Extra puzzles to generate | 0-10000 |\n",
        "| `GEN_DIFFICULTY` | Difficulty of generated | simple/larger/two_boxes/complex |\n",
        "\n",
        "### Difficulty Levels\n",
        "| Difficulty | Size | Boxes | Avg Solution Length |\n",
        "|------------|------|-------|---------------------|\n",
        "| simple | 6Ã—6 | 1 | ~4 moves |\n",
        "| larger | 10Ã—10 | 1 | ~15 moves |\n",
        "| two_boxes | 6Ã—6 | 2 | ~20 moves |\n",
        "| complex | 10Ã—10 | 2 | ~18 moves |\n",
        "\n",
        "### Example Optuna Study\n",
        "```python\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    R = trial.suggest_int('R', 1, 16)\n",
        "    H_CYCLES = trial.suggest_int('H_cycles', 1, 8)\n",
        "    L_CYCLES = trial.suggest_int('L_cycles', 1, 16)\n",
        "    D_MODEL = trial.suggest_categorical('d_model', [128, 256, 512])\n",
        "    # ... train and return complex_acc\n",
        "    return complex_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "```\n",
        "\n",
        "### References\n",
        "- [Debunking SFT Generalization](https://arxiv.org/pdf/2510.00237)\n",
        "- [LLMs + LangGraph for Sokoban](https://blog.gopenai.com/using-llms-and-langgraph-to-tackle-sokoban-puzzles-5f50b43b9515)\n",
        "- [HuggingFace Dataset](https://huggingface.co/datasets/Xiaofeng77/sokoban)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
