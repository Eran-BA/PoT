{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfae Sokoban Supervised Learning with PoT\n",
        "\n",
        "This notebook trains a **Pondering over Thoughts (PoT)** model on Sokoban puzzles using **supervised learning** - identical to our Sudoku training pipeline.\n",
        "\n",
        "## What is Sokoban?\n",
        "\n",
        "**Sokoban** (\u5009\u5eab\u756a, \"warehouse keeper\") is a classic puzzle game where you push boxes onto target locations.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4b/Sokoban_ani.gif\" width=\"300\" alt=\"Sokoban gameplay animation\">\n",
        "\n",
        "**Rules:**\n",
        "- \ud83e\uddd1 Player can move in 4 directions (up/down/left/right)\n",
        "- \ud83d\udce6 Player can push ONE box at a time (not pull)\n",
        "- \ud83c\udfaf Goal: Push ALL boxes onto target squares\n",
        "- \u26a0\ufe0f Boxes can get stuck in corners (deadlock = game over!)\n",
        "\n",
        "\u25b6\ufe0f **Watch gameplay:** [Sokoban Tutorial on YouTube](https://www.youtube.com/watch?v=4SjXQ_bHTxU)\n",
        "\n",
        "**Why is it hard for AI?**\n",
        "- PSPACE-complete (exponential state space)\n",
        "- Sparse rewards (only get reward when solved)\n",
        "- Long-horizon planning required\n",
        "- Easy to create unsolvable states\n",
        "\n",
        "## Benchmark Comparison\n",
        "\n",
        "| Method | Simple (6\u00d76, 1 box) | Complex (10\u00d710, 2 boxes) | Notes |\n",
        "|--------|---------------------|--------------------------|-------|\n",
        "| SFT (paper) | ~50% | ~15% | Supervised fine-tuning |\n",
        "| GPT-4 + LangGraph | varies | varies | [Blog](https://blog.gopenai.com/using-llms-and-langgraph-to-tackle-sokoban-puzzles-5f50b43b9515) |\n",
        "| RL (PPO) | ~20% | <5% | Very hard to train |\n",
        "| Random | 25% | 25% | 4 actions |\n",
        "| **PoT (this notebook)** | TBD | TBD | Adaptive depth |\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We use the [Xiaofeng77/sokoban](https://huggingface.co/datasets/Xiaofeng77/sokoban) HuggingFace dataset with:\n",
        "- ~3,000 (board, optimal_action) pairs\n",
        "- On-the-fly augmentation (8x via rotations/flips)\n",
        "- Cross-entropy loss + Q-halt loss (identical to Sudoku)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udd27 Setup (Run First)\n",
        "# @markdown Install dependencies and clone repository\n",
        "\n",
        "!pip install -q torch datasets tqdm wandb gym-sokoban\n",
        "\n",
        "# Clone PoT repository\n",
        "!git clone -q https://github.com/ebenartzy/PoT.git 2>/dev/null || (cd PoT && git pull -q)\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'PoT')\n",
        "\n",
        "print(\"\u2705 Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udd11 Weights & Biases Login (Optional)\n",
        "# @markdown Enable USE_WANDB in Configuration to track experiments\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Login to W&B (will prompt for API key on first run)\n",
        "wandb.login()\n",
        "print(\"\u2705 Logged in to Weights & Biases!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udcca Configuration\n",
        "# @markdown ### Model Type\n",
        "MODEL_TYPE = \"hybrid_pot\"  # @param [\"pot\", \"hybrid_pot\", \"baseline\"]\n",
        "CONTROLLER_TYPE = \"transformer\"  # @param [\"transformer\", \"gru\", \"lstm\", \"diffusion\", \"swin\", \"mamba\"]\n",
        "\n",
        "# @markdown ### Architecture\n",
        "D_MODEL = 512  # @param {type:\"slider\", min:64, max:512, step:64}\n",
        "D_FF = 1024  # @param {type:\"slider\", min:128, max:2048, step:128}\n",
        "N_HEADS = 4  # @param {type:\"slider\", min:2, max:16, step:2}\n",
        "N_LAYERS = 2  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "DROPOUT = 0.0  # @param {type:\"slider\", min:0.0, max:0.5, step:0.1}\n",
        "\n",
        "# @markdown ### PoT Iteration Parameters\n",
        "R = 4  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "T = 4  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "# @markdown ### Hybrid PoT Parameters (H/L cycles)\n",
        "H_LAYERS = 2  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "L_LAYERS = 2  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "H_CYCLES = 2  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "L_CYCLES = 6  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "# @markdown ### Controller Parameters\n",
        "D_CTRL = 128  # @param {type:\"slider\", min:32, max:256, step:32}\n",
        "MAX_DEPTH = 128  # @param {type:\"slider\", min:32, max:256, step:32}\n",
        "\n",
        "# @markdown ### Feature Injection\n",
        "INJECTION_MODE = \"broadcast\"  # @param [\"none\", \"broadcast\", \"film\", \"depth_token\", \"cross_attn\", \"alpha_gated\"]\n",
        "INJECTION_MEMORY_SIZE = 8  # @param {type:\"slider\", min:4, max:32, step:4}\n",
        "INJECTION_N_HEADS = 4  # @param {type:\"slider\", min:2, max:8, step:2}\n",
        "\n",
        "# @markdown ### ACT (Adaptive Computation Time) Parameters\n",
        "HALT_MAX_STEPS = 4  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "HALT_EXPLORATION_PROB = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "ALLOW_EARLY_HALT_EVAL = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Training Hyperparameters\n",
        "EPOCHS = 100  # @param {type:\"slider\", min:10, max:500, step:10}\n",
        "BATCH_SIZE = 64  # @param {type:\"slider\", min:16, max:256, step:16}\n",
        "LEARNING_RATE = 3e-4  # @param {type:\"number\"}\n",
        "WEIGHT_DECAY = 0.01  # @param {type:\"number\"}\n",
        "GRAD_CLIP = 1.0  # @param {type:\"slider\", min:0.1, max:5.0, step:0.1}\n",
        "WARMUP_STEPS = 100  # @param {type:\"slider\", min:0, max:1000, step:50}\n",
        "LR_MIN_RATIO = 0.1  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
        "BETA1 = 0.9  # @param {type:\"number\"}\n",
        "BETA2 = 0.95  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ### HRM Gradient Style\n",
        "HRM_GRAD_STYLE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Data\n",
        "AUGMENT = True  # @param {type:\"boolean\"}\n",
        "SEED = 42  # @param {type:\"integer\"}\n",
        "N_GENERATED = 1000  # @param {type:\"slider\", min:0, max:100000, step:1000}\n",
        "GEN_DIFFICULTY = \"simple\"  # @param [\"simple\", \"larger\", \"two_boxes\", \"complex\"]\n",
        "\n",
        "# @markdown ### Curriculum Learning\n",
        "CURRICULUM = False  # @param {type:\"boolean\"}\n",
        "CURRICULUM_WARMUP = 0.3  # @param {type:\"slider\", min:0.1, max:0.5, step:0.1}\n",
        "\n",
        "# @markdown ### Size Generalization (Padding)\n",
        "PAD_TO_SIZE = 10  # @param {type:\"slider\", min:0, max:16, step:2}\n",
        "# 0 = no padding, 10 = pad all boards to 10x10 for training on small, eval on large\n",
        "\n",
        "# @markdown ### Logging\n",
        "USE_WANDB = False  # @param {type:\"boolean\"}\n",
        "WANDB_PROJECT = \"sokoban-pot\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Evaluation\n",
        "EVAL_DIFFICULTIES = [\"simple\", \"complex\"]  # Easy (6x6,1box) and Hard (10x10,2boxes)\n",
        "EVAL_SAMPLES = 200  # @param {type:\"slider\", min:50, max:500, step:50}\n",
        "\n",
        "# Build config dict for easy access\n",
        "CONFIG = {\n",
        "    # Model\n",
        "    'model_type': MODEL_TYPE,\n",
        "    'controller_type': CONTROLLER_TYPE,\n",
        "    'd_model': D_MODEL,\n",
        "    'd_ff': D_FF,\n",
        "    'n_heads': N_HEADS,\n",
        "    'n_layers': N_LAYERS,\n",
        "    'dropout': DROPOUT,\n",
        "    # PoT iterations\n",
        "    'R': R,\n",
        "    'T': T,\n",
        "    # Hybrid H/L cycles\n",
        "    'H_layers': H_LAYERS,\n",
        "    'L_layers': L_LAYERS,\n",
        "    'H_cycles': H_CYCLES,\n",
        "    'L_cycles': L_CYCLES,\n",
        "    # Controller\n",
        "    'd_ctrl': D_CTRL,\n",
        "    'max_depth': MAX_DEPTH,\n",
        "    # Feature Injection\n",
        "    'injection_mode': INJECTION_MODE,\n",
        "    'injection_memory_size': INJECTION_MEMORY_SIZE,\n",
        "    'injection_n_heads': INJECTION_N_HEADS,\n",
        "    # ACT\n",
        "    'halt_max_steps': HALT_MAX_STEPS,\n",
        "    'halt_exploration_prob': HALT_EXPLORATION_PROB,\n",
        "    'allow_early_halt_eval': ALLOW_EARLY_HALT_EVAL,\n",
        "    # Training\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'lr': LEARNING_RATE,\n",
        "    'weight_decay': WEIGHT_DECAY,\n",
        "    'grad_clip': GRAD_CLIP,\n",
        "    'warmup_steps': WARMUP_STEPS,\n",
        "    'lr_min_ratio': LR_MIN_RATIO,\n",
        "    'beta1': BETA1,\n",
        "    'beta2': BETA2,\n",
        "    # HRM\n",
        "    'hrm_grad_style': HRM_GRAD_STYLE,\n",
        "    # Data\n",
        "    'augment': AUGMENT,\n",
        "    'seed': SEED,\n",
        "    'n_generated': N_GENERATED,\n",
        "    'gen_difficulty': GEN_DIFFICULTY,\n",
        "    # Curriculum\n",
        "    'curriculum': CURRICULUM,\n",
        "    'curriculum_warmup': CURRICULUM_WARMUP,\n",
        "    # Size generalization\n",
        "    'pad_to_size': PAD_TO_SIZE,\n",
        "}\n",
        "\n",
        "print(f\"Config: {MODEL_TYPE} ({CONTROLLER_TYPE})\")\n",
        "print(f\"  Architecture: d={D_MODEL}, ff={D_FF}, heads={N_HEADS}, layers={N_LAYERS}\")\n",
        "print(f\"  PoT: R={R}, T={T}\")\n",
        "print(f\"  Hybrid: H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "print(f\"  Controller: d_ctrl={D_CTRL}, max_depth={MAX_DEPTH}\")\n",
        "print(f\"  Injection: mode={INJECTION_MODE}\")\n",
        "print(f\"  ACT: halt_max={HALT_MAX_STEPS}\")\n",
        "print(f\"  Training: epochs={EPOCHS}, lr={LEARNING_RATE}, batch={BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udce5 Load Dataset\n",
        "# @markdown Downloads Sokoban dataset from HuggingFace + optional generated data\n",
        "# @markdown **Important:** Train/Val/Test are kept PURE (no augmentation leakage)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import numpy as np\n",
        "\n",
        "from src.data.sokoban_hf import SokobanHFDataset, SokobanCombinedDataset\n",
        "\n",
        "# Padding wrapper for size generalization\n",
        "class PaddedDataset(Dataset):\n",
        "    \"\"\"Wraps a dataset and pads all boards to target size with walls.\"\"\"\n",
        "    TILE_WALL = 0  # Wall tile for padding\n",
        "    \n",
        "    def __init__(self, dataset, target_size: int):\n",
        "        self.dataset = dataset\n",
        "        self.target_size = target_size\n",
        "        self._orig_shape = dataset.board_shape\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        \n",
        "        # Get original board\n",
        "        board_input = sample['input']  # [H, W, 7] one-hot\n",
        "        orig_h, orig_w = board_input.shape[:2]\n",
        "        \n",
        "        if orig_h >= self.target_size and orig_w >= self.target_size:\n",
        "            return sample  # Already big enough\n",
        "        \n",
        "        # Calculate padding (center the original board)\n",
        "        pad_h = self.target_size - orig_h\n",
        "        pad_w = self.target_size - orig_w\n",
        "        pad_top = pad_h // 2\n",
        "        pad_left = pad_w // 2\n",
        "        \n",
        "        # Create padded one-hot board (all walls)\n",
        "        padded = torch.zeros(self.target_size, self.target_size, 7)\n",
        "        padded[:, :, self.TILE_WALL] = 1.0  # Fill with walls\n",
        "        \n",
        "        # Place original board in center\n",
        "        padded[pad_top:pad_top+orig_h, pad_left:pad_left+orig_w] = board_input\n",
        "        \n",
        "        # Also pad board_indices if present\n",
        "        if 'board_indices' in sample:\n",
        "            board_idx = sample['board_indices']\n",
        "            padded_idx = torch.zeros(self.target_size, self.target_size, dtype=torch.long)\n",
        "            padded_idx[pad_top:pad_top+orig_h, pad_left:pad_left+orig_w] = board_idx\n",
        "            sample['board_indices'] = padded_idx\n",
        "        \n",
        "        sample['input'] = padded\n",
        "        return sample\n",
        "    \n",
        "    @property\n",
        "    def board_shape(self):\n",
        "        return (self.target_size, self.target_size)\n",
        "\n",
        "# IMPORTANT: Load WITHOUT augmentation first to split cleanly\n",
        "# Then apply augmentation only to training samples\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "if N_GENERATED > 0:\n",
        "    print(f\"  HuggingFace + generating {N_GENERATED} additional {GEN_DIFFICULTY} puzzles...\")\n",
        "    # Load without augmentation for clean split\n",
        "    full_ds_no_aug = SokobanCombinedDataset(\n",
        "        hf_split=\"train\",\n",
        "        n_generated=N_GENERATED,\n",
        "        difficulty=GEN_DIFFICULTY,\n",
        "        augment=False,  # No augmentation for splitting\n",
        "        seed=SEED,\n",
        "    )\n",
        "    # Load with augmentation for training\n",
        "    full_ds_aug = SokobanCombinedDataset(\n",
        "        hf_split=\"train\",\n",
        "        n_generated=N_GENERATED,\n",
        "        difficulty=GEN_DIFFICULTY,\n",
        "        augment=AUGMENT,\n",
        "        seed=SEED,\n",
        "    )\n",
        "else:\n",
        "    # Load without augmentation for clean split\n",
        "    full_ds_no_aug = SokobanHFDataset(split=\"train\", augment=False)\n",
        "    # Load with augmentation for training\n",
        "    full_ds_aug = SokobanHFDataset(split=\"train\", augment=AUGMENT)\n",
        "\n",
        "# Test set: completely separate (from HuggingFace 'test' split)\n",
        "test_ds = SokobanHFDataset(split=\"test\", augment=False)\n",
        "\n",
        "# Apply padding for size generalization\n",
        "if PAD_TO_SIZE > 0:\n",
        "    orig_shape = full_ds_no_aug.board_shape\n",
        "    print(f\"  \ud83d\udd32 Padding {orig_shape} \u2192 ({PAD_TO_SIZE}\u00d7{PAD_TO_SIZE}) for size generalization\")\n",
        "    full_ds_no_aug = PaddedDataset(full_ds_no_aug, PAD_TO_SIZE)\n",
        "    full_ds_aug = PaddedDataset(full_ds_aug, PAD_TO_SIZE)\n",
        "    # Note: test_ds is NOT padded - we want to evaluate on native sizes\n",
        "\n",
        "# Split indices (not datasets!) to keep train/val PURE\n",
        "n_total = len(full_ds_no_aug)\n",
        "val_size = min(500, n_total // 5)\n",
        "train_size = n_total - val_size\n",
        "\n",
        "# Deterministic shuffle\n",
        "rng = np.random.default_rng(SEED)\n",
        "indices = rng.permutation(n_total)\n",
        "train_indices = indices[:train_size].tolist()\n",
        "val_indices = indices[train_size:].tolist()\n",
        "\n",
        "# Train: uses augmented dataset (on-the-fly augmentation)\n",
        "# Val: uses non-augmented dataset (PURE - no augmentation)\n",
        "train_subset = Subset(full_ds_aug, train_indices)\n",
        "val_subset = Subset(full_ds_no_aug, val_indices)\n",
        "\n",
        "# Create data loaders (curriculum handled in training loop)\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# For curriculum: get solution lengths if available (generated data only)\n",
        "curriculum_order = None\n",
        "if CURRICULUM and N_GENERATED > 0 and hasattr(full_ds_no_aug, 'generated_examples'):\n",
        "    # Sort by solution length (easiest first)\n",
        "    sol_lengths = []\n",
        "    for idx in train_indices:\n",
        "        if idx >= full_ds_no_aug.n_hf:\n",
        "            gen_idx = idx - full_ds_no_aug.n_hf\n",
        "            sol_len = full_ds_no_aug.generated_examples[gen_idx].get('solution_length', 0)\n",
        "        else:\n",
        "            sol_len = 0  # HF data - no solution length, treat as easy\n",
        "        sol_lengths.append((idx, sol_len))\n",
        "    \n",
        "    # Sort by solution length\n",
        "    sol_lengths.sort(key=lambda x: x[1])\n",
        "    curriculum_order = [idx for idx, _ in sol_lengths]\n",
        "    avg_len = np.mean([s for _, s in sol_lengths if s > 0])\n",
        "    print(f\"   \ud83d\udcda Curriculum: sorted by solution length (avg={avg_len:.1f})\")\n",
        "elif CURRICULUM:\n",
        "    print(f\"   \u26a0\ufe0f Curriculum requires N_GENERATED > 0 (need solution lengths)\")\n",
        "\n",
        "print(f\"\\n\u2705 Data splits (PURE - no leakage):\")\n",
        "print(f\"   Train: {len(train_subset)} samples (augment={'ON' if AUGMENT else 'OFF'})\")\n",
        "print(f\"   Val:   {len(val_subset)} samples (augment=OFF, pure)\")\n",
        "print(f\"   Test:  {len(test_ds)} samples (augment=OFF, separate HF split)\")\n",
        "print(f\"   Board shape: {full_ds_no_aug.board_shape}\")\n",
        "if N_GENERATED > 0:\n",
        "    print(f\"   +{N_GENERATED} generated {GEN_DIFFICULTY} puzzles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83c\udfd7\ufe0f Create Model\n",
        "\n",
        "from src.pot.models.sokoban_solver import PoTSokobanSolver, HybridPoTSokobanSolver, BaselineSokobanSolver\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "H, W = full_ds_no_aug.board_shape\n",
        "seq_len = H * W\n",
        "\n",
        "# Controller kwargs for advanced controllers\n",
        "# Note: n_heads is passed separately to the model, not in controller_kwargs\n",
        "controller_kwargs = {\n",
        "    'd_ctrl': D_CTRL,\n",
        "    'max_depth': MAX_DEPTH,\n",
        "}\n",
        "\n",
        "# Injection kwargs for cross_attn mode\n",
        "injection_kwargs = {\n",
        "    'memory_size': INJECTION_MEMORY_SIZE,\n",
        "    'n_heads': INJECTION_N_HEADS,\n",
        "} if INJECTION_MODE == 'cross_attn' else None\n",
        "\n",
        "if MODEL_TYPE == \"pot\":\n",
        "    model = PoTSokobanSolver(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "        R=R,\n",
        "        controller_type=CONTROLLER_TYPE,\n",
        "        controller_kwargs=controller_kwargs,\n",
        "        max_depth=MAX_DEPTH,\n",
        "        board_height=H,\n",
        "        board_width=W,\n",
        "    )\n",
        "elif MODEL_TYPE == \"hybrid_pot\":\n",
        "    model = HybridPoTSokobanSolver(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        H_layers=H_LAYERS,\n",
        "        L_layers=L_LAYERS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "        H_cycles=H_CYCLES,\n",
        "        L_cycles=L_CYCLES,\n",
        "        T=T,\n",
        "        halt_max_steps=HALT_MAX_STEPS,\n",
        "        halt_exploration_prob=HALT_EXPLORATION_PROB,\n",
        "        allow_early_halt_eval=ALLOW_EARLY_HALT_EVAL,\n",
        "        hrm_grad_style=HRM_GRAD_STYLE,\n",
        "        controller_type=CONTROLLER_TYPE,\n",
        "        controller_kwargs=controller_kwargs,\n",
        "        injection_mode=INJECTION_MODE,\n",
        "        injection_kwargs=injection_kwargs,\n",
        "        board_height=H,\n",
        "        board_width=W,\n",
        "    )\n",
        "else:\n",
        "    # Baseline uses different signature (pure CNN, no transformer)\n",
        "    model = BaselineSokobanSolver(\n",
        "        n_filters=64,\n",
        "        n_layers=N_LAYERS,\n",
        "        d_hidden=D_MODEL,\n",
        "        dropout=DROPOUT,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\u2705 {MODEL_TYPE} ({CONTROLLER_TYPE}) model created\")\n",
        "print(f\"   Parameters: {n_params:,}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Board: {H}x{W} = {seq_len} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\ude80 Train Model\n",
        "# @markdown Supervised training with cross-entropy + Q-halt loss (identical to Sudoku)\n",
        "\n",
        "from src.training.sokoban_supervised import train_supervised\n",
        "import time\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    run_name = f\"{MODEL_TYPE}-{CONTROLLER_TYPE}-R{R}-H{H_CYCLES}L{L_CYCLES}\"\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=run_name,\n",
        "        config=CONFIG,\n",
        "    )\n",
        "\n",
        "print(f\"Training {MODEL_TYPE} ({CONTROLLER_TYPE})...\")\n",
        "print(f\"  Epochs: {EPOCHS}, LR: {LEARNING_RATE}, Batch: {BATCH_SIZE}\")\n",
        "print(f\"  PoT: R={R}, T={T}, H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "if CURRICULUM and curriculum_order is not None:\n",
        "    warmup_epochs = int(EPOCHS * CURRICULUM_WARMUP)\n",
        "    print(f\"  \ud83d\udcda Curriculum: {warmup_epochs} warmup epochs ({CURRICULUM_WARMUP:.0%})\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Curriculum learning: gradually reveal harder samples\n",
        "if CURRICULUM and curriculum_order is not None:\n",
        "    from src.training.sokoban_supervised import train_epoch, evaluate\n",
        "    from torch.utils.data import Sampler\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=LEARNING_RATE, \n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        betas=(BETA1, BETA2),\n",
        "    )\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    warmup_epochs = int(EPOCHS * CURRICULUM_WARMUP)\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        # Calculate how much of the curriculum to use\n",
        "        if epoch < warmup_epochs:\n",
        "            # Gradually increase from 30% to 100% during warmup\n",
        "            frac = 0.3 + 0.7 * (epoch / warmup_epochs)\n",
        "        else:\n",
        "            frac = 1.0\n",
        "        \n",
        "        n_samples = int(len(curriculum_order) * frac)\n",
        "        epoch_indices = curriculum_order[:n_samples]\n",
        "        \n",
        "        # Create epoch-specific loader with curriculum subset\n",
        "        epoch_subset = Subset(full_ds_aug, epoch_indices)\n",
        "        epoch_loader = DataLoader(epoch_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        \n",
        "        # Train epoch\n",
        "        train_metrics = train_epoch(model, epoch_loader, optimizer, device, epoch, use_pot=(MODEL_TYPE != \"baseline\"), grad_clip=GRAD_CLIP)\n",
        "        val_metrics = evaluate(model, val_loader, device, compute_solve=(epoch % 10 == 0))\n",
        "        \n",
        "        if val_metrics['action_acc'] > best_val_acc:\n",
        "            best_val_acc = val_metrics['action_acc']\n",
        "        \n",
        "        if epoch % 10 == 0 or epoch == EPOCHS - 1:\n",
        "            solve_str = f\", solve={val_metrics.get('solve_rate', 0):.2%}\" if 'solve_rate' in val_metrics else \"\"\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS}: loss={train_metrics['loss']:.4f}, act_acc={val_metrics['action_acc']:.2%}{solve_str}, samples={n_samples}\")\n",
        "        \n",
        "        if USE_WANDB:\n",
        "            log_dict = {\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_metrics['loss'],\n",
        "                'train_action_acc': train_metrics['action_acc'],  # Like cell_acc\n",
        "                'val_action_acc': val_metrics['action_acc'],      # Like cell_acc\n",
        "                'curriculum_frac': frac,\n",
        "            }\n",
        "            if 'solve_rate' in val_metrics:\n",
        "                log_dict['val_solve_rate'] = val_metrics['solve_rate']  # Like grid_acc\n",
        "            wandb.log(log_dict)\n",
        "    \n",
        "    results = {'best_val_acc': best_val_acc, 'final_solve_rate': val_metrics.get('solve_rate')}\n",
        "else:\n",
        "    # Standard training (no curriculum)\n",
        "    results = train_supervised(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=device,\n",
        "        epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        grad_clip=GRAD_CLIP,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        use_pot=(MODEL_TYPE != \"baseline\"),\n",
        "        wandb_log=USE_WANDB,\n",
        "        betas=(BETA1, BETA2),\n",
        "        lr_min_ratio=LR_MIN_RATIO,\n",
        "    )\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n\u2705 Training complete!\")\n",
        "print(f\"   Best val accuracy: {results['best_val_acc']:.2%}\")\n",
        "print(f\"   Training time: {train_time / 60:.1f} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udcc8 Evaluate on HuggingFace Test Set\n",
        "# @markdown Computes action_acc (like cell_acc) and solve_rate (like grid_acc)\n",
        "\n",
        "from src.training.sokoban_supervised import evaluate\n",
        "\n",
        "# Check for board size mismatch\n",
        "train_shape = full_ds_no_aug.board_shape\n",
        "test_shape = test_ds.board_shape\n",
        "\n",
        "if train_shape != test_shape:\n",
        "    print(f\"\u26a0\ufe0f WARNING: Board size mismatch!\")\n",
        "    print(f\"   Train: {train_shape}, Test: {test_shape}\")\n",
        "    print(f\"   HF test evaluation may not work correctly.\")\n",
        "    print(f\"   Use multi-difficulty evaluation with generated boards instead.\")\n",
        "    test_metrics = {'action_acc': 0.0, 'solve_rate': 0.0, 'loss': float('inf'), 'note': 'size_mismatch'}\n",
        "else:\n",
        "    # Compute both action_acc and solve_rate (like Sudoku's cell_acc and grid_acc)\n",
        "    test_metrics = evaluate(model, test_loader, device, compute_solve=True, solve_samples=200)\n",
        "    print(f\"\\n\ud83d\udcca HuggingFace Test Set Results:\")\n",
        "    print(f\"   Action Acc: {test_metrics['action_acc']:.2%} (like cell_acc)\")\n",
        "    print(f\"   Solve Rate: {test_metrics.get('solve_rate', 0):.2%} (like grid_acc)\")\n",
        "    print(f\"   Loss: {test_metrics['loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83c\udfaf Multi-Difficulty Evaluation\n",
        "# @markdown Evaluate on Simple (6\u00d76, 1 box) and Complex (10\u00d710, 2 boxes)\n",
        "# @markdown Reports both action_acc (like cell_acc) and solve_rate (like grid_acc)\n",
        "\n",
        "from src.data.sokoban_generator import SokobanGeneratedDataset\n",
        "\n",
        "difficulty_results = {}\n",
        "model_size = full_ds_no_aug.board_shape  # Size model was trained on\n",
        "\n",
        "for difficulty in EVAL_DIFFICULTIES:\n",
        "    print(f\"\\nGenerating {difficulty} test boards...\")\n",
        "    \n",
        "    eval_ds = SokobanGeneratedDataset(\n",
        "        difficulty=difficulty,\n",
        "        n_samples=EVAL_SAMPLES,\n",
        "        seed=1042,\n",
        "        augment=False,\n",
        "    )\n",
        "    \n",
        "    eval_h, eval_w = eval_ds.board_shape\n",
        "    \n",
        "    # If using PAD_TO_SIZE and eval is smaller, pad it\n",
        "    if PAD_TO_SIZE > 0 and (eval_h < model_size[0] or eval_w < model_size[1]):\n",
        "        print(f\"   \ud83d\udd32 Padding eval from {eval_ds.board_shape} \u2192 {model_size}\")\n",
        "        eval_ds = PaddedDataset(eval_ds, PAD_TO_SIZE)\n",
        "        eval_h, eval_w = eval_ds.board_shape\n",
        "    \n",
        "    eval_loader = DataLoader(eval_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # Handle different board sizes\n",
        "    if (eval_h, eval_w) != model_size:\n",
        "        print(f\"   \u26a0\ufe0f Board size mismatch: model={model_size}, eval={eval_ds.board_shape}\")\n",
        "        print(f\"   Skipping (model trained on fixed size)\")\n",
        "        difficulty_results[difficulty] = {'action_acc': None, 'solve_rate': None, 'note': 'size_mismatch'}\n",
        "        continue\n",
        "    \n",
        "    # Compute both action_acc and solve_rate (like Sudoku's cell_acc and grid_acc)\n",
        "    eval_metrics = evaluate(model, eval_loader, device, compute_solve=True, solve_samples=min(100, EVAL_SAMPLES))\n",
        "    difficulty_results[difficulty] = {\n",
        "        'action_acc': eval_metrics['action_acc'],  # Like cell_acc\n",
        "        'solve_rate': eval_metrics.get('solve_rate', 0),  # Like grid_acc\n",
        "        'loss': eval_metrics['loss'],\n",
        "    }\n",
        "    print(f\"   {difficulty}: action_acc={eval_metrics['action_acc']:.2%}, solve_rate={eval_metrics.get('solve_rate', 0):.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MULTI-DIFFICULTY RESULTS (like Sudoku's cell_acc / grid_acc)\")\n",
        "print(\"=\" * 60)\n",
        "for diff, res in difficulty_results.items():\n",
        "    if res.get('action_acc') is not None:\n",
        "        print(f\"  {diff}: action_acc={res['action_acc']:.2%}, solve_rate={res['solve_rate']:.2%}\")\n",
        "    else:\n",
        "        print(f\"  {diff}: N/A ({res.get('note', 'error')})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udccb Final Comparison Table\n",
        "\n",
        "# Get both action_acc (like cell_acc) and solve_rate (like grid_acc)\n",
        "simple_action_acc = difficulty_results.get('simple', {}).get('action_acc', 0) or 0\n",
        "simple_solve_rate = difficulty_results.get('simple', {}).get('solve_rate', 0) or 0\n",
        "complex_action_acc = difficulty_results.get('complex', {}).get('action_acc', 0) or 0\n",
        "complex_solve_rate = difficulty_results.get('complex', {}).get('solve_rate', 0) or 0\n",
        "\n",
        "print(\"\"\"\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551                         SOKOBAN BENCHMARK COMPARISON                                   \u2551\n",
        "\u2551                  (action_acc = like cell_acc, solve_rate = like grid_acc)              \u2551\n",
        "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
        "\u2551 Method                   \u2502 Simple (6\u00d76,1)      \u2502 Complex (10\u00d710,2)     \u2502 Notes          \u2551\n",
        "\u2551                          \u2502 act_acc / solve     \u2502 act_acc / solve       \u2502                \u2551\n",
        "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
        "\u2551 SFT (paper)              \u2502   ~50%  /  ~30%     \u2502   ~15%  /  ~5%        \u2502 Baseline       \u2551\n",
        "\u2551 GPT-4 + LangGraph        \u2502  varies / varies    \u2502  varies / varies      \u2502 Zero-shot      \u2551\n",
        "\u2551 RL (PPO)                 \u2502   ~20%  /  ~10%     \u2502   <5%   /  <1%        \u2502 Very hard      \u2551\n",
        "\u2551 Random                   \u2502   25%   /   ~1%     \u2502   25%   /  <0.1%      \u2502 4 actions      \u2551\n",
        "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\"\"\")\n",
        "\n",
        "print(f\"\u2551 PoT (this run)           \u2502  {simple_action_acc:5.1%} / {simple_solve_rate:5.1%}    \u2502  {complex_action_acc:5.1%} / {complex_solve_rate:5.1%}     \u2502 R={R}            \u2551\")\n",
        "print(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n",
        "\n",
        "# Log final results to W&B\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    wandb.log({\n",
        "        'final/val_action_acc': results['best_val_acc'],\n",
        "        'final/val_solve_rate': results.get('final_solve_rate', 0) or 0,\n",
        "        'final/hf_test_action_acc': test_metrics['action_acc'],\n",
        "        'final/hf_test_solve_rate': test_metrics.get('solve_rate', 0) or 0,\n",
        "        'final/simple_action_acc': simple_action_acc,\n",
        "        'final/simple_solve_rate': simple_solve_rate,\n",
        "        'final/complex_action_acc': complex_action_acc,\n",
        "        'final/complex_solve_rate': complex_solve_rate,\n",
        "        'final/train_time_min': train_time / 60,\n",
        "        'final/n_params': n_params,\n",
        "    })\n",
        "    # Log summary metrics\n",
        "    wandb.run.summary['best_val_action_acc'] = results['best_val_acc']\n",
        "    wandb.run.summary['simple_action_acc'] = simple_action_acc\n",
        "    wandb.run.summary['simple_solve_rate'] = simple_solve_rate\n",
        "    wandb.run.summary['complex_action_acc'] = complex_action_acc\n",
        "    wandb.run.summary['complex_solve_rate'] = complex_solve_rate\n",
        "    wandb.run.summary['n_params'] = n_params\n",
        "    print(f\"\\n\ud83d\udcca Results logged to W&B: {wandb.run.url}\")\n",
        "\n",
        "# Store ALL config + results for Optuna/hyperparameter search\n",
        "COLAB_RESULTS = {\n",
        "    # Full configuration (for Optuna)\n",
        "    **CONFIG,\n",
        "    # Results (using Sudoku-style naming)\n",
        "    'best_val_action_acc': results['best_val_acc'],  # Like cell_acc\n",
        "    'best_val_solve_rate': results.get('final_solve_rate'),  # Like grid_acc\n",
        "    'hf_test_action_acc': test_metrics['action_acc'],\n",
        "    'hf_test_solve_rate': test_metrics.get('solve_rate'),\n",
        "    'simple_action_acc': simple_action_acc,\n",
        "    'simple_solve_rate': simple_solve_rate,\n",
        "    'complex_action_acc': complex_action_acc,\n",
        "    'complex_solve_rate': complex_solve_rate,\n",
        "    'train_time_min': train_time / 60,\n",
        "    'n_params': n_params,\n",
        "    'difficulty_results': difficulty_results,\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Full Results (for Optuna search):\")\n",
        "print(f\"   Model: {MODEL_TYPE} ({CONTROLLER_TYPE})\")\n",
        "print(f\"   Architecture: d={D_MODEL}, ff={D_FF}, heads={N_HEADS}\")\n",
        "print(f\"   PoT: R={R}, T={T}, H_cycles={H_CYCLES}, L_cycles={L_CYCLES}\")\n",
        "print(f\"   ACT: halt_max={HALT_MAX_STEPS}, max_depth={MAX_DEPTH}\")\n",
        "print(f\"   Val Action Acc: {results['best_val_acc']:.2%} (like cell_acc)\")\n",
        "print(f\"   Val Solve Rate: {results.get('final_solve_rate', 0) or 0:.2%} (like grid_acc)\")\n",
        "print(f\"   Test Action Acc: {test_metrics['action_acc']:.2%}\")\n",
        "print(f\"   Test Solve Rate: {test_metrics.get('solve_rate', 0) or 0:.2%}\")\n",
        "print(f\"   Simple: action={simple_action_acc:.2%}, solve={simple_solve_rate:.2%}\")\n",
        "print(f\"   Complex: action={complex_action_acc:.2%}, solve={complex_solve_rate:.2%}\")\n",
        "print(f\"   Params: {n_params:,}\")\n",
        "print(f\"   Time: {train_time / 60:.1f} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title \ud83d\udcbe Save Model & Results (Optional)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Save model checkpoint\n",
        "model_name = f\"sokoban_{MODEL_TYPE}_{CONTROLLER_TYPE}_R{R}_H{H_CYCLES}L{L_CYCLES}\"\n",
        "save_path = f\"checkpoints/{model_name}.pt\"\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'results': COLAB_RESULTS,\n",
        "}, save_path)\n",
        "\n",
        "print(f\"\u2705 Model saved to {save_path}\")\n",
        "\n",
        "# Save results JSON (for Optuna aggregation)\n",
        "results_path = f\"checkpoints/{model_name}_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    # Convert non-serializable items\n",
        "    results_to_save = {k: v for k, v in COLAB_RESULTS.items() \n",
        "                       if not isinstance(v, dict) or k == 'difficulty_results'}\n",
        "    json.dump(results_to_save, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\u2705 Results saved to {results_path}\")\n",
        "\n",
        "# Print command to download\n",
        "print(f\"\\n\ud83d\udce5 Download command:\")\n",
        "print(f\"   from google.colab import files\")\n",
        "print(f\"   files.download('{save_path}')\")\n",
        "print(f\"   files.download('{results_path}')\")\n",
        "\n",
        "# Finish W&B run\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    wandb.save(save_path)  # Save model to W&B\n",
        "    wandb.save(results_path)  # Save results JSON to W&B\n",
        "    wandb.finish()\n",
        "    print(f\"\\n\u2705 W&B run finished and artifacts uploaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcdd Notes\n",
        "\n",
        "### Metrics (Identical to Sudoku)\n",
        "\n",
        "| Sudoku | Sokoban | Description |\n",
        "|--------|---------|-------------|\n",
        "| `cell_acc` | `action_acc` | % of single predictions correct |\n",
        "| `grid_acc` | `solve_rate` | % of puzzles fully solved |\n",
        "\n",
        "**Note:** `solve_rate` runs full rollouts (model plays until solved/stuck), so it's slower to compute.\n",
        "\n",
        "### Loss Function (Identical to Sudoku - 3 losses)\n",
        "```python\n",
        "# LOSS 1: Main task (cross-entropy on action)\n",
        "ce_loss = cross_entropy(action_logits, action_label)\n",
        "\n",
        "# LOSS 2: Q-halt (should I stop iterating?)\n",
        "q_halt_loss = bce(q_halt, is_correct)\n",
        "\n",
        "# LOSS 3: Q-continue (ACT Q-learning)\n",
        "q_continue_loss = mse(sigmoid(q_continue), target_q_continue)\n",
        "\n",
        "# Combined\n",
        "loss = ce_loss + 0.5 * q_halt_loss + 0.5 * q_continue_loss\n",
        "```\n",
        "\n",
        "### Configuration Parameters for Optuna Search\n",
        "\n",
        "| Parameter | Description | Typical Range |\n",
        "|-----------|-------------|---------------|\n",
        "| **Architecture** | | |\n",
        "| `D_MODEL` | Hidden dimension | 64-512 |\n",
        "| `D_FF` | Feedforward dimension | 128-2048 |\n",
        "| `N_HEADS` | Attention heads | 2-16 |\n",
        "| `N_LAYERS` | Transformer layers | 1-8 |\n",
        "| **PoT Iterations** | | |\n",
        "| `R` | Refinement iterations | 1-16 |\n",
        "| `T` | HRM period | 1-8 |\n",
        "| **Hybrid H/L Cycles** | | |\n",
        "| `H_CYCLES` | High-level (slow) cycles | 1-8 |\n",
        "| `L_CYCLES` | Low-level (fast) cycles | 1-16 |\n",
        "| `H_LAYERS` | Layers in H-level | 1-4 |\n",
        "| `L_LAYERS` | Layers in L-level | 1-4 |\n",
        "| **Controller** | | |\n",
        "| `D_CTRL` | Controller hidden dimension | 32-256 |\n",
        "| `MAX_DEPTH` | Max controller depth | 32-256 |\n",
        "| **Feature Injection** | | |\n",
        "| `INJECTION_MODE` | Injection mode | none/broadcast/film/etc |\n",
        "| `INJECTION_MEMORY_SIZE` | Memory size (cross_attn) | 4-32 |\n",
        "| `INJECTION_N_HEADS` | Heads (cross_attn) | 2-8 |\n",
        "| **ACT (Adaptive Computation)** | | |\n",
        "| `HALT_MAX_STEPS` | Max halting steps | 1-16 |\n",
        "| `HALT_EXPLORATION_PROB` | Exploration probability | 0.0-1.0 |\n",
        "| **Controller Types** | | |\n",
        "| `transformer` | CausalDepthTransformerRouter | Default |\n",
        "| `gru` | GRU-based controller | Fast |\n",
        "| `lstm` | LSTM-based controller | |\n",
        "| `diffusion` | Diffusion denoising | DiT-style |\n",
        "| `swin` | Swin Transformer | Vision |\n",
        "| `mamba` | Mamba SSM | State-space |\n",
        "| **Data Generation** | | |\n",
        "| `N_GENERATED` | Extra puzzles to generate | 0-100000 |\n",
        "| `GEN_DIFFICULTY` | Difficulty of generated | simple/larger/two_boxes/complex |\n",
        "| **Curriculum Learning** | | |\n",
        "| `CURRICULUM` | Enable curriculum (easy\u2192hard) | True/False |\n",
        "| `CURRICULUM_WARMUP` | Fraction of epochs for warmup | 0.1-0.5 |\n",
        "| **Size Generalization** | | |\n",
        "| `PAD_TO_SIZE` | Pad boards to NxN (0=off) | 0, 10, 12, 16 |\n",
        "\n",
        "### Difficulty Levels\n",
        "| Difficulty | Size | Boxes | Avg Solution Length |\n",
        "|------------|------|-------|---------------------|\n",
        "| simple | 6\u00d76 | 1 | ~4 moves |\n",
        "| larger | 10\u00d710 | 1 | ~15 moves |\n",
        "| two_boxes | 6\u00d76 | 2 | ~20 moves |\n",
        "| complex | 10\u00d710 | 2 | ~18 moves |\n",
        "\n",
        "### Example Optuna Study\n",
        "```python\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    R = trial.suggest_int('R', 1, 16)\n",
        "    H_CYCLES = trial.suggest_int('H_cycles', 1, 8)\n",
        "    L_CYCLES = trial.suggest_int('L_cycles', 1, 16)\n",
        "    D_MODEL = trial.suggest_categorical('d_model', [128, 256, 512])\n",
        "    # ... train and return complex_acc\n",
        "    return complex_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "```\n",
        "\n",
        "### References\n",
        "- [Debunking SFT Generalization](https://arxiv.org/pdf/2510.00237)\n",
        "- [LLMs + LangGraph for Sokoban](https://blog.gopenai.com/using-llms-and-langgraph-to-tackle-sokoban-puzzles-5f50b43b9515)\n",
        "- [HuggingFace Dataset](https://huggingface.co/datasets/Xiaofeng77/sokoban)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}