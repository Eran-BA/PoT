{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wm_Xoc0a7O0"
      },
      "source": [
        "# ðŸŒŠ Diffusion Sudoku Training (Iterative Denoising)\n",
        "\n",
        "Train a Sudoku solver using the **Diffusion Depth Controller** with iterative denoising.\n",
        "\n",
        "**Key Features:**\n",
        "- Iterative denoising of routing weights\n",
        "- Learned noise schedules (linear, cosine, sqrt)\n",
        "- Adaptive LayerNorm (adaLN) conditioning from DiT\n",
        "- Smooth, temporally coherent routing evolution\n",
        "\n",
        "**Reference:** [Scalable Diffusion Models with Transformers (DiT)](https://arxiv.org/abs/2212.09748)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9k9kw3la7O1",
        "outputId": "b703e734-9e1d-42e4-b6f2-59add6e454e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PoT'...\n",
            "remote: Enumerating objects: 2637, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 2637 (delta 110), reused 107 (delta 58), pack-reused 2471 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2637/2637), 2.42 MiB | 23.11 MiB/s, done.\n",
            "Resolving deltas: 100% (1667/1667), done.\n",
            "/content/PoT\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Clone repository and install dependencies\n",
        "!git clone https://github.com/Eran-BA/PoT.git\n",
        "%cd PoT\n",
        "!pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AEh7tUCa7O1",
        "outputId": "8c4c80de-6fe9-424a-f86a-13fe3ea7581b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory: 42.5 GB\n",
            "\n",
            "Diffusion controller available: âœ“\n",
            "Controller params: 2,567,176\n",
            "Initial sigma (noise): 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Check GPU and verify Diffusion controller\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "from src.pot.core import DiffusionDepthController, create_controller\n",
        "print(\"\\nDiffusion controller available: âœ“\")\n",
        "\n",
        "# Quick test\n",
        "controller = create_controller(\"diffusion\", d_model=256, n_heads=8, noise_schedule=\"cosine\")\n",
        "print(f\"Controller params: {sum(p.numel() for p in controller.parameters()):,}\")\n",
        "\n",
        "# Test denoising step\n",
        "X = torch.randn(2, 81, 256)\n",
        "alpha, state, aux = controller.step(X)\n",
        "print(f\"Initial sigma (noise): {aux['sigma']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VQIO1Arcn2w",
        "outputId": "2692e4f3-9076-4bf4-cf79-d02882bc1156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From https://github.com/Eran-BA/PoT\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flUssPU8c5vz",
        "outputId": "dd8b9052-3cf9-4f3c-ef56-36cc12bfe064"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meranbt92\u001b[0m (\u001b[33meranbt92-open-university-of-israel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# W&B Login\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypagEEaEa7O1"
      },
      "source": [
        "## ðŸš€ Train with Diffusion Controller\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN6v0Y2va7O1",
        "outputId": "b29450d4-94cb-43a5-92b2-adebb1009fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Downloading Sudoku-Extreme dataset from HuggingFace...\n",
            "Note: Augmentation is now ON-THE-FLY (not pre-computed)\n",
            "train.csv: 100% 719M/719M [00:02<00:00, 308MB/s]\n",
            "  Train puzzles: 9000, Val puzzles: 1000\n",
            "Processing train: 100% 9000/9000 [00:00<00:00, 35093.92it/s]\n",
            "  train: 9000 puzzles (augmentation: on-the-fly)\n",
            "Processing val: 100% 1000/1000 [00:00<00:00, 35173.54it/s]\n",
            "  val: 1000 puzzles (augmentation: on-the-fly)\n",
            "test.csv: 100% 79.4M/79.4M [00:00<00:00, 95.4MB/s]\n",
            "Processing test: 422786it [00:12, 33213.29it/s]\n",
            "  test: 422786 puzzles\n",
            "âœ“ Dataset saved to data/sudoku-extreme-10k-aug-100\n",
            "[train] Loaded 9000 puzzles\n",
            "  Augmentation: ON-THE-FLY\n",
            "[val] Loaded 1000 puzzles\n",
            "  Augmentation: OFF\n",
            "Using VAL split (held-out training puzzles) for evaluation\n",
            "Hybrid model: H_cycles=2, L_cycles=6\n",
            "H_layers=2, L_layers=2, dropout=0.0\n",
            "Controller: diffusion\n",
            "Gradient style: HRM (last L+H only)\n",
            "ACT enabled: halt_max_steps=4, exploration=0.1\n",
            "\n",
            "Model: HYBRID\n",
            "Parameters: 37,335,580 (37.34M)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meranbt92\u001b[0m (\u001b[33meranbt92-open-university-of-israel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run 1j3u5yrf (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 1j3u5yrf (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run 1j3u5yrf (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m setting up run 1j3u5yrf (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/PoT/wandb/run-20251227_201712-1j3u5yrf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhybrid_diffusion_20251227_201712\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/eranbt92-open-university-of-israel/sudoku-diffusion\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/eranbt92-open-university-of-israel/sudoku-diffusion/runs/1j3u5yrf\u001b[0m\n",
            "W&B logging enabled: sudoku-diffusion/hybrid_diffusion_20251227_201712\n",
            "\n",
            "Optimizer: ADAMW (betas=(0.9, 0.95))\n",
            "  Model params: 37,335,068, lr=0.0003, wd=0.01\n",
            "Puzzle Optimizer: ADAMW\n",
            "  Puzzle params: 512, lr=0.0003 (1.0x), wd=0.01\n",
            "\n",
            "============================================================\n",
            "Training HYBRID Sudoku Solver\n",
            "============================================================\n",
            "Epochs: 500, Batch size: 128\n",
            "Optimizer: adamw, Puzzle optimizer: adamw\n",
            "LR: 0.0003, Weight decay: 0.01, Betas: (0.9, 0.95)\n",
            "Warmup: 500 steps, LR min ratio: 0.1\n",
            "Total steps: 35500\n",
            "R=8, T=4, max_halt=16\n",
            "Epoch 1: 100% 71/71 [01:12<00:00,  1.02s/it, loss=1.5205, cell_acc=30.7%, grid_acc=0.0%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 1/500\n",
            "  Train: Loss=1.8662, Cell=30.73%, Grid=0.00%\n",
            "  Test:  Loss=1.5147, Cell=40.08%, Grid=0.00%\n",
            "Epoch 2: 100% 71/71 [01:11<00:00,  1.01s/it, loss=1.5122, cell_acc=40.5%, grid_acc=0.0%]\n",
            "Epoch 3: 100% 71/71 [01:10<00:00,  1.00it/s, loss=1.4982, cell_acc=40.9%, grid_acc=0.0%]\n",
            "Epoch 4: 100% 71/71 [01:10<00:00,  1.00it/s, loss=1.2590, cell_acc=43.6%, grid_acc=0.0%]\n",
            "Epoch 5: 100% 71/71 [01:11<00:00,  1.00s/it, loss=1.0109, cell_acc=51.6%, grid_acc=0.0%]\n",
            "Epoch 6: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.8954, cell_acc=57.0%, grid_acc=0.0%]\n",
            "Epoch 7: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8689, cell_acc=59.2%, grid_acc=0.0%]\n",
            "Epoch 8: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8547, cell_acc=60.9%, grid_acc=0.0%]\n",
            "Epoch 9: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8266, cell_acc=61.9%, grid_acc=0.0%]\n",
            "Epoch 10: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8173, cell_acc=62.5%, grid_acc=0.0%]\n",
            "Epoch 11: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8252, cell_acc=63.0%, grid_acc=0.0%]\n",
            "Epoch 12: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7987, cell_acc=63.3%, grid_acc=0.1%]\n",
            "Epoch 13: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7985, cell_acc=63.5%, grid_acc=0.2%]\n",
            "Epoch 14: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.8103, cell_acc=63.8%, grid_acc=0.3%]\n",
            "Epoch 15: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.8309, cell_acc=64.0%, grid_acc=0.4%]\n",
            "Epoch 16: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7918, cell_acc=64.3%, grid_acc=0.4%]\n",
            "Epoch 17: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7636, cell_acc=64.4%, grid_acc=0.8%]\n",
            "Epoch 18: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7694, cell_acc=65.1%, grid_acc=1.1%]\n",
            "Epoch 19: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7320, cell_acc=65.5%, grid_acc=1.3%]\n",
            "Epoch 20: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7801, cell_acc=65.7%, grid_acc=1.8%]\n",
            "Epoch 21: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7320, cell_acc=66.1%, grid_acc=1.8%]\n",
            "Epoch 22: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7692, cell_acc=66.4%, grid_acc=2.0%]\n",
            "Epoch 23: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7370, cell_acc=66.6%, grid_acc=2.0%]\n",
            "Epoch 24: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7576, cell_acc=66.8%, grid_acc=2.0%]\n",
            "Epoch 25: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7304, cell_acc=66.8%, grid_acc=1.3%]\n",
            "Epoch 26: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6929, cell_acc=67.0%, grid_acc=1.7%]\n",
            "Epoch 27: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7542, cell_acc=67.2%, grid_acc=2.0%]\n",
            "Epoch 28: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6721, cell_acc=67.3%, grid_acc=2.0%]\n",
            "Epoch 29: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6346, cell_acc=67.5%, grid_acc=2.1%]\n",
            "Epoch 30: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7346, cell_acc=67.5%, grid_acc=2.1%]\n",
            "Epoch 31: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6700, cell_acc=67.5%, grid_acc=2.1%]\n",
            "Epoch 32: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6963, cell_acc=67.5%, grid_acc=2.0%]\n",
            "Epoch 33: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6889, cell_acc=67.7%, grid_acc=2.0%]\n",
            "Epoch 34: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7405, cell_acc=67.6%, grid_acc=2.0%]\n",
            "Epoch 35: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7604, cell_acc=67.7%, grid_acc=2.1%]\n",
            "Epoch 36: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6846, cell_acc=67.7%, grid_acc=1.9%]\n",
            "Epoch 37: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7832, cell_acc=67.9%, grid_acc=2.1%]\n",
            "Epoch 38: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7467, cell_acc=67.7%, grid_acc=2.2%]\n",
            "Epoch 39: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6995, cell_acc=67.7%, grid_acc=1.1%]\n",
            "Epoch 40: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6661, cell_acc=67.5%, grid_acc=1.5%]\n",
            "Epoch 41: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7249, cell_acc=67.8%, grid_acc=2.1%]\n",
            "Epoch 42: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7014, cell_acc=67.9%, grid_acc=2.2%]\n",
            "Epoch 43: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7327, cell_acc=67.9%, grid_acc=2.1%]\n",
            "Epoch 44: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6962, cell_acc=67.9%, grid_acc=2.1%]\n",
            "Epoch 45: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6927, cell_acc=68.1%, grid_acc=2.2%]\n",
            "Epoch 46: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6829, cell_acc=68.0%, grid_acc=1.4%]\n",
            "Epoch 47: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6963, cell_acc=68.2%, grid_acc=2.1%]\n",
            "Epoch 48: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7227, cell_acc=68.1%, grid_acc=1.7%]\n",
            "Epoch 49: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7355, cell_acc=68.2%, grid_acc=2.2%]\n",
            "Epoch 50: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7582, cell_acc=68.4%, grid_acc=2.3%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 50/500\n",
            "  Train: Loss=0.7028, Cell=68.41%, Grid=2.31%\n",
            "  Test:  Loss=0.6954, Cell=68.29%, Grid=2.30%\n",
            "  âœ“ New best: 2.30%\n",
            "Epoch 51: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7100, cell_acc=68.2%, grid_acc=2.1%]\n",
            "Epoch 52: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7211, cell_acc=68.3%, grid_acc=2.3%]\n",
            "Epoch 53: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7733, cell_acc=68.4%, grid_acc=2.4%]\n",
            "Epoch 54: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6462, cell_acc=68.3%, grid_acc=2.4%]\n",
            "Epoch 55: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6109, cell_acc=68.3%, grid_acc=2.4%]\n",
            "Epoch 56: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6555, cell_acc=68.4%, grid_acc=2.4%]\n",
            "Epoch 57: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7122, cell_acc=68.2%, grid_acc=2.4%]\n",
            "Epoch 58: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7228, cell_acc=68.3%, grid_acc=2.6%]\n",
            "Epoch 59: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7361, cell_acc=68.5%, grid_acc=2.6%]\n",
            "Epoch 60: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6516, cell_acc=68.3%, grid_acc=2.5%]\n",
            "Epoch 61: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7797, cell_acc=68.4%, grid_acc=2.3%]\n",
            "Epoch 62: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7314, cell_acc=68.5%, grid_acc=2.5%]\n",
            "Epoch 63: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7090, cell_acc=68.2%, grid_acc=2.3%]\n",
            "Epoch 64: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7606, cell_acc=68.3%, grid_acc=2.5%]\n",
            "Epoch 65: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6830, cell_acc=68.4%, grid_acc=2.4%]\n",
            "Epoch 66: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6712, cell_acc=68.5%, grid_acc=2.5%]\n",
            "Epoch 67: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6815, cell_acc=68.4%, grid_acc=2.2%]\n",
            "Epoch 68: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7555, cell_acc=68.5%, grid_acc=2.0%]\n",
            "Epoch 69: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7569, cell_acc=68.3%, grid_acc=1.7%]\n",
            "Epoch 70: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7321, cell_acc=68.4%, grid_acc=2.5%]\n",
            "Epoch 71: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6932, cell_acc=68.4%, grid_acc=2.5%]\n",
            "Epoch 72: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6595, cell_acc=68.3%, grid_acc=2.5%]\n",
            "Epoch 73: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6859, cell_acc=68.5%, grid_acc=2.5%]\n",
            "Epoch 74: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7010, cell_acc=68.5%, grid_acc=2.6%]\n",
            "Epoch 75: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7159, cell_acc=68.5%, grid_acc=2.4%]\n",
            "Epoch 76: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6911, cell_acc=68.5%, grid_acc=2.6%]\n",
            "Epoch 77: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7201, cell_acc=68.7%, grid_acc=2.8%]\n",
            "Epoch 78: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7325, cell_acc=68.6%, grid_acc=2.7%]\n",
            "Epoch 79: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7271, cell_acc=68.6%, grid_acc=2.7%]\n",
            "Epoch 80: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6858, cell_acc=68.8%, grid_acc=2.9%]\n",
            "Epoch 81: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7495, cell_acc=68.7%, grid_acc=2.7%]\n",
            "Epoch 82: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7444, cell_acc=68.7%, grid_acc=2.9%]\n",
            "Epoch 83: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6959, cell_acc=68.7%, grid_acc=2.7%]\n",
            "Epoch 84: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6507, cell_acc=68.7%, grid_acc=2.6%]\n",
            "Epoch 85: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7803, cell_acc=68.7%, grid_acc=2.6%]\n",
            "Epoch 86: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6695, cell_acc=68.8%, grid_acc=2.9%]\n",
            "Epoch 87: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7339, cell_acc=69.0%, grid_acc=3.0%]\n",
            "Epoch 88: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7282, cell_acc=69.0%, grid_acc=2.6%]\n",
            "Epoch 89: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6465, cell_acc=68.8%, grid_acc=2.6%]\n",
            "Epoch 90: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7713, cell_acc=68.6%, grid_acc=2.5%]\n",
            "Epoch 91: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7301, cell_acc=68.7%, grid_acc=1.5%]\n",
            "Epoch 92: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7377, cell_acc=69.0%, grid_acc=2.7%]\n",
            "Epoch 93: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6507, cell_acc=69.1%, grid_acc=2.8%]\n",
            "Epoch 94: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6648, cell_acc=69.3%, grid_acc=3.1%]\n",
            "Epoch 95: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7112, cell_acc=69.2%, grid_acc=3.0%]\n",
            "Epoch 96: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6912, cell_acc=69.3%, grid_acc=3.0%]\n",
            "Epoch 97: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6530, cell_acc=68.8%, grid_acc=3.0%]\n",
            "Epoch 98: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7191, cell_acc=69.4%, grid_acc=3.0%]\n",
            "Epoch 99: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6855, cell_acc=69.3%, grid_acc=2.9%]\n",
            "Epoch 100: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6817, cell_acc=69.4%, grid_acc=3.1%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 100/500\n",
            "  Train: Loss=0.6905, Cell=69.39%, Grid=3.11%\n",
            "  Test:  Loss=0.6758, Cell=69.46%, Grid=3.50%\n",
            "  âœ“ New best: 3.50%\n",
            "Epoch 101: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6582, cell_acc=69.3%, grid_acc=2.9%]\n",
            "Epoch 102: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6710, cell_acc=69.4%, grid_acc=3.1%]\n",
            "Epoch 103: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7160, cell_acc=69.4%, grid_acc=3.0%]\n",
            "Epoch 104: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6457, cell_acc=69.3%, grid_acc=3.0%]\n",
            "Epoch 105: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7731, cell_acc=69.5%, grid_acc=3.0%]\n",
            "Epoch 106: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6426, cell_acc=69.6%, grid_acc=3.2%]\n",
            "Epoch 107: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6453, cell_acc=69.6%, grid_acc=3.0%]\n",
            "Epoch 108: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6525, cell_acc=69.8%, grid_acc=3.3%]\n",
            "Epoch 109: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7071, cell_acc=69.6%, grid_acc=2.4%]\n",
            "Epoch 110: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5870, cell_acc=69.5%, grid_acc=2.8%]\n",
            "Epoch 111: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6712, cell_acc=69.6%, grid_acc=2.6%]\n",
            "Epoch 112: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7215, cell_acc=69.7%, grid_acc=2.9%]\n",
            "Epoch 113: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7071, cell_acc=69.8%, grid_acc=2.9%]\n",
            "Epoch 114: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6162, cell_acc=69.8%, grid_acc=2.7%]\n",
            "Epoch 115: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6770, cell_acc=69.8%, grid_acc=3.0%]\n",
            "Epoch 116: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6809, cell_acc=69.9%, grid_acc=3.0%]\n",
            "Epoch 117: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6748, cell_acc=69.9%, grid_acc=2.9%]\n",
            "Epoch 118: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7288, cell_acc=69.7%, grid_acc=2.2%]\n",
            "Epoch 119: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7124, cell_acc=70.2%, grid_acc=2.5%]\n",
            "Epoch 120: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6844, cell_acc=70.1%, grid_acc=3.0%]\n",
            "Epoch 121: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6139, cell_acc=70.2%, grid_acc=3.0%]\n",
            "Epoch 122: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7098, cell_acc=70.3%, grid_acc=3.0%]\n",
            "Epoch 123: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6142, cell_acc=70.2%, grid_acc=3.1%]\n",
            "Epoch 124: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6563, cell_acc=70.5%, grid_acc=3.3%]\n",
            "Epoch 125: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6378, cell_acc=70.0%, grid_acc=2.8%]\n",
            "Epoch 126: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6982, cell_acc=70.1%, grid_acc=2.9%]\n",
            "Epoch 127: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6812, cell_acc=70.4%, grid_acc=3.2%]\n",
            "Epoch 128: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6943, cell_acc=70.3%, grid_acc=2.9%]\n",
            "Epoch 129: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6966, cell_acc=70.4%, grid_acc=3.2%]\n",
            "Epoch 130: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6552, cell_acc=70.4%, grid_acc=2.9%]\n",
            "Epoch 131: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6252, cell_acc=70.7%, grid_acc=3.6%]\n",
            "Epoch 132: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6235, cell_acc=70.8%, grid_acc=3.3%]\n",
            "Epoch 133: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6135, cell_acc=70.5%, grid_acc=3.2%]\n",
            "Epoch 134: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6604, cell_acc=70.5%, grid_acc=3.1%]\n",
            "Epoch 135: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6277, cell_acc=70.4%, grid_acc=1.6%]\n",
            "Epoch 136: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6399, cell_acc=71.0%, grid_acc=3.3%]\n",
            "Epoch 137: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7176, cell_acc=70.5%, grid_acc=3.2%]\n",
            "Epoch 138: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6566, cell_acc=71.1%, grid_acc=3.7%]\n",
            "Epoch 139: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6836, cell_acc=71.0%, grid_acc=3.5%]\n",
            "Epoch 140: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6474, cell_acc=71.0%, grid_acc=3.5%]\n",
            "Epoch 141: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6908, cell_acc=71.0%, grid_acc=3.4%]\n",
            "Epoch 142: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6832, cell_acc=70.8%, grid_acc=3.0%]\n",
            "Epoch 143: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6354, cell_acc=70.8%, grid_acc=3.2%]\n",
            "Epoch 144: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6677, cell_acc=71.0%, grid_acc=3.3%]\n",
            "Epoch 145: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.6858, cell_acc=71.2%, grid_acc=3.6%]\n",
            "Epoch 146: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6222, cell_acc=71.1%, grid_acc=2.9%]\n",
            "Epoch 147: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7202, cell_acc=71.1%, grid_acc=3.4%]\n",
            "Epoch 148: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5957, cell_acc=71.4%, grid_acc=3.5%]\n",
            "Epoch 149: 100% 71/71 [01:10<00:00,  1.00it/s, loss=0.7022, cell_acc=71.3%, grid_acc=3.6%]\n",
            "Epoch 150: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6425, cell_acc=71.2%, grid_acc=3.3%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 150/500\n",
            "  Train: Loss=0.6538, Cell=71.16%, Grid=3.33%\n",
            "  Test:  Loss=0.6322, Cell=71.56%, Grid=4.10%\n",
            "  âœ“ New best: 4.10%\n",
            "Epoch 151: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6223, cell_acc=71.5%, grid_acc=4.2%]\n",
            "Epoch 152: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6934, cell_acc=71.4%, grid_acc=3.7%]\n",
            "Epoch 153: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6142, cell_acc=71.4%, grid_acc=3.6%]\n",
            "Epoch 154: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6141, cell_acc=71.7%, grid_acc=4.5%]\n",
            "Epoch 155: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6345, cell_acc=71.5%, grid_acc=4.5%]\n",
            "Epoch 156: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6014, cell_acc=71.6%, grid_acc=4.1%]\n",
            "Epoch 157: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6672, cell_acc=71.8%, grid_acc=4.5%]\n",
            "Epoch 158: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6697, cell_acc=71.8%, grid_acc=4.2%]\n",
            "Epoch 159: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6339, cell_acc=71.6%, grid_acc=4.4%]\n",
            "Epoch 160: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6773, cell_acc=71.6%, grid_acc=4.2%]\n",
            "Epoch 161: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6234, cell_acc=71.8%, grid_acc=4.5%]\n",
            "Epoch 162: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6938, cell_acc=71.8%, grid_acc=4.2%]\n",
            "Epoch 163: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6614, cell_acc=71.9%, grid_acc=4.5%]\n",
            "Epoch 164: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6351, cell_acc=71.9%, grid_acc=4.5%]\n",
            "Epoch 165: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6784, cell_acc=72.0%, grid_acc=4.5%]\n",
            "Epoch 166: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6685, cell_acc=71.7%, grid_acc=4.5%]\n",
            "Epoch 167: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7070, cell_acc=72.1%, grid_acc=4.6%]\n",
            "Epoch 168: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5910, cell_acc=72.0%, grid_acc=4.1%]\n",
            "Epoch 169: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.7199, cell_acc=72.0%, grid_acc=4.6%]\n",
            "Epoch 170: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6926, cell_acc=72.2%, grid_acc=4.9%]\n",
            "Epoch 171: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5704, cell_acc=72.3%, grid_acc=4.6%]\n",
            "Epoch 172: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6533, cell_acc=72.4%, grid_acc=4.6%]\n",
            "Epoch 173: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6245, cell_acc=72.3%, grid_acc=4.8%]\n",
            "Epoch 174: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6646, cell_acc=72.5%, grid_acc=5.6%]\n",
            "Epoch 175: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6625, cell_acc=72.3%, grid_acc=5.0%]\n",
            "Epoch 176: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5859, cell_acc=72.4%, grid_acc=5.2%]\n",
            "Epoch 177: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6287, cell_acc=72.5%, grid_acc=5.4%]\n",
            "Epoch 178: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6408, cell_acc=72.5%, grid_acc=4.8%]\n",
            "Epoch 179: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6740, cell_acc=72.3%, grid_acc=4.9%]\n",
            "Epoch 180: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5435, cell_acc=72.3%, grid_acc=4.9%]\n",
            "Epoch 181: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6496, cell_acc=72.4%, grid_acc=4.2%]\n",
            "Epoch 182: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6873, cell_acc=72.5%, grid_acc=4.9%]\n",
            "Epoch 183: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6746, cell_acc=72.6%, grid_acc=5.2%]\n",
            "Epoch 184: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6399, cell_acc=72.7%, grid_acc=4.8%]\n",
            "Epoch 185: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5723, cell_acc=72.9%, grid_acc=5.4%]\n",
            "Epoch 186: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5964, cell_acc=72.7%, grid_acc=5.5%]\n",
            "Epoch 187: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6709, cell_acc=72.8%, grid_acc=5.2%]\n",
            "Epoch 188: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6029, cell_acc=73.1%, grid_acc=5.9%]\n",
            "Epoch 189: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6254, cell_acc=73.0%, grid_acc=6.0%]\n",
            "Epoch 190: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6557, cell_acc=72.6%, grid_acc=5.0%]\n",
            "Epoch 191: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6604, cell_acc=72.8%, grid_acc=5.4%]\n",
            "Epoch 192: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6267, cell_acc=72.8%, grid_acc=5.9%]\n",
            "Epoch 193: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6310, cell_acc=73.1%, grid_acc=5.8%]\n",
            "Epoch 194: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6431, cell_acc=72.8%, grid_acc=5.5%]\n",
            "Epoch 195: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5945, cell_acc=73.1%, grid_acc=6.1%]\n",
            "Epoch 196: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6538, cell_acc=73.1%, grid_acc=6.9%]\n",
            "Epoch 197: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6432, cell_acc=73.1%, grid_acc=6.4%]\n",
            "Epoch 198: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5747, cell_acc=73.1%, grid_acc=6.6%]\n",
            "Epoch 199: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5930, cell_acc=73.2%, grid_acc=6.9%]\n",
            "Epoch 200: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6770, cell_acc=73.3%, grid_acc=6.5%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 200/500\n",
            "  Train: Loss=0.6207, Cell=73.27%, Grid=6.51%\n",
            "  Test:  Loss=0.6118, Cell=72.71%, Grid=5.60%\n",
            "  âœ“ New best: 5.60%\n",
            "Epoch 201: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6172, cell_acc=73.4%, grid_acc=6.7%]\n",
            "Epoch 202: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6115, cell_acc=73.2%, grid_acc=6.4%]\n",
            "Epoch 203: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6392, cell_acc=73.0%, grid_acc=5.6%]\n",
            "Epoch 204: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5807, cell_acc=73.3%, grid_acc=5.9%]\n",
            "Epoch 205: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6252, cell_acc=73.5%, grid_acc=6.3%]\n",
            "Epoch 206: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5733, cell_acc=73.3%, grid_acc=6.9%]\n",
            "Epoch 207: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6355, cell_acc=73.3%, grid_acc=6.6%]\n",
            "Epoch 208: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5709, cell_acc=73.4%, grid_acc=6.7%]\n",
            "Epoch 209: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6642, cell_acc=73.0%, grid_acc=5.7%]\n",
            "Epoch 210: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6414, cell_acc=73.2%, grid_acc=6.5%]\n",
            "Epoch 211: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6018, cell_acc=73.5%, grid_acc=6.4%]\n",
            "Epoch 212: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6570, cell_acc=73.4%, grid_acc=6.7%]\n",
            "Epoch 213: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5891, cell_acc=73.3%, grid_acc=5.4%]\n",
            "Epoch 214: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5852, cell_acc=73.4%, grid_acc=7.3%]\n",
            "Epoch 215: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6257, cell_acc=73.6%, grid_acc=7.5%]\n",
            "Epoch 216: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6642, cell_acc=73.6%, grid_acc=6.8%]\n",
            "Epoch 217: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5866, cell_acc=73.7%, grid_acc=6.8%]\n",
            "Epoch 218: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5565, cell_acc=73.4%, grid_acc=5.7%]\n",
            "Epoch 219: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5400, cell_acc=73.6%, grid_acc=7.3%]\n",
            "Epoch 220: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5957, cell_acc=73.7%, grid_acc=7.7%]\n",
            "Epoch 221: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6662, cell_acc=74.0%, grid_acc=8.8%]\n",
            "Epoch 222: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5918, cell_acc=73.6%, grid_acc=7.7%]\n",
            "Epoch 223: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5993, cell_acc=73.5%, grid_acc=7.3%]\n",
            "Epoch 224: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5920, cell_acc=73.9%, grid_acc=7.9%]\n",
            "Epoch 225: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6214, cell_acc=73.7%, grid_acc=7.5%]\n",
            "Epoch 226: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6969, cell_acc=73.5%, grid_acc=7.1%]\n",
            "Epoch 227: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6312, cell_acc=73.8%, grid_acc=7.8%]\n",
            "Epoch 228: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6466, cell_acc=73.8%, grid_acc=7.3%]\n",
            "Epoch 229: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5441, cell_acc=73.5%, grid_acc=7.1%]\n",
            "Epoch 230: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5856, cell_acc=73.7%, grid_acc=7.1%]\n",
            "Epoch 231: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6382, cell_acc=73.8%, grid_acc=7.5%]\n",
            "Epoch 232: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5840, cell_acc=73.8%, grid_acc=7.2%]\n",
            "Epoch 233: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6366, cell_acc=74.1%, grid_acc=8.6%]\n",
            "Epoch 234: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5757, cell_acc=73.9%, grid_acc=8.5%]\n",
            "Epoch 235: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6340, cell_acc=73.9%, grid_acc=8.1%]\n",
            "Epoch 236: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5876, cell_acc=74.0%, grid_acc=8.2%]\n",
            "Epoch 237: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6114, cell_acc=74.0%, grid_acc=8.5%]\n",
            "Epoch 238: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5506, cell_acc=74.2%, grid_acc=8.5%]\n",
            "Epoch 239: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6299, cell_acc=74.2%, grid_acc=9.5%]\n",
            "Epoch 240: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6020, cell_acc=74.0%, grid_acc=8.3%]\n",
            "Epoch 241: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6275, cell_acc=74.0%, grid_acc=8.0%]\n",
            "Epoch 242: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6054, cell_acc=73.9%, grid_acc=8.2%]\n",
            "Epoch 243: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6632, cell_acc=73.9%, grid_acc=7.3%]\n",
            "Epoch 244: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6324, cell_acc=74.0%, grid_acc=8.0%]\n",
            "Epoch 245: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6017, cell_acc=73.9%, grid_acc=7.7%]\n",
            "Epoch 246: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5334, cell_acc=74.1%, grid_acc=9.2%]\n",
            "Epoch 247: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5919, cell_acc=74.4%, grid_acc=9.8%]\n",
            "Epoch 248: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5480, cell_acc=74.0%, grid_acc=8.7%]\n",
            "Epoch 249: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6291, cell_acc=74.3%, grid_acc=8.6%]\n",
            "Epoch 250: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6436, cell_acc=74.4%, grid_acc=9.4%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 250/500\n",
            "  Train: Loss=0.5953, Cell=74.38%, Grid=9.43%\n",
            "  Test:  Loss=0.5848, Cell=74.08%, Grid=10.40%\n",
            "  âœ“ New best: 10.40%\n",
            "Epoch 251: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5171, cell_acc=74.2%, grid_acc=8.6%]\n",
            "Epoch 252: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6063, cell_acc=74.1%, grid_acc=8.2%]\n",
            "Epoch 253: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5263, cell_acc=74.3%, grid_acc=8.5%]\n",
            "Epoch 254: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6255, cell_acc=74.7%, grid_acc=10.3%]\n",
            "Epoch 255: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5248, cell_acc=74.6%, grid_acc=10.1%]\n",
            "Epoch 256: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5346, cell_acc=74.2%, grid_acc=9.2%]\n",
            "Epoch 257: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5322, cell_acc=74.8%, grid_acc=10.0%]\n",
            "Epoch 258: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5657, cell_acc=74.8%, grid_acc=10.6%]\n",
            "Epoch 259: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6846, cell_acc=74.6%, grid_acc=9.6%]\n",
            "Epoch 260: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6077, cell_acc=74.7%, grid_acc=10.7%]\n",
            "Epoch 261: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5486, cell_acc=74.5%, grid_acc=9.6%]\n",
            "Epoch 262: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6266, cell_acc=74.6%, grid_acc=10.3%]\n",
            "Epoch 263: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6793, cell_acc=74.8%, grid_acc=10.9%]\n",
            "Epoch 264: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5701, cell_acc=74.7%, grid_acc=9.9%]\n",
            "Epoch 265: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6765, cell_acc=74.7%, grid_acc=10.5%]\n",
            "Epoch 266: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5173, cell_acc=75.1%, grid_acc=11.2%]\n",
            "Epoch 267: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6513, cell_acc=75.0%, grid_acc=10.8%]\n",
            "Epoch 268: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5557, cell_acc=74.8%, grid_acc=11.1%]\n",
            "Epoch 269: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6046, cell_acc=74.8%, grid_acc=10.6%]\n",
            "Epoch 270: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5993, cell_acc=74.8%, grid_acc=10.6%]\n",
            "Epoch 271: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6169, cell_acc=75.0%, grid_acc=10.7%]\n",
            "Epoch 272: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5380, cell_acc=74.9%, grid_acc=10.5%]\n",
            "Epoch 273: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6501, cell_acc=74.9%, grid_acc=10.4%]\n",
            "Epoch 274: 100% 71/71 [01:11<00:00,  1.01s/it, loss=0.6422, cell_acc=74.6%, grid_acc=9.8%]\n",
            "Epoch 275: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6144, cell_acc=74.9%, grid_acc=10.7%]\n",
            "Epoch 276: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5543, cell_acc=75.0%, grid_acc=10.4%]\n",
            "Epoch 277: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5530, cell_acc=75.2%, grid_acc=11.7%]\n",
            "Epoch 278: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6379, cell_acc=75.0%, grid_acc=11.1%]\n",
            "Epoch 279: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5902, cell_acc=75.3%, grid_acc=12.1%]\n",
            "Epoch 280: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5302, cell_acc=75.5%, grid_acc=11.8%]\n",
            "Epoch 281: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5785, cell_acc=75.5%, grid_acc=12.2%]\n",
            "Epoch 282: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5519, cell_acc=75.2%, grid_acc=11.3%]\n",
            "Epoch 283: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6054, cell_acc=74.9%, grid_acc=10.7%]\n",
            "Epoch 284: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5371, cell_acc=75.2%, grid_acc=11.1%]\n",
            "Epoch 285: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5703, cell_acc=75.4%, grid_acc=11.3%]\n",
            "Epoch 286: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5898, cell_acc=75.3%, grid_acc=11.8%]\n",
            "Epoch 287: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6036, cell_acc=75.4%, grid_acc=11.3%]\n",
            "Epoch 288: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6249, cell_acc=75.5%, grid_acc=11.4%]\n",
            "Epoch 289: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5535, cell_acc=75.1%, grid_acc=11.4%]\n",
            "Epoch 290: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5685, cell_acc=75.0%, grid_acc=10.0%]\n",
            "Epoch 291: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5923, cell_acc=75.2%, grid_acc=10.9%]\n",
            "Epoch 292: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5857, cell_acc=75.5%, grid_acc=11.8%]\n",
            "Epoch 293: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5859, cell_acc=75.6%, grid_acc=12.3%]\n",
            "Epoch 294: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5347, cell_acc=75.1%, grid_acc=11.3%]\n",
            "Epoch 295: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5988, cell_acc=75.3%, grid_acc=10.8%]\n",
            "Epoch 296: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5991, cell_acc=75.2%, grid_acc=11.2%]\n",
            "Epoch 297: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5261, cell_acc=75.1%, grid_acc=11.5%]\n",
            "Epoch 298: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5572, cell_acc=75.7%, grid_acc=12.0%]\n",
            "Epoch 299: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5255, cell_acc=75.4%, grid_acc=11.8%]\n",
            "Epoch 300: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5978, cell_acc=75.4%, grid_acc=11.4%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 300/500\n",
            "  Train: Loss=0.5686, Cell=75.41%, Grid=11.44%\n",
            "  Test:  Loss=0.5489, Cell=75.52%, Grid=11.90%\n",
            "  âœ“ New best: 11.90%\n",
            "Epoch 301: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5682, cell_acc=75.8%, grid_acc=12.0%]\n",
            "Epoch 302: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5443, cell_acc=75.6%, grid_acc=12.2%]\n",
            "Epoch 303: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6105, cell_acc=75.8%, grid_acc=11.9%]\n",
            "Epoch 304: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6012, cell_acc=75.9%, grid_acc=12.7%]\n",
            "Epoch 305: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5983, cell_acc=75.6%, grid_acc=12.1%]\n",
            "Epoch 306: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4385, cell_acc=75.6%, grid_acc=11.7%]\n",
            "Epoch 307: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5592, cell_acc=76.0%, grid_acc=13.4%]\n",
            "Epoch 308: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6338, cell_acc=75.8%, grid_acc=13.7%]\n",
            "Epoch 309: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6262, cell_acc=75.8%, grid_acc=13.2%]\n",
            "Epoch 310: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5668, cell_acc=76.2%, grid_acc=13.9%]\n",
            "Epoch 311: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5532, cell_acc=76.1%, grid_acc=14.2%]\n",
            "Epoch 312: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6174, cell_acc=75.7%, grid_acc=12.7%]\n",
            "Epoch 313: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5206, cell_acc=75.9%, grid_acc=13.3%]\n",
            "Epoch 314: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5709, cell_acc=75.6%, grid_acc=12.9%]\n",
            "Epoch 315: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5200, cell_acc=76.2%, grid_acc=13.4%]\n",
            "Epoch 316: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5434, cell_acc=75.8%, grid_acc=11.9%]\n",
            "Epoch 317: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4951, cell_acc=76.1%, grid_acc=13.4%]\n",
            "Epoch 318: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5683, cell_acc=76.2%, grid_acc=13.8%]\n",
            "Epoch 319: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5246, cell_acc=76.3%, grid_acc=13.4%]\n",
            "Epoch 320: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.3850, cell_acc=76.0%, grid_acc=13.6%]\n",
            "Epoch 321: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4799, cell_acc=76.4%, grid_acc=14.4%]\n",
            "Epoch 322: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5629, cell_acc=76.0%, grid_acc=13.3%]\n",
            "Epoch 323: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5827, cell_acc=76.3%, grid_acc=14.0%]\n",
            "Epoch 324: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5630, cell_acc=76.4%, grid_acc=14.2%]\n",
            "Epoch 325: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5031, cell_acc=76.2%, grid_acc=14.1%]\n",
            "Epoch 326: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5615, cell_acc=76.3%, grid_acc=14.5%]\n",
            "Epoch 327: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5844, cell_acc=76.5%, grid_acc=14.7%]\n",
            "Epoch 328: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5129, cell_acc=76.5%, grid_acc=15.0%]\n",
            "Epoch 329: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4559, cell_acc=76.7%, grid_acc=15.5%]\n",
            "Epoch 330: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5287, cell_acc=76.5%, grid_acc=14.6%]\n",
            "Epoch 331: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5112, cell_acc=76.5%, grid_acc=14.8%]\n",
            "Epoch 332: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6089, cell_acc=76.7%, grid_acc=15.7%]\n",
            "Epoch 333: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4895, cell_acc=76.7%, grid_acc=15.9%]\n",
            "Epoch 334: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5126, cell_acc=76.9%, grid_acc=16.4%]\n",
            "Epoch 335: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4803, cell_acc=76.6%, grid_acc=15.8%]\n",
            "Epoch 336: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5345, cell_acc=76.6%, grid_acc=15.9%]\n",
            "Epoch 337: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6449, cell_acc=76.7%, grid_acc=14.9%]\n",
            "Epoch 338: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5360, cell_acc=76.4%, grid_acc=15.2%]\n",
            "Epoch 339: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5580, cell_acc=76.5%, grid_acc=15.7%]\n",
            "Epoch 340: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5006, cell_acc=76.8%, grid_acc=15.2%]\n",
            "Epoch 341: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5946, cell_acc=76.9%, grid_acc=15.6%]\n",
            "Epoch 342: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5398, cell_acc=76.6%, grid_acc=15.3%]\n",
            "Epoch 343: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5284, cell_acc=76.7%, grid_acc=15.2%]\n",
            "Epoch 344: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5596, cell_acc=77.0%, grid_acc=16.7%]\n",
            "Epoch 345: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.3970, cell_acc=76.4%, grid_acc=14.3%]\n",
            "Epoch 346: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5236, cell_acc=76.8%, grid_acc=15.3%]\n",
            "Epoch 347: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5501, cell_acc=76.9%, grid_acc=15.9%]\n",
            "Epoch 348: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5197, cell_acc=77.0%, grid_acc=16.4%]\n",
            "Epoch 349: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4993, cell_acc=77.1%, grid_acc=16.7%]\n",
            "Epoch 350: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4924, cell_acc=77.0%, grid_acc=16.1%]\n",
            "Evaluating: 100% 8/8 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Epoch 350/500\n",
            "  Train: Loss=0.5356, Cell=76.99%, Grid=16.07%\n",
            "  Test:  Loss=0.5341, Cell=76.44%, Grid=16.20%\n",
            "  âœ“ New best: 16.20%\n",
            "Epoch 351: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5884, cell_acc=77.0%, grid_acc=17.1%]\n",
            "Epoch 352: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5635, cell_acc=77.2%, grid_acc=17.3%]\n",
            "Epoch 353: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4997, cell_acc=76.9%, grid_acc=15.9%]\n",
            "Epoch 354: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4694, cell_acc=77.0%, grid_acc=16.4%]\n",
            "Epoch 355: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5327, cell_acc=77.1%, grid_acc=16.3%]\n",
            "Epoch 356: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4406, cell_acc=76.8%, grid_acc=16.0%]\n",
            "Epoch 357: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4821, cell_acc=77.1%, grid_acc=17.1%]\n",
            "Epoch 358: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5533, cell_acc=77.2%, grid_acc=17.1%]\n",
            "Epoch 359: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6216, cell_acc=77.2%, grid_acc=17.2%]\n",
            "Epoch 360: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5494, cell_acc=76.9%, grid_acc=15.5%]\n",
            "Epoch 361: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5556, cell_acc=77.0%, grid_acc=16.7%]\n",
            "Epoch 362: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4910, cell_acc=76.9%, grid_acc=16.2%]\n",
            "Epoch 363: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6323, cell_acc=77.2%, grid_acc=16.4%]\n",
            "Epoch 364: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5266, cell_acc=77.1%, grid_acc=16.2%]\n",
            "Epoch 365: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5262, cell_acc=77.4%, grid_acc=17.6%]\n",
            "Epoch 366: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5194, cell_acc=77.2%, grid_acc=17.2%]\n",
            "Epoch 367: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.5612, cell_acc=77.2%, grid_acc=17.2%]\n",
            "Epoch 368: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.4825, cell_acc=77.4%, grid_acc=17.8%]\n",
            "Epoch 369: 100% 71/71 [01:11<00:00,  1.00s/it, loss=0.6073, cell_acc=77.3%, grid_acc=18.2%]\n",
            "Epoch 370:  76% 54/71 [00:54<00:17,  1.01s/it, loss=0.5524, cell_acc=77.5%, grid_acc=18.1%]"
          ]
        }
      ],
      "source": [
        "# Run Diffusion training with all arguments\n",
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --download \\\n",
        "    --model hybrid \\\n",
        "    --controller diffusion \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --H-cycles 2 \\\n",
        "    --L-cycles 6 \\\n",
        "    --halt-max-steps 4 \\\n",
        "    --halt-exploration-prob 0.1 \\\n",
        "    --max-depth 32 \\\n",
        "    --hrm-grad-style \\\n",
        "    --epochs 500 \\\n",
        "    --batch-size 128 \\\n",
        "    --lr 3e-4 \\\n",
        "    --weight-decay 0.01 \\\n",
        "    --beta1 0.9 \\\n",
        "    --beta2 0.95 \\\n",
        "    --warmup-steps 500 \\\n",
        "    --lr-min-ratio 0.1 \\\n",
        "    --grad-clip 1.0 \\\n",
        "    --dropout 0.0 \\\n",
        "    --subsample 10000 \\\n",
        "    --num-aug 100 \\\n",
        "    --eval-interval 50 \\\n",
        "    --wandb \\\n",
        "    --project sudoku-diffusion \\\n",
        "    --seed 42 \\\n",
        "    --output experiments/results/diffusion_sudoku\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mWTBjMDa7O1"
      },
      "source": [
        "## ðŸ“ˆ Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7c8Zh-4a7O1"
      },
      "outputs": [],
      "source": [
        "# Plot results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open('experiments/results/diffusion_sudoku/hybrid_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(f\"Best Grid Accuracy: {results['best_grid_acc']:.2f}%\")\n",
        "print(f\"Parameters: {results['parameters']:,}\")\n",
        "\n",
        "history = results['history']\n",
        "epochs = [h['epoch'] for h in history]\n",
        "train_acc = [h['train_grid_acc'] for h in history]\n",
        "test_acc = [h['test_grid_acc'] for h in history]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_acc, label='Train')\n",
        "plt.plot(epochs, test_acc, label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Grid Accuracy (%)')\n",
        "plt.title('Diffusion Controller - Sudoku Training')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt84ksLOa7O1"
      },
      "source": [
        "## ðŸ”¬ Visualize Noise Schedule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgSZuJx0a7O1"
      },
      "outputs": [],
      "source": [
        "# Visualize sigma (noise level) across depth steps\n",
        "from src.pot.core import DiffusionDepthController\n",
        "\n",
        "controller = DiffusionDepthController(d_model=256, n_heads=8, max_depth=32, noise_schedule=\"cosine\")\n",
        "sigmas = controller.sigma_schedule.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(range(len(sigmas)), sigmas, 'b-', linewidth=2)\n",
        "plt.xlabel('Depth Step')\n",
        "plt.ylabel('Sigma (Noise Level)')\n",
        "plt.title('Cosine Noise Schedule - Denoising Process')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Step 0 (start):  Ïƒ = {sigmas[0]:.4f} (noisy)\")\n",
        "print(f\"Step 16 (mid):   Ïƒ = {sigmas[16]:.4f}\")\n",
        "print(f\"Step 31 (end):   Ïƒ = {sigmas[31]:.4f} (clean)\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
