{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ€ Diffusion HRM Sudoku Solver\n",
        "\n",
        "Train a **master-level Sudoku AI** using **fully diffusion-based architecture** where everything is diffusion!\n",
        "\n",
        "## Architecture - Everything is Diffusion!\n",
        "- **z_H, z_L**: Progressively **denoised** at different timescales (not GRU!)\n",
        "- **L-level**: Fast denoising (every step)\n",
        "- **H-level**: Slow denoising (learned timing via Gumbel-softmax)\n",
        "- **adaLN conditioning**: DiT-style adaptive LayerNorm\n",
        "- **Noise schedules**: Cosine schedules for both timescales\n",
        "- **Skip connections**: HRM-style input injection + output residual\n",
        "\n",
        "## Key Innovations\n",
        "| Component | HybridHRM (GRU-based) | DiffusionHRMSolver |\n",
        "|-----------|----------------------|-------------------|\n",
        "| H,L updates | Deterministic GRU | Diffusion denoising |\n",
        "| Timing | Fixed T=4 | Learned via Gumbel-softmax |\n",
        "| State evolution | Recurrent | Progressive denoising |\n",
        "| Controller | GRU/Transformer | Diffusion denoisers |\n",
        "\n",
        "## Hardware Requirements\n",
        "- **GPU**: A100/H100 recommended (16-80GB VRAM)\n",
        "- **Runtime**: ~12-24 hours for full training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone PoT repository\n",
        "!git clone https://github.com/Eran-BA/PoT.git /content/PoT 2>/dev/null || (cd /content/PoT && git pull)\n",
        "%cd /content/PoT\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q tqdm numpy huggingface_hub\n",
        "!pip install -q wandb  # Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Weights & Biases\n",
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Sudoku Dataset\n",
        "\n",
        "Downloads the **Sudoku-Extreme** dataset from HuggingFace:\n",
        "- 10,000 extreme-difficulty puzzles\n",
        "- 100 augmentations per puzzle\n",
        "- Total: ~1,000,000 training samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Sudoku-Extreme dataset from HuggingFace\n",
        "from src.data import download_sudoku_dataset\n",
        "\n",
        "download_sudoku_dataset(\n",
        "    output_dir='data/sudoku-extreme-10k-aug-100',\n",
        "    subsample_size=10000,\n",
        "    num_aug=100,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Script with W&B Integration\n",
        "\n",
        "The script below implements:\n",
        "- **DiffusionSudokuSolver** with fully diffusion-based H,L cycles\n",
        "- **W&B logging** for loss, accuracy, gradients, and hyperparameters\n",
        "- **Cosine LR schedule** with warmup\n",
        "- **Gradient clipping** for stable training\n",
        "- **Best model checkpointing** with W&B artifact saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_diffusion_sudoku.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Diffusion HRM Sudoku Training Script with W&B Integration\n",
        "==========================================================\n",
        "\n",
        "Trains a fully diffusion-based Sudoku solver where:\n",
        "- H,L cycles use diffusion denoising (not GRU)\n",
        "- Timing for H-updates is learned via Gumbel-softmax\n",
        "- adaLN conditioning (DiT-style)\n",
        "\n",
        "Author: Eran Ben Artzy\n",
        "Year: 2025\n",
        "License: Apache 2.0\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, '/content/PoT')\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "from src.data import SudokuDataset\n",
        "from src.pot.models.diffusion_hrm_solver import DiffusionSudokuSolver\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
        "    \"\"\"Cosine learning rate schedule with warmup.\"\"\"\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / max(1, warmup_steps)\n",
        "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        return min_lr_ratio + (1 - min_lr_ratio) * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    \"\"\"Evaluate model on dataset.\"\"\"\n",
        "    model.eval()\n",
        "    correct_cells = 0\n",
        "    total_cells = 0\n",
        "    correct_grids = 0\n",
        "    total_grids = 0\n",
        "    total_steps = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            inputs = batch['input'].to(device)\n",
        "            targets = batch['label'].to(device)\n",
        "            puzzle_ids = batch.get('puzzle_id', torch.zeros(inputs.size(0), dtype=torch.long)).to(device)\n",
        "            \n",
        "            logits, _, _, steps = model(inputs, puzzle_ids)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            \n",
        "            mask = (inputs == 0)\n",
        "            correct_cells += ((preds == targets) & mask).sum().item()\n",
        "            total_cells += mask.sum().item()\n",
        "            \n",
        "            grid_correct = ((preds == targets) | ~mask).all(dim=1)\n",
        "            correct_grids += grid_correct.sum().item()\n",
        "            total_grids += inputs.size(0)\n",
        "            \n",
        "            total_steps += steps\n",
        "            num_batches += 1\n",
        "    \n",
        "    return {\n",
        "        'cell_acc': 100 * correct_cells / max(1, total_cells),\n",
        "        'grid_acc': 100 * correct_grids / max(1, total_grids),\n",
        "        'avg_steps': total_steps / max(1, num_batches),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scheduler, device, epoch, log_interval=50):\n",
        "    \"\"\"Train for one epoch with W&B logging.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    global_step = (epoch - 1) * len(loader)\n",
        "    \n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        inputs = batch['input'].to(device)\n",
        "        targets = batch['label'].to(device)\n",
        "        puzzle_ids = batch.get('puzzle_id', torch.zeros(inputs.size(0), dtype=torch.long)).to(device)\n",
        "        \n",
        "        logits, q_halt, q_continue, steps = model(inputs, puzzle_ids)\n",
        "        \n",
        "        mask = (inputs == 0)\n",
        "        loss = F.cross_entropy(logits[mask], targets[mask])\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        global_step += 1\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            wandb.log({\n",
        "                'train/loss': loss.item(),\n",
        "                'train/lr': scheduler.get_last_lr()[0],\n",
        "                'train/grad_norm': grad_norm.item(),\n",
        "                'train/act_steps': steps,\n",
        "                'train/q_halt_mean': q_halt.mean().item(),\n",
        "                'train/q_continue_mean': q_continue.mean().item(),\n",
        "                'global_step': global_step,\n",
        "            })\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}', 'steps': steps})\n",
        "    \n",
        "    return total_loss / max(1, num_batches)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Train Diffusion HRM Sudoku Solver')\n",
        "    \n",
        "    # Model args\n",
        "    parser.add_argument('--d-model', type=int, default=512)\n",
        "    parser.add_argument('--n-heads', type=int, default=8)\n",
        "    parser.add_argument('--max-steps', type=int, default=32, help='Diffusion steps')\n",
        "    parser.add_argument('--T', type=int, default=4, help='Base H/L timescale ratio')\n",
        "    parser.add_argument('--noise-schedule', type=str, default='cosine', choices=['linear', 'cosine', 'sqrt'])\n",
        "    parser.add_argument('--learned-timing', action='store_true', default=True)\n",
        "    parser.add_argument('--no-learned-timing', action='store_false', dest='learned_timing')\n",
        "    parser.add_argument('--halt-max-steps', type=int, default=4, help='ACT outer steps')\n",
        "    parser.add_argument('--num-puzzles', type=int, default=10000)\n",
        "    \n",
        "    # Training args\n",
        "    parser.add_argument('--epochs', type=int, default=10000)\n",
        "    parser.add_argument('--batch-size', type=int, default=256)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight-decay', type=float, default=0.1)\n",
        "    parser.add_argument('--warmup-steps', type=int, default=2000)\n",
        "    parser.add_argument('--lr-min-ratio', type=float, default=0.1)\n",
        "    parser.add_argument('--dropout', type=float, default=0.0)\n",
        "    \n",
        "    # Data args\n",
        "    parser.add_argument('--data-dir', type=str, default='data/sudoku-extreme-10k-aug-100')\n",
        "    parser.add_argument('--num-workers', type=int, default=4)\n",
        "    \n",
        "    # Eval args\n",
        "    parser.add_argument('--eval-interval', type=int, default=100)\n",
        "    parser.add_argument('--save-dir', type=str, default='experiments/results/diffusion_sudoku')\n",
        "    \n",
        "    # W&B args\n",
        "    parser.add_argument('--wandb-project', type=str, default='diffusion-sudoku')\n",
        "    parser.add_argument('--wandb-entity', type=str, default=None)\n",
        "    parser.add_argument('--wandb-name', type=str, default=None)\n",
        "    parser.add_argument('--wandb-tags', type=str, nargs='+', default=['diffusion', 'sudoku', 'hrm'])\n",
        "    parser.add_argument('--no-wandb', action='store_true', help='Disable W&B logging')\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    \n",
        "    # Initialize W&B\n",
        "    if not args.no_wandb:\n",
        "        run_name = args.wandb_name or f\"diffusion-sudoku-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "        wandb.init(project=args.wandb_project, entity=args.wandb_entity, name=run_name, tags=args.wandb_tags, config=vars(args))\n",
        "        print(f\"W&B run: {wandb.run.url}\")\n",
        "    \n",
        "    # Load data\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = SudokuDataset(args.data_dir, 'train')\n",
        "    val_dataset = SudokuDataset(args.data_dir, 'val')\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "    \n",
        "    print(f\"Train: {len(train_dataset)} samples, Val: {len(val_dataset)} samples\")\n",
        "    \n",
        "    # Create model\n",
        "    print(\"\\nCreating Diffusion HRM Sudoku Solver...\")\n",
        "    model = DiffusionSudokuSolver(\n",
        "        d_model=args.d_model, n_heads=args.n_heads, max_steps=args.max_steps, T=args.T,\n",
        "        noise_schedule=args.noise_schedule, num_puzzles=args.num_puzzles, halt_max_steps=args.halt_max_steps,\n",
        "        dropout=args.dropout, learned_timing=args.learned_timing,\n",
        "    ).to(device)\n",
        "    \n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "    \n",
        "    if not args.no_wandb:\n",
        "        wandb.config.update({'num_params': num_params})\n",
        "        wandb.watch(model, log='gradients', log_freq=500)\n",
        "    \n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.95))\n",
        "    total_steps = args.epochs * len(train_loader)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, args.warmup_steps, total_steps, args.lr_min_ratio)\n",
        "    \n",
        "    # Training loop\n",
        "    best_grid_acc = 0.0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Starting Training - Diffusion HRM Sudoku Solver\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Diffusion steps: {args.max_steps}, T: {args.T}, Noise: {args.noise_schedule}\")\n",
        "    print(f\"  Learned timing: {args.learned_timing}, ACT steps: {args.halt_max_steps}\")\n",
        "    print(f\"  Epochs: {args.epochs}, Batch: {args.batch_size}, LR: {args.lr}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, epoch)\n",
        "        \n",
        "        if not args.no_wandb:\n",
        "            wandb.log({'epoch': epoch, 'train/epoch_loss': train_loss})\n",
        "        \n",
        "        if epoch % args.eval_interval == 0 or epoch == 1:\n",
        "            val_metrics = evaluate(model, val_loader, device)\n",
        "            \n",
        "            print(f\"\\nEpoch {epoch}: loss={train_loss:.4f}, val_cell={val_metrics['cell_acc']:.2f}%, val_grid={val_metrics['grid_acc']:.2f}%\")\n",
        "            \n",
        "            if not args.no_wandb:\n",
        "                wandb.log({'val/cell_acc': val_metrics['cell_acc'], 'val/grid_acc': val_metrics['grid_acc'], \n",
        "                          'val/avg_steps': val_metrics['avg_steps'], 'val/best_grid_acc': max(best_grid_acc, val_metrics['grid_acc']), 'epoch': epoch})\n",
        "            \n",
        "            if val_metrics['grid_acc'] > best_grid_acc:\n",
        "                best_grid_acc = val_metrics['grid_acc']\n",
        "                checkpoint = {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
        "                             'val_cell_acc': val_metrics['cell_acc'], 'val_grid_acc': val_metrics['grid_acc'], 'args': vars(args)}\n",
        "                save_path = f\"{args.save_dir}/diffusion_best.pt\"\n",
        "                torch.save(checkpoint, save_path)\n",
        "                print(f\"  âœ“ New best model saved! Grid accuracy: {best_grid_acc:.2f}%\")\n",
        "                \n",
        "                if not args.no_wandb:\n",
        "                    wandb.save(save_path)\n",
        "                    wandb.run.summary['best_grid_acc'] = best_grid_acc\n",
        "                    wandb.run.summary['best_epoch'] = epoch\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training complete! Best grid accuracy: {best_grid_acc:.2f}%\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if not args.no_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Full Training (A100/H100)\n",
        "\n",
        "Run full training with diffusion-based architecture and W&B logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training with W&B integration\n",
        "!python train_diffusion_sudoku.py \\\n",
        "    --d-model 512 \\\n",
        "    --n-heads 8 \\\n",
        "    --max-steps 32 \\\n",
        "    --T 4 \\\n",
        "    --noise-schedule cosine \\\n",
        "    --learned-timing \\\n",
        "    --halt-max-steps 4 \\\n",
        "    --epochs 10000 \\\n",
        "    --batch-size 512 \\\n",
        "    --lr 1e-4 \\\n",
        "    --weight-decay 0.1 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --eval-interval 100 \\\n",
        "    --wandb-project diffusion-sudoku \\\n",
        "    --wandb-tags diffusion sudoku hrm full-training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quick Test (~1-2 hours on T4)\n",
        "\n",
        "For a quick sanity check with smaller model and fewer epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test with W&B\n",
        "!python train_diffusion_sudoku.py \\\n",
        "    --d-model 256 \\\n",
        "    --n-heads 8 \\\n",
        "    --max-steps 16 \\\n",
        "    --T 4 \\\n",
        "    --halt-max-steps 2 \\\n",
        "    --epochs 500 \\\n",
        "    --batch-size 128 \\\n",
        "    --lr 1e-4 \\\n",
        "    --warmup-steps 200 \\\n",
        "    --eval-interval 50 \\\n",
        "    --wandb-project diffusion-sudoku \\\n",
        "    --wandb-tags diffusion sudoku quick-test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load and Evaluate Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from src.pot.models.diffusion_hrm_solver import DiffusionSudokuSolver\n",
        "from src.data import SudokuDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load('experiments/results/diffusion_sudoku/diffusion_best.pt')\n",
        "args = checkpoint['args']\n",
        "\n",
        "# Recreate model with same config\n",
        "model = DiffusionSudokuSolver(\n",
        "    d_model=args['d_model'],\n",
        "    n_heads=args['n_heads'],\n",
        "    max_steps=args['max_steps'],\n",
        "    T=args['T'],\n",
        "    noise_schedule=args['noise_schedule'],\n",
        "    num_puzzles=args['num_puzzles'],\n",
        "    halt_max_steps=args['halt_max_steps'],\n",
        "    dropout=args['dropout'],\n",
        "    learned_timing=args['learned_timing'],\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Val Cell Accuracy: {checkpoint['val_cell_acc']:.2f}%\")\n",
        "print(f\"  Val Grid Accuracy: {checkpoint['val_grid_acc']:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_dataset = SudokuDataset(args['data_dir'], 'test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"\\nEvaluating on {len(test_dataset)} test puzzles...\")\n",
        "model.eval()\n",
        "correct_cells = 0\n",
        "total_cells = 0\n",
        "correct_grids = 0\n",
        "total_grids = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = batch['input'].to(device)\n",
        "        targets = batch['label'].to(device)\n",
        "        puzzle_ids = batch.get('puzzle_id', torch.zeros(inputs.size(0), dtype=torch.long)).to(device)\n",
        "        \n",
        "        logits, _, _, _ = model(inputs, puzzle_ids)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        \n",
        "        mask = (inputs == 0)\n",
        "        correct_cells += ((preds == targets) & mask).sum().item()\n",
        "        total_cells += mask.sum().item()\n",
        "        \n",
        "        grid_correct = ((preds == targets) | ~mask).all(dim=1)\n",
        "        correct_grids += grid_correct.sum().item()\n",
        "        total_grids += inputs.size(0)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"FINAL TEST RESULTS (Diffusion HRM Solver)\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"  Cell Accuracy: {100*correct_cells/total_cells:.2f}%\")\n",
        "print(f\"  Grid Accuracy: {100*correct_grids/total_grids:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Diffusion Process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Visualize how noise levels evolve\n",
        "from src.pot.core.diffusion_hl_cycles import get_noise_schedule\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, schedule_type in zip(axes, ['linear', 'cosine', 'sqrt']):\n",
        "    L_schedule = get_noise_schedule(schedule_type, 32, torch.device('cpu'))\n",
        "    H_schedule = get_noise_schedule(schedule_type, 8, torch.device('cpu'))  # 32 // T\n",
        "    \n",
        "    ax.plot(range(32), L_schedule.numpy(), 'b-', label='L-level (fast)', linewidth=2)\n",
        "    ax.plot(range(0, 32, 4), H_schedule.numpy(), 'r--', label='H-level (slow)', linewidth=2, marker='o')\n",
        "    ax.set_xlabel('Diffusion Step')\n",
        "    ax.set_ylabel('Noise Level (Ïƒ)')\n",
        "    ax.set_title(f'{schedule_type.capitalize()} Schedule')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Dual-Timescale Noise Schedules', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('noise_schedules.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Log to W&B if available\n",
        "try:\n",
        "    import wandb\n",
        "    if wandb.run is not None:\n",
        "        wandb.log({\"noise_schedules\": wandb.Image('noise_schedules.png')})\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"Saved: noise_schedules.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compare with Standard HybridHRM\n",
        "\n",
        "Compare parameter counts and architectural differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare architectures\n",
        "import torch\n",
        "from src.pot.models.diffusion_hrm_solver import DiffusionSudokuSolver\n",
        "from src.pot.models import HybridPoHHRMSolver\n",
        "\n",
        "# Count parameters for each architecture\n",
        "diffusion_model = DiffusionSudokuSolver(\n",
        "    d_model=512, n_heads=8, max_steps=32, T=4,\n",
        "    halt_max_steps=4, num_puzzles=10000,\n",
        ")\n",
        "\n",
        "hybrid_model = HybridPoHHRMSolver(\n",
        "    d_model=512, n_heads=8, H_cycles=2, L_cycles=8,\n",
        "    H_layers=2, L_layers=2, halt_max_steps=4, num_puzzles=1,\n",
        ")\n",
        "\n",
        "diff_params = sum(p.numel() for p in diffusion_model.parameters())\n",
        "hybrid_params = sum(p.numel() for p in hybrid_model.parameters())\n",
        "\n",
        "print(\"Architecture Comparison\")\n",
        "print(\"=\"*60)\n",
        "print(f\"DiffusionSudokuSolver: {diff_params:,} params ({diff_params/1e6:.2f}M)\")\n",
        "print(f\"HybridPoHHRMSolver:    {hybrid_params:,} params ({hybrid_params/1e6:.2f}M)\")\n",
        "print()\n",
        "print(\"Key Differences:\")\n",
        "print(\"  Diffusion Architecture:\")\n",
        "print(\"    - z_H, z_L are progressively denoised\")\n",
        "print(\"    - H-update timing is learned (Gumbel-softmax)\")\n",
        "print(\"    - Uses adaLN conditioning (DiT-style)\")\n",
        "print(\"    - Noise schedules: linear, cosine, or sqrt\")\n",
        "print()\n",
        "print(\"  Hybrid (GRU-based) Architecture:\")\n",
        "print(\"    - z_H, z_L are updated via deterministic GRU\")\n",
        "print(\"    - H-updates are fixed every T steps\")\n",
        "print(\"    - Uses standard LayerNorm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. W&B Hyperparameter Sweep\n",
        "\n",
        "Run a hyperparameter sweep to find optimal settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile sweep_config.yaml\n",
        "program: train_diffusion_sudoku.py\n",
        "method: bayes\n",
        "metric:\n",
        "  name: val/grid_acc\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  d_model:\n",
        "    values: [256, 384, 512]\n",
        "  max_steps:\n",
        "    values: [16, 24, 32]\n",
        "  T:\n",
        "    values: [2, 4, 6]\n",
        "  noise_schedule:\n",
        "    values: [\"linear\", \"cosine\", \"sqrt\"]\n",
        "  lr:\n",
        "    distribution: log_uniform_values\n",
        "    min: 1e-5\n",
        "    max: 1e-3\n",
        "  batch_size:\n",
        "    values: [128, 256, 512]\n",
        "  halt_max_steps:\n",
        "    values: [2, 4, 6]\n",
        "command:\n",
        "  - python\n",
        "  - ${program}\n",
        "  - --epochs\n",
        "  - \"1000\"\n",
        "  - --eval-interval\n",
        "  - \"50\"\n",
        "  - --wandb-project\n",
        "  - diffusion-sudoku-sweep\n",
        "  - ${args}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize sweep (uncomment to run)\n",
        "# This will create a sweep in your W&B project\n",
        "# !wandb sweep sweep_config.yaml\n",
        "\n",
        "# Then run the sweep agent:\n",
        "# !wandb agent YOUR_ENTITY/diffusion-sudoku-sweep/SWEEP_ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook trains a **fully diffusion-based Sudoku solver** where:\n",
        "\n",
        "1. **H,L cycles** use diffusion denoising (not GRU)\n",
        "2. **Timing** for H-updates is learned via Gumbel-softmax\n",
        "3. **adaLN conditioning** provides DiT-style modulation\n",
        "4. **Dual noise schedules** create two-timescale reasoning\n",
        "5. **HRM-style skip connections** ensure stable training\n",
        "\n",
        "### Key Files\n",
        "- `src/pot/core/diffusion_hl_cycles.py` â€” Core diffusion H,L module\n",
        "- `src/pot/models/diffusion_hrm_solver.py` â€” Standalone solver\n",
        "- `tests/test_diffusion_hl_cycles.py` â€” 47 tests (all passing)\n",
        "\n",
        "### W&B Metrics Tracked\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| `train/loss` | Cross-entropy loss on blank cells |\n",
        "| `train/lr` | Current learning rate |\n",
        "| `train/grad_norm` | Gradient norm after clipping |\n",
        "| `train/act_steps` | Number of ACT outer steps used |\n",
        "| `train/q_halt_mean` | Mean Q-value for halting |\n",
        "| `train/q_continue_mean` | Mean Q-value for continuing |\n",
        "| `val/cell_acc` | Cell-level accuracy on validation set |\n",
        "| `val/grid_acc` | Grid-level accuracy (full puzzle) |\n",
        "| `val/avg_steps` | Average ACT steps per sample |\n",
        "\n",
        "### References\n",
        "- [HRM Paper](https://arxiv.org/abs/2506.21734) â€” Two-timescale reasoning\n",
        "- [DiT Paper](https://arxiv.org/abs/2212.09748) â€” Diffusion Transformers (adaLN)\n",
        "- [DDPM Paper](https://arxiv.org/abs/2006.11239) â€” Denoising Diffusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
