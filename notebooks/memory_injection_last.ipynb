{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-clone",
   "metadata": {
    "id": "setup-clone"
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# injection_last.sh - Memory Injection Experiment\n",
    "# Run on RunPod / Colab with GPU\n",
    "\n",
    "# Clone repo\n",
    "!git clone https://github.com/Eran-BA/PoT.git\n",
    "%cd PoT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-deps",
   "metadata": {
    "id": "setup-deps"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install tqdm numpy huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wandb-login",
   "metadata": {
    "id": "wandb-login"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-broadcast-phase1",
   "metadata": {
    "id": "baseline-broadcast-phase1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Phase 1: Baseline Broadcast (no memory) - 1500 epochs\n",
    "# ============================================================\n",
    "# This is the baseline. If you already have a trained broadcast\n",
    "# checkpoint, skip this cell and use --resume in the next cells.\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast \\\n",
    "    --epochs 1500 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 2 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name baseline-broadcast-1500ep \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-broadcast-phase2",
   "metadata": {
    "id": "baseline-broadcast-phase2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Phase 2: Baseline Broadcast - scale to halt_max_steps=4\n",
    "# ============================================================\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 4 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name baseline-broadcast-halt4 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attn-phase1",
   "metadata": {
    "id": "cross-attn-phase1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cross-Attention with Memory Preservation - Phase 1\n",
    "# ============================================================\n",
    "# injection_memory now persists across ACT steps via ACTCarry.\n",
    "# The memory bank accumulates controller states from all prior\n",
    "# ACT steps (capped at memory_size=16).\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode cross_attn \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 1500 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 2 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name cross-attn-memory-1500ep \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attn-phase2",
   "metadata": {
    "id": "cross-attn-phase2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cross-Attention with Memory - scale to halt_max_steps=4\n",
    "# ============================================================\n",
    "# More ACT steps = more memory entries accumulated.\n",
    "# This is where memory preservation should shine.\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode cross_attn \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 4 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name cross-attn-memory-halt4 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attn-phase3",
   "metadata": {
    "id": "cross-attn-phase3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cross-Attention with Memory - scale to halt_max_steps=6\n",
    "# ============================================================\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode cross_attn \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 6 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name cross-attn-memory-halt6 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-memory-phase1",
   "metadata": {
    "id": "broadcast-memory-phase1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Broadcast Memory - Phase 1\n",
    "# ============================================================\n",
    "# New mode: memory bank + learned attention summary + gated broadcast.\n",
    "# Combines broadcast simplicity with cross_attn memory accumulation.\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast_memory \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 1500 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 2 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name broadcast-memory-1500ep \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-memory-phase2",
   "metadata": {
    "id": "broadcast-memory-phase2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Broadcast Memory - scale to halt_max_steps=4\n",
    "# ============================================================\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast_memory \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 4 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name broadcast-memory-halt4 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-memory-phase3",
   "metadata": {
    "id": "broadcast-memory-phase3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Broadcast Memory - scale to halt_max_steps=6\n",
    "# ============================================================\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast_memory \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 2 \\\n",
    "    --L-cycles 6 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 6 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name broadcast-memory-halt6 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-memory-phase4",
   "metadata": {
    "id": "broadcast-memory-phase4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Broadcast Memory - scale to halt_max_steps=8, bigger cycles\n",
    "# ============================================================\n",
    "\n",
    "!python experiments/sudoku_poh_benchmark.py \\\n",
    "    --d-model 512 \\\n",
    "    --d-ff 2048 \\\n",
    "    --model hybrid \\\n",
    "    --controller transformer \\\n",
    "    --d-ctrl 256 \\\n",
    "    --max-depth 32 \\\n",
    "    --injection-mode broadcast_memory \\\n",
    "    --injection-memory-size 16 \\\n",
    "    --injection-n-heads 4 \\\n",
    "    --epochs 3000 \\\n",
    "    --batch-size 768 \\\n",
    "    --lr 3e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --n-heads 8 \\\n",
    "    --H-cycles 4 \\\n",
    "    --L-cycles 12 \\\n",
    "    --H-layers 2 \\\n",
    "    --L-layers 2 \\\n",
    "    --hrm-grad-style \\\n",
    "    --halt-max-steps 8 \\\n",
    "    --eval-interval 25 \\\n",
    "    --dropout 0.039 \\\n",
    "    --wandb \\\n",
    "    --project memory-injection-experiment \\\n",
    "    --run-name broadcast-memory-halt8 \\\n",
    "    --resume \"eranbt92-open-university-of-israel/memory-injection-experiment/hybrid-transformer-best:latest\" \\\n",
    "    --download"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
