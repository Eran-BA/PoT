{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a45a61-ba51-492f-a972-42d6a0ec0234",
      "metadata": {
        "id": "26a45a61-ba51-492f-a972-42d6a0ec0234"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "# train_broadcast_memory.sh - Run on RunPod\n",
        "\n",
        "# Clone repo\n",
        "!git clone https://github.com/Eran-BA/PoT.git\n",
        "%cd PoT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423abbff-6126-4210-a50f-9c16a41ee547",
      "metadata": {
        "id": "423abbff-6126-4210-a50f-9c16a41ee547"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install tqdm numpy huggingface_hub wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a2d6a2-f008-467d-888e-4a5237f5bca3",
      "metadata": {
        "id": "a8a2d6a2-f008-467d-888e-4a5237f5bca3"
      },
      "outputs": [],
      "source": [
        "!pip install -U typing_extensions\n",
        "# Or specifically:\n",
        "# pip install typing_extensions>=4.7.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d803809f-3797-4a99-abdd-3052e9c3b2b1",
      "metadata": {
        "id": "d803809f-3797-4a99-abdd-3052e9c3b2b1"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases\n",
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41bc3be-1079-4416-bc3d-2d3b40bfaa28",
      "metadata": {
        "id": "d41bc3be-1079-4416-bc3d-2d3b40bfaa28"
      },
      "outputs": [],
      "source": [
        "\n",
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --model hybrid \\\n",
        "    --controller transformer \\\n",
        "    --d-ctrl 256 \\\n",
        "    --max-depth 32 \\\n",
        "    --injection-mode broadcast_memory \\\n",
        "    --injection-memory-size 16 \\\n",
        "    --injection-n-heads 4 \\\n",
        "    --epochs 1500 \\\n",
        "    --batch-size 768 \\\n",
        "    --lr 3e-4 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --d-model 512 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-cycles 2 \\\n",
        "    --L-cycles 6 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --hrm-grad-style \\\n",
        "    --halt-max-steps 2 \\\n",
        "    --eval-interval 25 \\\n",
        "    --dropout 0.039 \\\n",
        "    --wandb \\\n",
        "    --project feature-injection-ablation \\\n",
        "    --run-name broadcast-memory-1500ep \\\n",
        "    --download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PfWghYTnv0MT",
      "metadata": {
        "id": "PfWghYTnv0MT"
      },
      "outputs": [],
      "source": [
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --model hybrid \\\n",
        "    --controller transformer \\\n",
        "    --d-ctrl 256 \\\n",
        "    --max-depth 32 \\\n",
        "    --injection-mode broadcast_memory \\\n",
        "    --injection-memory-size 16 \\\n",
        "    --injection-n-heads 4 \\\n",
        "    --epochs 3000 \\\n",
        "    --batch-size 768 \\\n",
        "    --lr 3e-4 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-cycles 2 \\\n",
        "    --L-cycles 6 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --hrm-grad-style \\\n",
        "    --halt-max-steps 3 \\\n",
        "    --eval-interval 25 \\\n",
        "    --dropout 0.039 \\\n",
        "    --wandb \\\n",
        "    --project feature-injection-ablation \\\n",
        "    --run-name broadcast-memory-halt3 \\\n",
        "    --resume \"eranbt92-open-university-of-israel/feature-injection-ablation/hybrid-transformer-best:latest\" \\\n",
        "    --download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AM71GC8aYfo7",
      "metadata": {
        "id": "AM71GC8aYfo7"
      },
      "outputs": [],
      "source": [
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --model hybrid \\\n",
        "    --controller transformer \\\n",
        "    --d-ctrl 256 \\\n",
        "    --max-depth 32 \\\n",
        "    --injection-mode broadcast_memory \\\n",
        "    --injection-memory-size 16 \\\n",
        "    --injection-n-heads 4 \\\n",
        "    --epochs 3000 \\\n",
        "    --batch-size 768 \\\n",
        "    --lr 3e-4 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-cycles 2 \\\n",
        "    --L-cycles 6 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --hrm-grad-style \\\n",
        "    --halt-max-steps 4 \\\n",
        "    --eval-interval 25 \\\n",
        "    --dropout 0.039 \\\n",
        "    --wandb \\\n",
        "    --project feature-injection-ablation \\\n",
        "    --run-name broadcast-memory-halt4 \\\n",
        "    --resume \"eranbt92-open-university-of-israel/feature-injection-ablation/hybrid-transformer-best:latest\" \\\n",
        "    --download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2RQoC-p4Kd_u",
      "metadata": {
        "id": "2RQoC-p4Kd_u"
      },
      "outputs": [],
      "source": [
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --model hybrid \\\n",
        "    --controller transformer \\\n",
        "    --d-ctrl 256 \\\n",
        "    --max-depth 32 \\\n",
        "    --injection-mode broadcast_memory \\\n",
        "    --injection-memory-size 16 \\\n",
        "    --injection-n-heads 4 \\\n",
        "    --epochs 3000 \\\n",
        "    --batch-size 768 \\\n",
        "    --lr 3e-4 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-cycles 2 \\\n",
        "    --L-cycles 6 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --hrm-grad-style \\\n",
        "    --halt-max-steps 6 \\\n",
        "    --eval-interval 25 \\\n",
        "    --dropout 0.039 \\\n",
        "    --wandb \\\n",
        "    --project feature-injection-ablation \\\n",
        "    --run-name broadcast-memory-halt6 \\\n",
        "    --resume \"eranbt92-open-university-of-israel/feature-injection-ablation/hybrid-transformer-best:latest\" \\\n",
        "    --download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "An3lt5ahh1PM",
      "metadata": {
        "id": "An3lt5ahh1PM"
      },
      "outputs": [],
      "source": [
        "!python experiments/sudoku_poh_benchmark.py \\\n",
        "    --d-model 512 \\\n",
        "    --d-ff 2048 \\\n",
        "    --model hybrid \\\n",
        "    --controller transformer \\\n",
        "    --d-ctrl 256 \\\n",
        "    --max-depth 32 \\\n",
        "    --injection-mode broadcast_memory \\\n",
        "    --injection-memory-size 16 \\\n",
        "    --injection-n-heads 4 \\\n",
        "    --epochs 4500 \\\n",
        "    --batch-size 768 \\\n",
        "    --lr 3e-4 \\\n",
        "    --warmup-steps 2000 \\\n",
        "    --n-heads 8 \\\n",
        "    --H-cycles 4 \\\n",
        "    --L-cycles 12 \\\n",
        "    --H-layers 2 \\\n",
        "    --L-layers 2 \\\n",
        "    --hrm-grad-style \\\n",
        "    --halt-max-steps 8 \\\n",
        "    --eval-interval 25 \\\n",
        "    --dropout 0.039 \\\n",
        "    --wandb \\\n",
        "    --project feature-injection-ablation \\\n",
        "    --run-name broadcast-memory-halt8 \\\n",
        "    --resume \"eranbt92-open-university-of-israel/feature-injection-ablation/hybrid-transformer-best:latest\" \\\n",
        "    --download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6AnpBJ30Q1o6",
      "metadata": {
        "id": "6AnpBJ30Q1o6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
