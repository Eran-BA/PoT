{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sudoku-Extreme Training with Alternative Depth Controllers\n",
    "\n",
    "This notebook trains the HybridPoHHRMSolver on Sudoku-Extreme with different depth controllers:\n",
    "- **LSTM Controller** - Stronger gating than GRU\n",
    "- **Transformer Controller** - Causal attention over depth history\n",
    "\n",
    "## Features\n",
    "- On-the-fly Sudoku augmentation (digit permutation, transpose, row/col shuffling)\n",
    "- W&B logging (optional)\n",
    "- Checkpoint saving\n",
    "- Cosine LR schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment if running on Colab)\n",
    "# !pip install torch wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pot.models.sudoku_solver import HybridPoHHRMSolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# =============================================================================\n",
    "\n",
    "config = {\n",
    "    # Controller type: \"lstm\", \"transformer\", \"gru\", \"xlstm\", \"mingru\"\n",
    "    \"controller_type\": \"lstm\",  # <-- CHANGE THIS TO SWITCH CONTROLLERS\n",
    "    \n",
    "    # Data\n",
    "    \"data_dir\": \"../data/sudoku-extreme\",\n",
    "    \n",
    "    # Model architecture\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"h_layers\": 2,\n",
    "    \"l_layers\": 2,\n",
    "    \"h_cycles\": 2,\n",
    "    \"l_cycles\": 8,\n",
    "    \"halt_max_steps\": 4,\n",
    "    \n",
    "    # Transformer controller specific (only used if controller_type=\"transformer\")\n",
    "    \"d_ctrl\": 256,\n",
    "    \"n_ctrl_layers\": 2,\n",
    "    \"n_ctrl_heads\": 4,\n",
    "    \"max_depth\": 32,\n",
    "    \n",
    "    # Training\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 768,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"augment\": True,  # On-the-fly Sudoku augmentation\n",
    "    \n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \n",
    "    # Logging\n",
    "    \"use_wandb\": False,  # Set to True to enable W&B logging\n",
    "    \"wandb_project\": \"sudoku-controllers\",\n",
    "    \n",
    "    # Checkpoints\n",
    "    \"save_dir\": \"../checkpoints\",\n",
    "    \"save_every\": 50,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sudoku Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply validity-preserving augmentation to a Sudoku puzzle.\n",
    "    \n",
    "    Transforms include:\n",
    "    - Digit permutation (1-9 -> random permutation)\n",
    "    - Transpose (50% chance)\n",
    "    - Row band shuffling (shuffle the 3 bands of 3 rows each)\n",
    "    - Row shuffling within bands\n",
    "    - Column stack shuffling (shuffle the 3 stacks of 3 columns each)  \n",
    "    - Column shuffling within stacks\n",
    "    \n",
    "    Args:\n",
    "        board: Input puzzle [81] with 0=blank, 1-9=digits\n",
    "        solution: Solution [81] with 1-9\n",
    "        \n",
    "    Returns:\n",
    "        Augmented (board, solution) tuple\n",
    "    \"\"\"\n",
    "    # Create a random digit mapping: a permutation of 1..9, with zero (blank) unchanged\n",
    "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
    "    \n",
    "    # Randomly decide whether to transpose\n",
    "    transpose_flag = np.random.rand() < 0.5\n",
    "\n",
    "    # Generate a valid row permutation:\n",
    "    # - Shuffle the 3 bands (each band = 3 rows) and for each band, shuffle its 3 rows.\n",
    "    bands = np.random.permutation(3)\n",
    "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
    "\n",
    "    # Similarly for columns (stacks).\n",
    "    stacks = np.random.permutation(3)\n",
    "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
    "\n",
    "    # Build an 81->81 mapping\n",
    "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
    "\n",
    "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
    "        # Reshape to 9x9 for transpose\n",
    "        x_2d = x.reshape(9, 9)\n",
    "        if transpose_flag:\n",
    "            x_2d = x_2d.T\n",
    "        x_flat = x_2d.flatten()\n",
    "        # Apply row/col permutation\n",
    "        new_board = x_flat[mapping]\n",
    "        # Apply digit mapping\n",
    "        return digit_map[new_board]\n",
    "\n",
    "    return apply_transformation(board), apply_transformation(solution)\n",
    "\n",
    "\n",
    "class SudokuDataset(Dataset):\n",
    "    \"\"\"Sudoku dataset with on-the-fly augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, solutions, puzzle_ids=None, augment=True):\n",
    "        self.inputs = inputs\n",
    "        self.solutions = solutions\n",
    "        self.puzzle_ids = puzzle_ids if puzzle_ids is not None else torch.zeros(len(inputs), dtype=torch.long)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.inputs[idx].numpy() if isinstance(self.inputs[idx], torch.Tensor) else self.inputs[idx]\n",
    "        sol = self.solutions[idx].numpy() if isinstance(self.solutions[idx], torch.Tensor) else self.solutions[idx]\n",
    "        pid = self.puzzle_ids[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            inp, sol = shuffle_sudoku(inp, sol)\n",
    "        \n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(sol, dtype=torch.long), pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sudoku_data(data_dir: str):\n",
    "    \"\"\"\n",
    "    Load Sudoku-Extreme dataset.\n",
    "    \n",
    "    Expected format: .pt files with 'inputs' and 'solutions' tensors.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    \n",
    "    # Try to load preprocessed .pt files\n",
    "    train_pt = data_path / \"train.pt\"\n",
    "    val_pt = data_path / \"val.pt\"\n",
    "    \n",
    "    if train_pt.exists() and val_pt.exists():\n",
    "        print(f\"Loading preprocessed data from {data_dir}...\")\n",
    "        train_data = torch.load(train_pt, map_location=\"cpu\")\n",
    "        val_data = torch.load(val_pt, map_location=\"cpu\")\n",
    "        print(f\"  Train samples: {len(train_data['inputs'])}\")\n",
    "        print(f\"  Val samples: {len(val_data['inputs'])}\")\n",
    "    else:\n",
    "        # Create synthetic data for testing\n",
    "        print(\"âš ï¸ No preprocessed data found. Creating synthetic data for testing...\")\n",
    "        print(\"   Place train.pt and val.pt in the data directory for real training.\")\n",
    "        n_train, n_val = 1000, 200\n",
    "        train_data = {\n",
    "            'inputs': torch.randint(0, 10, (n_train, 81)),\n",
    "            'solutions': torch.randint(1, 10, (n_train, 81)),\n",
    "            'puzzle_ids': torch.zeros(n_train, dtype=torch.long),\n",
    "        }\n",
    "        val_data = {\n",
    "            'inputs': torch.randint(0, 10, (n_val, 81)),\n",
    "            'solutions': torch.randint(1, 10, (n_val, 81)),\n",
    "            'puzzle_ids': torch.zeros(n_val, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def create_dataloaders(train_data, val_data, batch_size: int, augment: bool = True):\n",
    "    \"\"\"Create DataLoaders from data dicts with on-the-fly augmentation.\"\"\"\n",
    "    \n",
    "    # Get or create puzzle IDs\n",
    "    train_ids = train_data.get('puzzle_ids', torch.zeros(len(train_data['inputs']), dtype=torch.long))\n",
    "    val_ids = val_data.get('puzzle_ids', torch.zeros(len(val_data['inputs']), dtype=torch.long))\n",
    "    \n",
    "    # Use SudokuDataset with augmentation for training\n",
    "    train_dataset = SudokuDataset(\n",
    "        train_data['inputs'],\n",
    "        train_data['solutions'],\n",
    "        train_ids,\n",
    "        augment=augment,\n",
    "    )\n",
    "    \n",
    "    # No augmentation for validation\n",
    "    val_dataset = SudokuDataset(\n",
    "        val_data['inputs'],\n",
    "        val_data['solutions'],\n",
    "        val_ids,\n",
    "        augment=False,\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, val_data = load_sudoku_data(config[\"data_dir\"])\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_data, val_data, \n",
    "    batch_size=config[\"batch_size\"],\n",
    "    augment=config[\"augment\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    \"\"\"Create HybridPoHHRMSolver with specified controller type.\"\"\"\n",
    "    \n",
    "    controller_type = config[\"controller_type\"]\n",
    "    \n",
    "    # Build controller kwargs based on type\n",
    "    if controller_type == \"transformer\":\n",
    "        controller_kwargs = {\n",
    "            \"d_ctrl\": config[\"d_ctrl\"],\n",
    "            \"n_ctrl_layers\": config[\"n_ctrl_layers\"],\n",
    "            \"n_ctrl_heads\": config[\"n_ctrl_heads\"],\n",
    "            \"max_depth\": config[\"max_depth\"],\n",
    "            \"token_conditioned\": True,\n",
    "        }\n",
    "    else:\n",
    "        # LSTM, GRU, xLSTM, minGRU\n",
    "        controller_kwargs = {\n",
    "            \"d_ctrl\": config[\"d_model\"],\n",
    "            \"token_conditioned\": True,\n",
    "        }\n",
    "    \n",
    "    model = HybridPoHHRMSolver(\n",
    "        d_model=config[\"d_model\"],\n",
    "        n_heads=config[\"n_heads\"],\n",
    "        H_layers=config[\"h_layers\"],\n",
    "        L_layers=config[\"l_layers\"],\n",
    "        H_cycles=config[\"h_cycles\"],\n",
    "        L_cycles=config[\"l_cycles\"],\n",
    "        halt_max_steps=config[\"halt_max_steps\"],\n",
    "        controller_type=controller_type,\n",
    "        controller_kwargs=controller_kwargs,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = config[\"device\"]\n",
    "model = create_model(config).to(device)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created on {device}\")\n",
    "print(f\"  Controller: {config['controller_type'].upper()}\")\n",
    "print(f\"  Total parameters: {param_count:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_cells = 0\n",
    "    total_grids_correct = 0\n",
    "    total_grids = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for inputs, targets, puzzle_ids in pbar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        puzzle_ids = puzzle_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs, puzzle_ids)\n",
    "        logits = outputs[0]  # [B, 81, 10]\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, 10),\n",
    "            targets.view(-1),\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct = (preds == targets).sum().item()\n",
    "        total_correct += correct\n",
    "        total_cells += targets.numel()\n",
    "        \n",
    "        # Grid accuracy\n",
    "        grid_correct = (preds == targets).all(dim=1).sum().item()\n",
    "        total_grids_correct += grid_correct\n",
    "        total_grids += targets.size(0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'cell': f'{100*correct/targets.numel():.1f}%',\n",
    "            'grid': f'{100*grid_correct/targets.size(0):.1f}%',\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'cell_acc': total_correct / total_cells,\n",
    "        'grid_acc': total_grids_correct / total_grids,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_cells = 0\n",
    "    total_grids_correct = 0\n",
    "    total_grids = 0\n",
    "    \n",
    "    for inputs, targets, puzzle_ids in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        puzzle_ids = puzzle_ids.to(device)\n",
    "        \n",
    "        outputs = model(inputs, puzzle_ids)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, 10),\n",
    "            targets.view(-1),\n",
    "        )\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == targets).sum().item()\n",
    "        total_cells += targets.numel()\n",
    "        total_grids_correct += (preds == targets).all(dim=1).sum().item()\n",
    "        total_grids += targets.size(0)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'cell_acc': total_correct / total_cells,\n",
    "        'grid_acc': total_grids_correct / total_grids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Scheduler (cosine annealing)\n",
    "total_steps = config[\"epochs\"] * len(train_loader)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={config['lr']}, weight_decay={config['weight_decay']})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR (T_max={total_steps})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. W&B Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B if enabled\n",
    "if config[\"use_wandb\"]:\n",
    "    import wandb\n",
    "    \n",
    "    run_name = f\"{config['controller_type']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    wandb.init(\n",
    "        project=config[\"wandb_project\"],\n",
    "        name=run_name,\n",
    "        config=config,\n",
    "    )\n",
    "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
    "    print(f\"W&B initialized: {run_name}\")\n",
    "else:\n",
    "    print(\"W&B logging disabled. Set config['use_wandb'] = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "save_dir = Path(config[\"save_dir\"]) / config[\"controller_type\"]\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_cell_acc': [],\n",
    "    'train_grid_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_cell_acc': [],\n",
    "    'val_grid_acc': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "best_grid_acc = 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting {config['controller_type'].upper()} Controller Training\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_cell_acc'].append(train_metrics['cell_acc'])\n",
    "    history['train_grid_acc'].append(train_metrics['grid_acc'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_cell_acc'].append(val_metrics['cell_acc'])\n",
    "    history['val_grid_acc'].append(val_metrics['grid_acc'])\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}/{config['epochs']} | \"\n",
    "          f\"Train: loss={train_metrics['loss']:.4f}, cell={100*train_metrics['cell_acc']:.1f}%, grid={100*train_metrics['grid_acc']:.1f}% | \"\n",
    "          f\"Val: loss={val_metrics['loss']:.4f}, cell={100*val_metrics['cell_acc']:.1f}%, grid={100*val_metrics['grid_acc']:.1f}%\")\n",
    "    \n",
    "    # W&B logging\n",
    "    if config[\"use_wandb\"]:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train/loss\": train_metrics['loss'],\n",
    "            \"train/cell_acc\": train_metrics['cell_acc'],\n",
    "            \"train/grid_acc\": train_metrics['grid_acc'],\n",
    "            \"val/loss\": val_metrics['loss'],\n",
    "            \"val/cell_acc\": val_metrics['cell_acc'],\n",
    "            \"val/grid_acc\": val_metrics['grid_acc'],\n",
    "            \"lr\": current_lr,\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['grid_acc'] > best_grid_acc:\n",
    "        best_grid_acc = val_metrics['grid_acc']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_grid_acc': best_grid_acc,\n",
    "            'config': config,\n",
    "        }, save_dir / \"best_model.pt\")\n",
    "        print(f\"  âœ“ New best! Grid acc: {100*best_grid_acc:.2f}%\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if epoch % config[\"save_every\"] == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'grid_acc': val_metrics['grid_acc'],\n",
    "            'config': config,\n",
    "        }, save_dir / f\"checkpoint_epoch{epoch}.pt\")\n",
    "        print(f\"  ðŸ’¾ Checkpoint saved: epoch {epoch}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best grid accuracy: {100*best_grid_acc:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if config[\"use_wandb\"]:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cell Accuracy\n",
    "axes[1].plot([100*x for x in history['train_cell_acc']], label='Train')\n",
    "axes[1].plot([100*x for x in history['val_cell_acc']], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Cell Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Grid Accuracy\n",
    "axes[2].plot([100*x for x in history['train_grid_acc']], label='Train')\n",
    "axes[2].plot([100*x for x in history['val_grid_acc']], label='Val')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Grid Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"{config['controller_type'].upper()} Controller Training\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to: {save_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Best Model & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(save_dir / \"best_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Best grid accuracy: {100*checkpoint['best_grid_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Final Evaluation Results\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Controller: {config['controller_type'].upper()}\")\n",
    "print(f\"Val Loss: {final_metrics['loss']:.4f}\")\n",
    "print(f\"Cell Accuracy: {100*final_metrics['cell_acc']:.2f}%\")\n",
    "print(f\"Grid Accuracy: {100*final_metrics['grid_acc']:.2f}%\")\n",
    "print(f\"{'='*40}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
