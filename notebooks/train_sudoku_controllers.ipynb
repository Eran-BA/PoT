{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sudoku-Extreme Training with Alternative Depth Controllers\n",
        "\n",
        "This notebook trains the HybridPoHHRMSolver on Sudoku-Extreme with different depth controllers:\n",
        "- **LSTM Controller** - Stronger gating than GRU\n",
        "- **Transformer Controller** - Causal attention over depth history\n",
        "- **PoT Transformer Controller** - **Nested PoT** with gated MHA internally\n",
        "- **GRU Controller** - Original baseline\n",
        "- **xLSTM Controller** - Exponential gating\n",
        "- **minGRU Controller** - Simplified GRU\n",
        "\n",
        "## Features\n",
        "- On-the-fly Sudoku augmentation (digit permutation, transpose, row/col shuffling)\n",
        "- W&B logging (optional)\n",
        "- Checkpoint saving\n",
        "- Cosine LR schedule\n",
        "\n",
        "## Quick Start (Colab)\n",
        "1. Run \"0. Colab Setup\" to clone repo and install deps\n",
        "2. Run \"1. Download Dataset\" to get Sudoku-Extreme from HuggingFace\n",
        "3. Run remaining cells to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Colab Setup (Run this first if on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# COLAB SETUP - Run this cell first if you're on Google Colab\n",
        "# ============================================================================\n",
        "\n",
        "# Check if running on Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone the repository\n",
        "    !git clone https://github.com/Eran-BA/PoT.git /content/PoT 2>/dev/null || (cd /content/PoT && git pull)\n",
        "    %cd /content/PoT\n",
        "    \n",
        "    # Install dependencies\n",
        "    !pip install -q wandb tqdm huggingface_hub\n",
        "    \n",
        "    # Add to path\n",
        "    sys.path.insert(0, '/content/PoT')\n",
        "    \n",
        "    print(\"✓ Colab setup complete!\")\n",
        "    print(f\"  Working directory: {%pwd}\")\n",
        "else:\n",
        "    print(\"Not running on Colab - skipping setup\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download Sudoku-Extreme Dataset\n",
        "\n",
        "Downloads raw puzzles from HuggingFace (`sapientinc/sudoku-extreme`).\n",
        "- 10,000 training puzzles (augmentation applied **on-the-fly** during training)\n",
        "- 1,000 validation puzzles (no augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# DOWNLOAD SUDOKU-EXTREME DATASET (raw puzzles - augmentation is on-the-fly)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def download_sudoku_dataset(output_dir, subsample_size=10000, val_size=1000, download_test=True):\n",
        "    \"\"\"\n",
        "    Download Sudoku-Extreme from HuggingFace and save as .pt files.\n",
        "    \n",
        "    Augmentation is NOT applied here - it happens on-the-fly during training\n",
        "    via the SudokuDataset class with shuffle_sudoku().\n",
        "    \n",
        "    Args:\n",
        "        output_dir: Where to save train.pt, val.pt, and optionally test.pt\n",
        "        subsample_size: Number of training puzzles to use\n",
        "        val_size: Number of validation puzzles\n",
        "        download_test: If True, also download the full test set (422k puzzles)\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Check if already downloaded\n",
        "    has_train_val = os.path.exists(f\"{output_dir}/train.pt\") and os.path.exists(f\"{output_dir}/val.pt\")\n",
        "    has_test = os.path.exists(f\"{output_dir}/test.pt\")\n",
        "    \n",
        "    if has_train_val and (not download_test or has_test):\n",
        "        train_data = torch.load(f\"{output_dir}/train.pt\")\n",
        "        val_data = torch.load(f\"{output_dir}/val.pt\")\n",
        "        print(f\"✓ Dataset already exists at {output_dir}\")\n",
        "        print(f\"  Train: {len(train_data['inputs'])} puzzles\")\n",
        "        print(f\"  Val: {len(val_data['inputs'])} puzzles\")\n",
        "        if has_test:\n",
        "            test_data = torch.load(f\"{output_dir}/test.pt\")\n",
        "            print(f\"  Test: {len(test_data['inputs'])} puzzles\")\n",
        "        return\n",
        "    \n",
        "    print(\"Downloading Sudoku-Extreme from HuggingFace...\")\n",
        "    \n",
        "    # Download train CSV from HuggingFace\n",
        "    csv_path = hf_hub_download(\n",
        "        repo_id=\"sapientinc/sudoku-extreme\",\n",
        "        filename=\"train.csv\",\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "    \n",
        "    # Parse CSV\n",
        "    inputs, solutions = [], []\n",
        "    with open(csv_path, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for row in tqdm(reader, desc=\"Parsing CSV\"):\n",
        "            source, q, a, rating = row\n",
        "            # Convert puzzle string to numpy array\n",
        "            # '.' or '0' = blank, '1'-'9' = digits\n",
        "            inp = np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8) - ord('0')\n",
        "            sol = np.frombuffer(a.encode(), dtype=np.uint8) - ord('0')\n",
        "            inputs.append(inp)\n",
        "            solutions.append(sol)\n",
        "    \n",
        "    print(f\"  Total puzzles in dataset: {len(inputs)}\")\n",
        "    \n",
        "    # Shuffle and split\n",
        "    total = len(inputs)\n",
        "    indices = np.random.permutation(total)\n",
        "    \n",
        "    train_size = min(subsample_size, total - val_size)\n",
        "    train_idx = indices[:train_size]\n",
        "    val_idx = indices[train_size:train_size + val_size]\n",
        "    \n",
        "    # Save train (raw - augmentation happens on-the-fly in DataLoader!)\n",
        "    train_inputs = torch.tensor(np.array([inputs[i] for i in train_idx]), dtype=torch.long)\n",
        "    train_solutions = torch.tensor(np.array([solutions[i] for i in train_idx]), dtype=torch.long)\n",
        "    torch.save({\n",
        "        'inputs': train_inputs,\n",
        "        'solutions': train_solutions,\n",
        "    }, f\"{output_dir}/train.pt\")\n",
        "    print(f\"✓ Train: {len(train_idx)} puzzles saved (augmentation: ON-THE-FLY)\")\n",
        "    \n",
        "    # Save val (no augmentation ever)\n",
        "    val_inputs = torch.tensor(np.array([inputs[i] for i in val_idx]), dtype=torch.long)\n",
        "    val_solutions = torch.tensor(np.array([solutions[i] for i in val_idx]), dtype=torch.long)\n",
        "    torch.save({\n",
        "        'inputs': val_inputs,\n",
        "        'solutions': val_solutions,\n",
        "    }, f\"{output_dir}/val.pt\")\n",
        "    print(f\"✓ Val: {len(val_idx)} puzzles saved\")\n",
        "    \n",
        "    # Download and save full test set (422k puzzles)\n",
        "    if download_test:\n",
        "        print(\"\\nDownloading full test set (422k puzzles)...\")\n",
        "        test_csv_path = hf_hub_download(\n",
        "            repo_id=\"sapientinc/sudoku-extreme\",\n",
        "            filename=\"test.csv\",\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "        \n",
        "        test_inputs, test_solutions = [], []\n",
        "        with open(test_csv_path, newline=\"\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)  # Skip header\n",
        "            for row in tqdm(reader, desc=\"Parsing test CSV\"):\n",
        "                source, q, a, rating = row\n",
        "                inp = np.frombuffer(q.replace('.', '0').encode(), dtype=np.uint8) - ord('0')\n",
        "                sol = np.frombuffer(a.encode(), dtype=np.uint8) - ord('0')\n",
        "                test_inputs.append(inp)\n",
        "                test_solutions.append(sol)\n",
        "        \n",
        "        test_inputs_t = torch.tensor(np.array(test_inputs), dtype=torch.long)\n",
        "        test_solutions_t = torch.tensor(np.array(test_solutions), dtype=torch.long)\n",
        "        torch.save({\n",
        "            'inputs': test_inputs_t,\n",
        "            'solutions': test_solutions_t,\n",
        "        }, f\"{output_dir}/test.pt\")\n",
        "        print(f\"✓ Test: {len(test_inputs)} puzzles saved\")\n",
        "    \n",
        "    print(f\"\\n✓ Dataset saved to {output_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download the dataset\n",
        "# Adjust paths for Colab vs local\n",
        "\n",
        "if IN_COLAB:\n",
        "    DATA_DIR = \"/content/PoT/data/sudoku-extreme-10k\"\n",
        "else:\n",
        "    DATA_DIR = \"../data/sudoku-extreme-10k\"\n",
        "\n",
        "download_sudoku_dataset(\n",
        "    output_dir=DATA_DIR,\n",
        "    subsample_size=10000,  # 10k training puzzles\n",
        "    val_size=1000,         # 1k validation puzzles\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies if needed (uncomment if running on Colab)\n",
        "# !pip install torch wandb tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = Path(\".\").resolve().parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from src.pot.models.sudoku_solver import HybridPoHHRMSolver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - Modify these settings as needed\n",
        "# =============================================================================\n",
        "\n",
        "config = {\n",
        "    # Controller type: \"lstm\", \"transformer\", \"pot_transformer\", \"gru\", \"xlstm\", \"mingru\"\n",
        "    # pot_transformer = Nested PoT (gated MHA inside the depth controller itself)\n",
        "    \"controller_type\": \"lstm\",  # <-- CHANGE THIS TO SWITCH CONTROLLERS\n",
        "    \n",
        "    # Data (uses DATA_DIR from download step)\n",
        "    \"data_dir\": DATA_DIR,\n",
        "    \n",
        "    # Model architecture\n",
        "    \"d_model\": 512,\n",
        "    \"d_ff\": 2048,  # Feedforward hidden dimension (typically 4x d_model)\n",
        "    \"n_heads\": 8,\n",
        "    \"h_layers\": 2,\n",
        "    \"l_layers\": 2,\n",
        "    \"h_cycles\": 2,\n",
        "    \"l_cycles\": 8,\n",
        "    \"dropout\": 0.01,  # Dropout rate\n",
        "    \n",
        "    # HRM/ACT parameters\n",
        "    \"T\": 4,  # HRM period for pointer controller\n",
        "    \"halt_max_steps\": 2,  # Max ACT outer steps (1 = no ACT)\n",
        "    \"halt_exploration_prob\": 0.1,  # Q-learning exploration probability\n",
        "    \"hrm_grad_style\": True,  # Only last L+H cycles get gradients (HRM-style)\n",
        "    \n",
        "    # Transformer/PoT-Transformer controller specific \n",
        "    # (only used if controller_type=\"transformer\" or \"pot_transformer\")\n",
        "    \"d_ctrl\": 256,\n",
        "    \"n_ctrl_layers\": 2,\n",
        "    \"n_ctrl_heads\": 4,\n",
        "    \"max_depth\": 32,\n",
        "    \n",
        "    # Training\n",
        "    \"epochs\": 1000,\n",
        "    \"batch_size\": 768,\n",
        "    \"lr\": 1e-4,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"beta2\": 0.95,  # AdamW beta2 (Llama-style, also used in HRM)\n",
        "    \"warmup_steps\": 1000,\n",
        "    \"lr_min_ratio\": 0.1,  # Cosine decay floor (10% of peak LR)\n",
        "    \"augment\": True,  # On-the-fly Sudoku augmentation\n",
        "    \"eval_interval\": 10,  # Evaluate every N epochs\n",
        "    \"halt_histogram_interval\": 50,  # Track halt histograms every N epochs\n",
        "    \"num_workers\": 4,  # DataLoader workers (parallel data loading)\n",
        "    \n",
        "    # Async batching (HRM-style) - samples that halt early are replaced immediately\n",
        "    \"async_batch\": True,  # HRM-style async batching for maximum GPU utilization\n",
        "    \n",
        "    # Puzzle embedding optimizer (HRM-style dual optimizer)\n",
        "    \"use_puzzle_optimizer\": True,  # Separate optimizer for puzzle embeddings\n",
        "    \"puzzle_lr_multiplier\": 100.0,  # Puzzle LR = lr * multiplier (HRM-style: puzzle embeds learn 100x faster)\n",
        "    \"puzzle_weight_decay\": 0.1,\n",
        "    \"puzzle_optimizer\": \"adamw\",  # \"adamw\" or \"signsgd\"\n",
        "    \n",
        "    # Device\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \n",
        "    # Logging\n",
        "    \"use_wandb\": False,  # Set to True to enable W&B logging\n",
        "    \"wandb_project\": \"sudoku-controllers\",\n",
        "    \n",
        "    # Checkpoints\n",
        "    \"save_dir\": \"../checkpoints\",\n",
        "    \"save_every\": 50,\n",
        "    \"resume_from\": None,  # Path to checkpoint to resume from (e.g., \"checkpoints/best_model.pt\")\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sudoku Augmentation (On-the-Fly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
        "    \"\"\"\n",
        "    Apply validity-preserving augmentation to a Sudoku puzzle.\n",
        "    \n",
        "    Transforms include:\n",
        "    - Digit permutation (1-9 -> random permutation)\n",
        "    - Transpose (50% chance)\n",
        "    - Row band shuffling (shuffle the 3 bands of 3 rows each)\n",
        "    - Row shuffling within bands\n",
        "    - Column stack shuffling (shuffle the 3 stacks of 3 columns each)  \n",
        "    - Column shuffling within stacks\n",
        "    \n",
        "    Args:\n",
        "        board: Input puzzle [81] with 0=blank, 1-9=digits\n",
        "        solution: Solution [81] with 1-9\n",
        "        \n",
        "    Returns:\n",
        "        Augmented (board, solution) tuple\n",
        "    \"\"\"\n",
        "    # Create a random digit mapping: a permutation of 1..9, with zero (blank) unchanged\n",
        "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
        "    \n",
        "    # Randomly decide whether to transpose\n",
        "    transpose_flag = np.random.rand() < 0.5\n",
        "\n",
        "    # Generate a valid row permutation:\n",
        "    # - Shuffle the 3 bands (each band = 3 rows) and for each band, shuffle its 3 rows.\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "\n",
        "    # Similarly for columns (stacks).\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "\n",
        "    # Build an 81->81 mapping\n",
        "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
        "\n",
        "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
        "        # Reshape to 9x9 for transpose\n",
        "        x_2d = x.reshape(9, 9)\n",
        "        if transpose_flag:\n",
        "            x_2d = x_2d.T\n",
        "        x_flat = x_2d.flatten()\n",
        "        # Apply row/col permutation\n",
        "        new_board = x_flat[mapping]\n",
        "        # Apply digit mapping\n",
        "        return digit_map[new_board]\n",
        "\n",
        "    return apply_transformation(board), apply_transformation(solution)\n",
        "\n",
        "\n",
        "class SudokuDataset(Dataset):\n",
        "    \"\"\"Sudoku dataset with on-the-fly augmentation.\n",
        "    \n",
        "    Returns dict format compatible with src.training functions:\n",
        "    {'input': tensor, 'label': tensor, 'puzzle_id': tensor}\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, inputs, solutions, puzzle_ids=None, augment=True):\n",
        "        self.inputs = inputs\n",
        "        self.solutions = solutions\n",
        "        self.puzzle_ids = puzzle_ids if puzzle_ids is not None else torch.zeros(len(inputs), dtype=torch.long)\n",
        "        self.augment = augment\n",
        "        self._epoch = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        inp = self.inputs[idx].numpy() if isinstance(self.inputs[idx], torch.Tensor) else self.inputs[idx]\n",
        "        sol = self.solutions[idx].numpy() if isinstance(self.solutions[idx], torch.Tensor) else self.solutions[idx]\n",
        "        pid = self.puzzle_ids[idx]\n",
        "        \n",
        "        if self.augment:\n",
        "            inp, sol = shuffle_sudoku(inp, sol)\n",
        "        \n",
        "        # Return dict format for compatibility with src.training functions\n",
        "        return {\n",
        "            'input': torch.tensor(inp, dtype=torch.long),\n",
        "            'label': torch.tensor(sol, dtype=torch.long),\n",
        "            'puzzle_id': torch.tensor(pid, dtype=torch.long) if not isinstance(pid, torch.Tensor) else pid,\n",
        "        }\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Called at end of each epoch (for compatibility with HRM training).\"\"\"\n",
        "        self._epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_sudoku_data(data_dir: str):\n",
        "    \"\"\"\n",
        "    Load Sudoku-Extreme dataset.\n",
        "    \n",
        "    Expected format: .pt files with 'inputs' and 'solutions' tensors.\n",
        "    \"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    \n",
        "    train_data = {}\n",
        "    val_data = {}\n",
        "    \n",
        "    # Try to load preprocessed .pt files\n",
        "    train_pt = data_path / \"train.pt\"\n",
        "    val_pt = data_path / \"val.pt\"\n",
        "    \n",
        "    if train_pt.exists() and val_pt.exists():\n",
        "        print(f\"Loading preprocessed data from {data_dir}...\")\n",
        "        train_data = torch.load(train_pt, map_location=\"cpu\")\n",
        "        val_data = torch.load(val_pt, map_location=\"cpu\")\n",
        "        print(f\"  Train samples: {len(train_data['inputs'])}\")\n",
        "        print(f\"  Val samples: {len(val_data['inputs'])}\")\n",
        "    else:\n",
        "        # Create synthetic data for testing\n",
        "        print(\"⚠️ No preprocessed data found. Creating synthetic data for testing...\")\n",
        "        print(\"   Place train.pt and val.pt in the data directory for real training.\")\n",
        "        n_train, n_val = 1000, 200\n",
        "        train_data = {\n",
        "            'inputs': torch.randint(0, 10, (n_train, 81)),\n",
        "            'solutions': torch.randint(1, 10, (n_train, 81)),\n",
        "            'puzzle_ids': torch.zeros(n_train, dtype=torch.long),\n",
        "        }\n",
        "        val_data = {\n",
        "            'inputs': torch.randint(0, 10, (n_val, 81)),\n",
        "            'solutions': torch.randint(1, 10, (n_val, 81)),\n",
        "            'puzzle_ids': torch.zeros(n_val, dtype=torch.long),\n",
        "        }\n",
        "    \n",
        "    return train_data, val_data\n",
        "\n",
        "\n",
        "def create_dataloaders(train_data, val_data, batch_size: int, augment: bool = True, num_workers: int = 4):\n",
        "    \"\"\"Create DataLoaders from data dicts with on-the-fly augmentation.\"\"\"\n",
        "    \n",
        "    # Get or create puzzle IDs\n",
        "    train_ids = train_data.get('puzzle_ids', torch.zeros(len(train_data['inputs']), dtype=torch.long))\n",
        "    val_ids = val_data.get('puzzle_ids', torch.zeros(len(val_data['inputs']), dtype=torch.long))\n",
        "    \n",
        "    # Use SudokuDataset with augmentation for training\n",
        "    train_dataset = SudokuDataset(\n",
        "        train_data['inputs'],\n",
        "        train_data['solutions'],\n",
        "        train_ids,\n",
        "        augment=augment,\n",
        "    )\n",
        "    \n",
        "    # No augmentation for validation\n",
        "    val_dataset = SudokuDataset(\n",
        "        val_data['inputs'],\n",
        "        val_data['solutions'],\n",
        "        val_ids,\n",
        "        augment=False,\n",
        "    )\n",
        "    \n",
        "    # Use pin_memory for faster GPU transfer (only when using CUDA)\n",
        "    pin_memory = torch.cuda.is_available()\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data\n",
        "train_data, val_data = load_sudoku_data(config[\"data_dir\"])\n",
        "train_loader, val_loader = create_dataloaders(\n",
        "    train_data, val_data, \n",
        "    batch_size=config[\"batch_size\"],\n",
        "    augment=config[\"augment\"],\n",
        "    num_workers=config.get(\"num_workers\", 4),\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_model(config):\n",
        "    \"\"\"Create HybridPoHHRMSolver with specified controller type.\"\"\"\n",
        "    \n",
        "    controller_type = config[\"controller_type\"]\n",
        "    \n",
        "    # Build controller kwargs based on type\n",
        "    if controller_type in (\"transformer\", \"pot_transformer\"):\n",
        "        # Transformer-based controllers (standard or nested PoT)\n",
        "        controller_kwargs = {\n",
        "            \"d_ctrl\": config[\"d_ctrl\"],\n",
        "            \"n_ctrl_layers\": config[\"n_ctrl_layers\"],\n",
        "            \"n_ctrl_heads\": config[\"n_ctrl_heads\"],\n",
        "            \"max_depth\": config[\"max_depth\"],\n",
        "            \"token_conditioned\": True,\n",
        "        }\n",
        "    else:\n",
        "        # LSTM, GRU, xLSTM, minGRU\n",
        "        controller_kwargs = {\n",
        "            \"d_ctrl\": config[\"d_model\"],\n",
        "            \"token_conditioned\": True,\n",
        "        }\n",
        "    \n",
        "    model = HybridPoHHRMSolver(\n",
        "        d_model=config[\"d_model\"],\n",
        "        d_ff=config.get(\"d_ff\", 2048),\n",
        "        n_heads=config[\"n_heads\"],\n",
        "        H_layers=config[\"h_layers\"],\n",
        "        L_layers=config[\"l_layers\"],\n",
        "        H_cycles=config[\"h_cycles\"],\n",
        "        L_cycles=config[\"l_cycles\"],\n",
        "        dropout=config.get(\"dropout\", 0.1),\n",
        "        T=config.get(\"T\", 4),\n",
        "        halt_max_steps=config[\"halt_max_steps\"],\n",
        "        halt_exploration_prob=config.get(\"halt_exploration_prob\", 0.1),\n",
        "        hrm_grad_style=config.get(\"hrm_grad_style\", True),\n",
        "        controller_type=controller_type,\n",
        "        controller_kwargs=controller_kwargs,\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create model\n",
        "device = config[\"device\"]\n",
        "model = create_model(config).to(device)\n",
        "\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model created on {device}\")\n",
        "print(f\"  Controller: {config['controller_type'].upper()}\")\n",
        "print(f\"  Total parameters: {param_count:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# Import from src.training for HRM-compatible training with dual optimizers\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    from src.training import train_epoch, train_epoch_async, evaluate, log_halt_histogram_to_wandb\n",
        "    print(\"✓ Using src.training functions (HRM-compatible)\")\n",
        "    USE_SRC_TRAINING = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ src.training not found, using built-in functions\")\n",
        "    USE_SRC_TRAINING = False\n",
        "\n",
        "# Fallback halt histogram utility\n",
        "def log_halt_histogram_to_wandb(halt_histogram, prefix=\"\"):\n",
        "    \"\"\"Convert halt histogram to W&B-friendly format.\"\"\"\n",
        "    if not halt_histogram:\n",
        "        return {}\n",
        "    \n",
        "    # Build raw steps list for wandb.Histogram\n",
        "    steps_raw = []\n",
        "    for step, count in halt_histogram.items():\n",
        "        steps_raw.extend([step] * count)\n",
        "    \n",
        "    # Calculate stats\n",
        "    total = sum(halt_histogram.values())\n",
        "    if total == 0:\n",
        "        return {}\n",
        "    \n",
        "    avg_steps = sum(s * c for s, c in halt_histogram.items()) / total\n",
        "    max_step = max(halt_histogram.keys()) if halt_histogram else 0\n",
        "    \n",
        "    return {\n",
        "        f\"{prefix}halt_steps_raw\": steps_raw,\n",
        "        f\"{prefix}halt_avg_steps\": avg_steps,\n",
        "        f\"{prefix}halt_max_step\": max_step,\n",
        "    }\n",
        "\n",
        "# Fallback training functions if src.training not available\n",
        "if not USE_SRC_TRAINING:\n",
        "    def train_epoch(model, dataloader, optimizer, puzzle_optimizer, device, epoch,\n",
        "                    use_poh=True, debug=False, scheduler=None, puzzle_scheduler=None):\n",
        "        \"\"\"Train for one epoch with dual optimizer support.\"\"\"\n",
        "        model.train()\n",
        "        base_model = model.module if hasattr(model, 'module') else model\n",
        "        \n",
        "        total_loss = 0\n",
        "        correct_cells = 0\n",
        "        total_cells = 0\n",
        "        correct_grids = 0\n",
        "        total_grids = 0\n",
        "        total_steps = 0\n",
        "        \n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            inp = batch['input'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            puzzle_ids = batch['puzzle_id'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            if puzzle_optimizer:\n",
        "                puzzle_optimizer.zero_grad()\n",
        "            \n",
        "            # Forward\n",
        "            model_out = model(inp, puzzle_ids)\n",
        "            if len(model_out) == 5:\n",
        "                logits, q_halt, q_continue, steps, target_q_continue = model_out\n",
        "            else:\n",
        "                logits, q_halt, q_continue, steps = model_out\n",
        "                target_q_continue = None\n",
        "            \n",
        "            # CE loss\n",
        "            lm_loss = nn.functional.cross_entropy(\n",
        "                logits.view(-1, base_model.vocab_size),\n",
        "                label.view(-1)\n",
        "            )\n",
        "            \n",
        "            # Q-halt loss (if PoH)\n",
        "            if use_poh and q_halt is not None:\n",
        "                with torch.no_grad():\n",
        "                    preds = logits.argmax(dim=-1)\n",
        "                    is_correct = (preds == label).all(dim=1).float()\n",
        "                \n",
        "                q_halt_loss = nn.functional.binary_cross_entropy_with_logits(q_halt, is_correct)\n",
        "                loss = lm_loss + 0.5 * q_halt_loss\n",
        "                \n",
        "                if target_q_continue is not None:\n",
        "                    q_continue_loss = nn.functional.mse_loss(torch.sigmoid(q_continue), target_q_continue)\n",
        "                    loss = loss + 0.5 * q_continue_loss\n",
        "            else:\n",
        "                loss = lm_loss\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            if puzzle_optimizer:\n",
        "                puzzle_optimizer.step()\n",
        "            \n",
        "            # Step schedulers (per-step, not per-epoch!)\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            if puzzle_scheduler:\n",
        "                puzzle_scheduler.step()\n",
        "            \n",
        "            # Metrics\n",
        "            total_loss += loss.item()\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct_cells += (preds == label).sum().item()\n",
        "            total_cells += label.numel()\n",
        "            correct_grids += (preds == label).all(dim=1).sum().item()\n",
        "            total_grids += label.size(0)\n",
        "            total_steps += steps if isinstance(steps, int) else steps.float().mean().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'cell': f'{100*correct_cells/total_cells:.1f}%',\n",
        "                'grid': f'{100*correct_grids/total_grids:.1f}%',\n",
        "            })\n",
        "        \n",
        "        return {\n",
        "            'loss': total_loss / len(dataloader),\n",
        "            'cell_acc': 100 * correct_cells / total_cells,\n",
        "            'grid_acc': 100 * correct_grids / total_grids,\n",
        "            'avg_steps': total_steps / len(dataloader),\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(model, dataloader, device, use_poh=True, track_halt_histogram=False):\n",
        "        \"\"\"Evaluate model.\"\"\"\n",
        "        model.eval()\n",
        "        base_model = model.module if hasattr(model, 'module') else model\n",
        "        \n",
        "        total_loss = 0\n",
        "        correct_cells = 0\n",
        "        total_cells = 0\n",
        "        correct_grids = 0\n",
        "        total_grids = 0\n",
        "        \n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            inp = batch['input'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            puzzle_ids = batch['puzzle_id'].to(device)\n",
        "            \n",
        "            model_out = model(inp, puzzle_ids)\n",
        "            logits = model_out[0]\n",
        "            \n",
        "            loss = nn.functional.cross_entropy(\n",
        "                logits.view(-1, base_model.vocab_size),\n",
        "                label.view(-1)\n",
        "            )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct_cells += (preds == label).sum().item()\n",
        "            total_cells += label.numel()\n",
        "            correct_grids += (preds == label).all(dim=1).sum().item()\n",
        "            total_grids += label.size(0)\n",
        "        \n",
        "        return {\n",
        "            'loss': total_loss / len(dataloader),\n",
        "            'cell_acc': 100 * correct_cells / total_cells,\n",
        "            'grid_acc': 100 * correct_grids / total_grids,\n",
        "        }\n",
        "    \n",
        "    # Placeholder for async training (requires ACT model)\n",
        "    def train_epoch_async(*args, **kwargs):\n",
        "        raise NotImplementedError(\"Async training requires src.training module\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Setup Optimizer & Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# DUAL OPTIMIZER SETUP (HRM-style)\n",
        "# =============================================================================\n",
        "import math\n",
        "\n",
        "# Separate puzzle embedding parameters from model parameters\n",
        "puzzle_params = []\n",
        "model_params = []\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'puzzle' in name.lower() or 'embedding' in name.lower():\n",
        "        puzzle_params.append(param)\n",
        "    else:\n",
        "        model_params.append(param)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model_params):,}\")\n",
        "print(f\"Puzzle parameters: {sum(p.numel() for p in puzzle_params):,}\")\n",
        "\n",
        "# Main optimizer (AdamW)\n",
        "beta2 = config.get(\"beta2\", 0.95)  # Llama-style / HRM default\n",
        "betas = (0.9, beta2)\n",
        "optimizer = optim.AdamW(\n",
        "    model_params,\n",
        "    lr=config[\"lr\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        "    betas=betas,\n",
        ")\n",
        "\n",
        "# Puzzle optimizer (optional - HRM uses separate optimizer for puzzle embeddings)\n",
        "puzzle_optimizer = None\n",
        "if config[\"use_puzzle_optimizer\"] and len(puzzle_params) > 0:\n",
        "    puzzle_lr = config[\"lr\"] * config[\"puzzle_lr_multiplier\"]\n",
        "    \n",
        "    if config[\"puzzle_optimizer\"] == \"signsgd\":\n",
        "        # SignSGD for puzzle embeddings (as in HRM paper)\n",
        "        class SignSGD(optim.Optimizer):\n",
        "            def __init__(self, params, lr=1e-3, weight_decay=0):\n",
        "                defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "                super().__init__(params, defaults)\n",
        "            \n",
        "            @torch.no_grad()\n",
        "            def step(self, closure=None):\n",
        "                for group in self.param_groups:\n",
        "                    for p in group['params']:\n",
        "                        if p.grad is None:\n",
        "                            continue\n",
        "                        d_p = p.grad.sign()\n",
        "                        if group['weight_decay'] != 0:\n",
        "                            d_p = d_p.add(p, alpha=group['weight_decay'])\n",
        "                        p.add_(d_p, alpha=-group['lr'])\n",
        "        \n",
        "        puzzle_optimizer = SignSGD(puzzle_params, lr=puzzle_lr, weight_decay=config[\"puzzle_weight_decay\"])\n",
        "        print(f\"Puzzle optimizer: SignSGD (lr={puzzle_lr:.2e})\")\n",
        "    else:\n",
        "        puzzle_optimizer = optim.AdamW(\n",
        "            puzzle_params,\n",
        "            lr=puzzle_lr,\n",
        "            weight_decay=config[\"puzzle_weight_decay\"],\n",
        "            betas=betas,\n",
        "        )\n",
        "        print(f\"Puzzle optimizer: AdamW (lr={puzzle_lr:.2e})\")\n",
        "else:\n",
        "    print(\"Puzzle optimizer: disabled\")\n",
        "\n",
        "# Cosine LR schedule with warmup (per-step, not per-epoch)\n",
        "total_steps = config[\"epochs\"] * len(train_loader)\n",
        "warmup_steps = config[\"warmup_steps\"]\n",
        "lr_min_ratio = config.get(\"lr_min_ratio\", 0.1)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / warmup_steps\n",
        "    progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "    return lr_min_ratio + (1 - lr_min_ratio) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "puzzle_scheduler = optim.lr_scheduler.LambdaLR(puzzle_optimizer, lr_lambda) if puzzle_optimizer else None\n",
        "\n",
        "print(f\"\\nOptimizer: AdamW (lr={config['lr']}, weight_decay={config['weight_decay']})\")\n",
        "print(f\"Scheduler: Cosine with warmup ({warmup_steps} steps), min_ratio={lr_min_ratio}\")\n",
        "print(f\"Total training steps: {total_steps:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. W&B Setup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize W&B if enabled\n",
        "if config[\"use_wandb\"]:\n",
        "    import wandb\n",
        "    \n",
        "    run_name = f\"{config['controller_type']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    wandb.init(\n",
        "        project=config[\"wandb_project\"],\n",
        "        name=run_name,\n",
        "        config=config,\n",
        "    )\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=100)\n",
        "    print(f\"W&B initialized: {run_name}\")\n",
        "else:\n",
        "    print(\"W&B logging disabled. Set config['use_wandb'] = True to enable.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create checkpoint directory\n",
        "save_dir = Path(config[\"save_dir\"]) / config[\"controller_type\"]\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Checkpoints will be saved to: {save_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_cell_acc': [],\n",
        "    'train_grid_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_cell_acc': [],\n",
        "    'val_grid_acc': [],\n",
        "    'lr': [],\n",
        "}\n",
        "\n",
        "# Resume from checkpoint if specified\n",
        "start_epoch = 1\n",
        "best_grid_acc = 0\n",
        "resume_checkpoint = None\n",
        "\n",
        "if config.get(\"resume_from\") and os.path.exists(config[\"resume_from\"]):\n",
        "    print(f\"Resuming from checkpoint: {config['resume_from']}\")\n",
        "    resume_checkpoint = torch.load(config[\"resume_from\"], map_location=device)\n",
        "    \n",
        "    # Load model weights\n",
        "    model.load_state_dict(resume_checkpoint['model_state_dict'])\n",
        "    print(f\"  ✓ Loaded model weights\")\n",
        "    \n",
        "    # Load optimizer states\n",
        "    if 'optimizer_state_dict' in resume_checkpoint:\n",
        "        optimizer.load_state_dict(resume_checkpoint['optimizer_state_dict'])\n",
        "        print(f\"  ✓ Loaded optimizer state\")\n",
        "    \n",
        "    if puzzle_optimizer and 'puzzle_optimizer_state_dict' in resume_checkpoint:\n",
        "        puzzle_optimizer.load_state_dict(resume_checkpoint['puzzle_optimizer_state_dict'])\n",
        "        print(f\"  ✓ Loaded puzzle optimizer state\")\n",
        "    \n",
        "    # Load scheduler states\n",
        "    if 'scheduler_state_dict' in resume_checkpoint:\n",
        "        scheduler.load_state_dict(resume_checkpoint['scheduler_state_dict'])\n",
        "        print(f\"  ✓ Loaded scheduler state\")\n",
        "    \n",
        "    if puzzle_scheduler and 'puzzle_scheduler_state_dict' in resume_checkpoint:\n",
        "        puzzle_scheduler.load_state_dict(resume_checkpoint['puzzle_scheduler_state_dict'])\n",
        "        print(f\"  ✓ Loaded puzzle scheduler state\")\n",
        "    \n",
        "    # Restore training state\n",
        "    if 'epoch' in resume_checkpoint:\n",
        "        start_epoch = resume_checkpoint['epoch'] + 1\n",
        "        print(f\"  ✓ Resuming from epoch {start_epoch}\")\n",
        "    \n",
        "    if 'best_grid_acc' in resume_checkpoint:\n",
        "        best_grid_acc = resume_checkpoint['best_grid_acc']\n",
        "        print(f\"  ✓ Best grid accuracy so far: {best_grid_acc:.2f}%\")\n",
        "    elif 'grid_acc' in resume_checkpoint:\n",
        "        best_grid_acc = resume_checkpoint['grid_acc']\n",
        "        print(f\"  ✓ Grid accuracy from checkpoint: {best_grid_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting {config['controller_type'].upper()} Controller Training\")\n",
        "if start_epoch > 1:\n",
        "    print(f\"  (Resuming from epoch {start_epoch})\")\n",
        "print(f\"{'='*60}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# MAIN TRAINING LOOP (HRM-compatible with dual optimizers)\n",
        "# =============================================================================\n",
        "\n",
        "use_async = config.get(\"async_batch\", True)\n",
        "eval_interval = config.get(\"eval_interval\", 10)\n",
        "halt_histogram_interval = config.get(\"halt_histogram_interval\", 50)\n",
        "val_metrics = None  # Will be set after first eval\n",
        "\n",
        "for epoch in range(start_epoch, config[\"epochs\"] + 1):\n",
        "    # Determine if we should track halt histograms this epoch\n",
        "    track_halt = (epoch % halt_histogram_interval == 0) or (epoch == 1)\n",
        "    \n",
        "    # Train (with dual optimizers and per-step scheduler)\n",
        "    if use_async:\n",
        "        train_metrics = train_epoch_async(\n",
        "            model, train_loader, optimizer, puzzle_optimizer,\n",
        "            device, epoch, use_poh=True,\n",
        "            scheduler=scheduler, puzzle_scheduler=puzzle_scheduler,\n",
        "            track_halt_histogram=track_halt,\n",
        "        )\n",
        "    else:\n",
        "        train_metrics = train_epoch(\n",
        "            model, train_loader, optimizer, puzzle_optimizer,\n",
        "            device, epoch, use_poh=True,\n",
        "            scheduler=scheduler, puzzle_scheduler=puzzle_scheduler,\n",
        "        )\n",
        "    \n",
        "    # Call on_epoch_end (for augmentation reset, etc.)\n",
        "    if hasattr(train_loader.dataset, 'on_epoch_end'):\n",
        "        train_loader.dataset.on_epoch_end()\n",
        "    \n",
        "    # Evaluate (every eval_interval epochs or first/last epoch)\n",
        "    do_eval = (epoch % eval_interval == 0) or (epoch == 1) or (epoch == config[\"epochs\"])\n",
        "    if do_eval:\n",
        "        val_metrics = evaluate(model, val_loader, device, use_poh=True, track_halt_histogram=track_halt)\n",
        "    \n",
        "    # Get current LR\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    # Store history\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['train_cell_acc'].append(train_metrics['cell_acc'] / 100)  # Normalize to 0-1\n",
        "    history['train_grid_acc'].append(train_metrics['grid_acc'] / 100)\n",
        "    if val_metrics:\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_cell_acc'].append(val_metrics['cell_acc'] / 100)\n",
        "        history['val_grid_acc'].append(val_metrics['grid_acc'] / 100)\n",
        "    history['lr'].append(current_lr)\n",
        "    \n",
        "    # Print progress\n",
        "    train_str = f\"loss={train_metrics['loss']:.4f}, cell={train_metrics['cell_acc']:.1f}%, grid={train_metrics['grid_acc']:.1f}%\"\n",
        "    if val_metrics:\n",
        "        val_str = f\"loss={val_metrics['loss']:.4f}, cell={val_metrics['cell_acc']:.1f}%, grid={val_metrics['grid_acc']:.1f}%\"\n",
        "        print(f\"Epoch {epoch}/{config['epochs']} | Train: {train_str} | Val: {val_str}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch}/{config['epochs']} | Train: {train_str}\")\n",
        "    \n",
        "    # W&B logging\n",
        "    if config[\"use_wandb\"]:\n",
        "        log_dict = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train/loss\": train_metrics['loss'],\n",
        "            \"train/cell_acc\": train_metrics['cell_acc'],\n",
        "            \"train/grid_acc\": train_metrics['grid_acc'],\n",
        "            \"lr\": current_lr,\n",
        "            \"best_grid_acc\": best_grid_acc,\n",
        "        }\n",
        "        if val_metrics:\n",
        "            log_dict.update({\n",
        "                \"val/loss\": val_metrics['loss'],\n",
        "                \"val/cell_acc\": val_metrics['cell_acc'],\n",
        "                \"val/grid_acc\": val_metrics['grid_acc'],\n",
        "            })\n",
        "        \n",
        "        # Log halt histograms (if tracked this epoch)\n",
        "        if 'halt_histogram' in train_metrics:\n",
        "            train_halt_log = log_halt_histogram_to_wandb(train_metrics['halt_histogram'], prefix=\"train_\")\n",
        "            log_dict.update(train_halt_log)\n",
        "            if train_halt_log.get(\"train_halt_steps_raw\"):\n",
        "                log_dict[\"train_halt_histogram\"] = wandb.Histogram(train_halt_log[\"train_halt_steps_raw\"])\n",
        "        \n",
        "        if val_metrics and 'halt_histogram' in val_metrics:\n",
        "            val_halt_log = log_halt_histogram_to_wandb(val_metrics['halt_histogram'], prefix=\"val_\")\n",
        "            log_dict.update(val_halt_log)\n",
        "            if val_halt_log.get(\"val_halt_steps_raw\"):\n",
        "                log_dict[\"val_halt_histogram\"] = wandb.Histogram(val_halt_log[\"val_halt_steps_raw\"])\n",
        "        \n",
        "        wandb.log(log_dict)\n",
        "    \n",
        "    # Save best model (only when we have val metrics)\n",
        "    if val_metrics and val_metrics['grid_acc'] > best_grid_acc:\n",
        "        best_grid_acc = val_metrics['grid_acc']\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_grid_acc': best_grid_acc,\n",
        "            'config': config,\n",
        "        }\n",
        "        if puzzle_optimizer:\n",
        "            checkpoint['puzzle_optimizer_state_dict'] = puzzle_optimizer.state_dict()\n",
        "        if puzzle_scheduler:\n",
        "            checkpoint['puzzle_scheduler_state_dict'] = puzzle_scheduler.state_dict()\n",
        "        best_model_path = save_dir / \"best_model.pt\"\n",
        "        torch.save(checkpoint, best_model_path)\n",
        "        print(f\"  ✓ New best! Grid acc: {best_grid_acc:.2f}%\")\n",
        "        \n",
        "        # Upload to W&B as artifact\n",
        "        if config[\"use_wandb\"]:\n",
        "            artifact = wandb.Artifact(\n",
        "                f\"sudoku-{config['controller_type']}-best\",\n",
        "                type=\"model\",\n",
        "                metadata={\"grid_acc\": best_grid_acc, \"epoch\": epoch}\n",
        "            )\n",
        "            artifact.add_file(str(best_model_path))\n",
        "            wandb.log_artifact(artifact, aliases=[\"best\", \"latest\"])\n",
        "            print(f\"  📤 Uploaded to W&B artifacts\")\n",
        "    \n",
        "    # Periodic checkpoint\n",
        "    if epoch % config[\"save_every\"] == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'grid_acc': val_metrics['grid_acc'] if val_metrics else train_metrics['grid_acc'],\n",
        "            'best_grid_acc': best_grid_acc,\n",
        "            'config': config,\n",
        "        }\n",
        "        if puzzle_optimizer:\n",
        "            checkpoint['puzzle_optimizer_state_dict'] = puzzle_optimizer.state_dict()\n",
        "        if puzzle_scheduler:\n",
        "            checkpoint['puzzle_scheduler_state_dict'] = puzzle_scheduler.state_dict()\n",
        "        torch.save(checkpoint, save_dir / f\"checkpoint_epoch{epoch}.pt\")\n",
        "        print(f\"  💾 Checkpoint saved: epoch {epoch}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete! Best grid accuracy: {best_grid_acc:.2f}%\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if config[\"use_wandb\"]:\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Cell Accuracy\n",
        "axes[1].plot([100*x for x in history['train_cell_acc']], label='Train')\n",
        "axes[1].plot([100*x for x in history['val_cell_acc']], label='Val')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Cell Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Grid Accuracy\n",
        "axes[2].plot([100*x for x in history['train_grid_acc']], label='Train')\n",
        "axes[2].plot([100*x for x in history['val_grid_acc']], label='Val')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Accuracy (%)')\n",
        "axes[2].set_title('Grid Accuracy')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f\"{config['controller_type'].upper()} Controller Training\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"training_curves.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPlot saved to: {save_dir / 'training_curves.png'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Load Best Model & Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(save_dir / \"best_model.pt\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Best grid accuracy: {100*checkpoint['best_grid_acc']:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final evaluation\n",
        "final_metrics = evaluate(model, val_loader, device)\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"Final Evaluation Results\")\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"Controller: {config['controller_type'].upper()}\")\n",
        "print(f\"Val Loss: {final_metrics['loss']:.4f}\")\n",
        "print(f\"Cell Accuracy: {100*final_metrics['cell_acc']:.2f}%\")\n",
        "print(f\"Grid Accuracy: {100*final_metrics['grid_acc']:.2f}%\")\n",
        "print(f\"{'='*40}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Test Set Evaluation (422k puzzles)\n",
        "\n",
        "Evaluate the best model on the full Sudoku-Extreme test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# TEST SET EVALUATION (422k puzzles)\n",
        "# ============================================================================\n",
        "\n",
        "# Load test set\n",
        "test_pt_path = Path(config[\"data_dir\"]) / \"test.pt\"\n",
        "\n",
        "if test_pt_path.exists():\n",
        "    print(\"Loading test set...\")\n",
        "    test_data = torch.load(test_pt_path, map_location=\"cpu\", weights_only=True)\n",
        "    \n",
        "    # Create test dataset (no augmentation)\n",
        "    test_ids = torch.zeros(len(test_data['inputs']), dtype=torch.long)\n",
        "    test_dataset = SudokuDataset(\n",
        "        test_data['inputs'],\n",
        "        test_data['solutions'],\n",
        "        test_ids,\n",
        "        augment=False,\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=config.get(\"num_workers\", 4),\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    \n",
        "    print(f\"✓ Test set loaded: {len(test_dataset):,} puzzles\")\n",
        "    \n",
        "    # Load best model\n",
        "    best_model_path = save_dir / \"best_model.pt\"\n",
        "    if best_model_path.exists():\n",
        "        checkpoint = torch.load(best_model_path, map_location=device, weights_only=True)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"✓ Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_metrics = evaluate(model, test_loader, device, use_poh=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🎯 TEST RESULTS ({len(test_dataset):,} puzzles)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Controller: {config['controller_type'].upper()}\")\n",
        "    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(f\"Cell Accuracy: {test_metrics['cell_acc']:.2f}%\")\n",
        "    print(f\"Grid Accuracy: {test_metrics['grid_acc']:.2f}%\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Log to W&B if enabled\n",
        "    if config[\"use_wandb\"]:\n",
        "        wandb.log({\n",
        "            \"test/loss\": test_metrics['loss'],\n",
        "            \"test/cell_acc\": test_metrics['cell_acc'],\n",
        "            \"test/grid_acc\": test_metrics['grid_acc'],\n",
        "        })\n",
        "        print(\"✓ Test results logged to W&B\")\n",
        "else:\n",
        "    print(f\"⚠️ Test set not found at {test_pt_path}\")\n",
        "    print(\"  Run download_sudoku_dataset(..., download_test=True) to download the 422k test set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Test Set Evaluation (422k puzzles)\n",
        "\n",
        "Evaluate the best model on the full Sudoku-Extreme test set."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}