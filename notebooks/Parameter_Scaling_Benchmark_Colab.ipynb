{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameter Scaling Benchmark - PoH-HRM vs Baseline Transformer (Enhanced)\n",
        "\n",
        "This notebook runs a parameter scaling experiment comparing:\n",
        "- **Baseline Transformer** (standard multi-head attention)\n",
        "- **PoH-HRM** (Pointer-over-Heads with Hierarchical Reasoning Module)\n",
        "\n",
        "We now focus on the two largest scales for clear differentiation:\n",
        "- **Large** (~30M params)\n",
        "- **XL** (~100M params)\n",
        "\n",
        "Additionally, an optional section allows testing a **HUGE (~500M)** model.\n",
        "\n",
        "## ðŸš€ Enhanced Training Features (MLM-U Inspired)\n",
        "\n",
        "This notebook now includes advanced training techniques:\n",
        "- âœ… **Label Smoothing** (0.1) - Prevents overconfidence\n",
        "- âœ… **Cosine LR Warmup** (2000 steps) - Smooth learning rate schedule\n",
        "- âœ… **Multi-Horizon Supervision** (3-step) - Predict multiple steps ahead\n",
        "- âœ… **Validity-Aware Loss** - Only predict valid moves\n",
        "- âœ… **Routing Entropy Regularization** (5e-4, annealed) - Sharper PoH routing\n",
        "- âœ… **CNN Maze Encoder** - Global maze conditioning\n",
        "- âœ… **Depth-First Parameter Parity** - Keep PoH depth, reduce width\n",
        "\n",
        "## Key Features\n",
        "\n",
        "**Parameter Parity:** PoH-HRM keeps full depth, adjusts width (d_model) to match baseline params (â‰¤10% tolerance).\n",
        "\n",
        "**Key Questions:**\n",
        "1. Does PoH-HRM maintain its advantage at large scales with enhanced training?\n",
        "2. How does advantage change from Large to XL (and optionally to HUGE)?\n",
        "3. Does performance saturate or continue improving?\n",
        "\n",
        "**Runtime:** ~1â€“2 hours on A100 GPU (Large + XL). HUGE is heavier; start with fewer epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/Eran-BA/PoT.git\n",
        "%cd PoT\n",
        "\n",
        "# Switch to scaling branch\n",
        "!git checkout scaling_parameter_size\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch transformers numpy matplotlib tqdm\n",
        "!pip install -q maze-dataset\n",
        "\n",
        "# Verify GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    try:\n",
        "        import torch.backends.mps\n",
        "        print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "    except Exception as e:\n",
        "        print(\"MPS check failed\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Adjust these parameters as needed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark configuration\n",
        "MAZE_SIZE = 16        # Maze grid size (16x16)\n",
        "N_TRAIN = 1000        # Training samples per model\n",
        "N_TEST = 100          # Test samples\n",
        "EPOCHS = 50           # Training epochs per model\n",
        "R = 4                 # PoH refinement iterations\n",
        "T = 4                 # HRM outer loop period\n",
        "SEED = 42             # Random seed\n",
        "\n",
        "# Enhanced training options (NEW!)\n",
        "LR = 1e-3             # Learning rate\n",
        "LABEL_SMOOTH = 0.1    # Label smoothing factor\n",
        "WARMUP_STEPS = 2000   # LR warmup steps\n",
        "MULTI_HORIZON = 3     # k-step supervision horizon\n",
        "VALIDITY_MASK = True  # Enable validity masking\n",
        "ROUTE_ENT = 5e-4      # PoH routing entropy weight\n",
        "ENT_ANNEAL = True     # Anneal entropy weight\n",
        "\n",
        "# For faster testing (recommended for first run):\n",
        "# MAZE_SIZE = 12\n",
        "# N_TRAIN = 500\n",
        "# N_TEST = 50\n",
        "# EPOCHS = 30\n",
        "# WARMUP_STEPS = 1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Benchmark with Enhanced Training\n",
        "\n",
        "This will test the two largest model sizes (Large and XL) with:\n",
        "- **Parameter parity** via depth-first approach (keep PoH depth, reduce width)\n",
        "- **All enhanced features** (label smoothing, cosine warmup, multi-horizon, validity masking, routing entropy, CNN encoder)\n",
        "\n",
        "**Progress:**\n",
        "1. Generate training/test data (once)\n",
        "2. For each size (Large â†’ XL):\n",
        "   - Train Baseline Transformer with enhancements\n",
        "   - Evaluate Baseline\n",
        "   - Train PoH-HRM with enhancements + routing entropy (width auto-adjusts for param parity)\n",
        "   - Evaluate PoH-HRM\n",
        "3. Save results\n",
        "\n",
        "**Training Enhancements Active:**\n",
        "- âœ… Label smoothing (0.1), Cosine LR warmup (2000 steps)\n",
        "- âœ… Multi-horizon supervision (3-step ahead)\n",
        "- âœ… Validity-aware loss (mask invalid moves)\n",
        "- âœ… CNN maze encoder (global conditioning)\n",
        "- âœ… PoH routing entropy regularization (5e-4, annealed)\n",
        "\n",
        "**Note:** Large and XL may take 30â€“60 minutes each. Use fewer epochs for a quick run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Large and XL with all enhanced training features\n",
        "validity_flag = \"--validity-mask\" if VALIDITY_MASK else \"\"\n",
        "anneal_flag = \"--ent-anneal\" if ENT_ANNEAL else \"\"\n",
        "\n",
        "!python experiments/parameter_scaling_benchmark.py \\\n",
        "    --maze-size {MAZE_SIZE} \\\n",
        "    --train {N_TRAIN} \\\n",
        "    --test {N_TEST} \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --R {R} \\\n",
        "    --T {T} \\\n",
        "    --seed {SEED} \\\n",
        "    --lr {LR} \\\n",
        "    --label-smoothing {LABEL_SMOOTH} \\\n",
        "    --warmup-steps {WARMUP_STEPS} \\\n",
        "    --multi-horizon {MULTI_HORIZON} \\\n",
        "    {validity_flag} \\\n",
        "    --route-ent-weight {ROUTE_ENT} \\\n",
        "    {anneal_flag} \\\n",
        "    --output experiments/results/parameter_scaling_colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load results\n",
        "with open(f'experiments/results/parameter_scaling_colab/scaling_results_maze{MAZE_SIZE}.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "results = data['results']\n",
        "config = data['config']\n",
        "\n",
        "# Extract data\n",
        "sizes = [r['size'] for r in results]\n",
        "baseline_params = [r['baseline_params'] / 1e6 for r in results]\n",
        "poh_params = [r['poh_params'] / 1e6 for r in results]\n",
        "\n",
        "baseline_acc = [r['baseline_acc'] for r in results]\n",
        "poh_acc = [r['poh_acc'] for r in results]\n",
        "\n",
        "baseline_opt = [r['baseline_opt'] for r in results]\n",
        "poh_opt = [r['poh_opt'] for r in results]\n",
        "\n",
        "poh_adv_acc = [r['poh_advantage_acc'] for r in results]\n",
        "poh_adv_opt = [r['poh_advantage_opt'] for r in results]\n",
        "\n",
        "# Create plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Accuracy vs Parameters\n",
        "ax = axes[0, 0]\n",
        "ax.plot(baseline_params, baseline_acc, 'o-', label='Baseline', linewidth=2, markersize=8)\n",
        "ax.plot(poh_params, poh_acc, 's-', label='PoH-HRM', linewidth=2, markersize=8)\n",
        "ax.set_xlabel('Parameters (M)', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title(f'Accuracy vs. Model Size\\\\n(Maze {config[\"maze_size\"]}Ã—{config[\"maze_size\"]})', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xscale('log')\n",
        "\n",
        "# Plot 2: Optimality vs Parameters\n",
        "ax = axes[0, 1]\n",
        "ax.plot(baseline_params, baseline_opt, 'o-', label='Baseline', linewidth=2, markersize=8)\n",
        "ax.plot(poh_params, poh_opt, 's-', label='PoH-HRM', linewidth=2, markersize=8)\n",
        "ax.set_xlabel('Parameters (M)', fontsize=12)\n",
        "ax.set_ylabel('Optimality (%)', fontsize=12)\n",
        "ax.set_title(f'Optimality vs. Model Size\\\\n(Maze {config[\"maze_size\"]}Ã—{config[\"maze_size\"]})', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xscale('log')\n",
        "\n",
        "# Plot 3: PoH Advantage in Accuracy\n",
        "ax = axes[1, 0]\n",
        "colors = ['green' if x > 0 else 'red' for x in poh_adv_acc]\n",
        "ax.bar(sizes, poh_adv_acc, color=colors, alpha=0.7)\n",
        "ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('Model Size', fontsize=12)\n",
        "ax.set_ylabel('PoH Advantage (%)', fontsize=12)\n",
        "ax.set_title('PoH-HRM Accuracy Advantage\\\\n(PoH - Baseline)', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 4: PoH Advantage in Optimality\n",
        "ax = axes[1, 1]\n",
        "colors = ['green' if x > 0 else 'red' for x in poh_adv_opt]\n",
        "ax.bar(sizes, poh_adv_opt, color=colors, alpha=0.7)\n",
        "ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('Model Size', fontsize=12)\n",
        "ax.set_ylabel('PoH Advantage (%)', fontsize=12)\n",
        "ax.set_title('PoH-HRM Optimality Advantage\\\\n(PoH - Baseline)', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'scaling_plot_maze{config[\"maze_size\"]}.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\nâœ“ Plot saved to: scaling_plot_maze{config['maze_size']}.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*100)\n",
        "print(f\"PARAMETER SCALING RESULTS - Maze {config['maze_size']}Ã—{config['maze_size']}\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Training: {config['n_train']} samples, {config['epochs']} epochs\")\n",
        "print(f\"Testing: {config['n_test']} samples\")\n",
        "print(f\"PoH Config: R={config['R']}, T={config['T']}\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "\n",
        "print(f\"{'Size':<10} {'Params (M)':<20} {'Accuracy (%)':<25} {'Optimality (%)':<25}\")\n",
        "print(f\"{'':<10} {'Baseline / PoH':<20} {'Baseline / PoH / Î”':<25} {'Baseline / PoH / Î”':<25}\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r['size']:<10} \"\n",
        "          f\"{r['baseline_params']/1e6:>5.1f} / {r['poh_params']/1e6:>5.1f}   \"\n",
        "          f\"{r['baseline_acc']:>5.1f} / {r['poh_acc']:>5.1f} / {r['poh_advantage_acc']:>+5.1f}   \"\n",
        "          f\"{r['baseline_opt']:>5.1f} / {r['poh_opt']:>5.1f} / {r['poh_advantage_opt']:>+5.1f}\")\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"\\\\nKey Findings:\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "# Calculate average advantage\n",
        "avg_adv_acc = np.mean([r['poh_advantage_acc'] for r in results])\n",
        "avg_adv_opt = np.mean([r['poh_advantage_opt'] for r in results])\n",
        "\n",
        "print(f\"Average PoH Advantage (Accuracy): {avg_adv_acc:+.2f}%\")\n",
        "print(f\"Average PoH Advantage (Optimality): {avg_adv_opt:+.2f}%\")\n",
        "print()\n",
        "\n",
        "# Find best size for PoH\n",
        "best_acc_idx = np.argmax([r['poh_acc'] for r in results])\n",
        "best_opt_idx = np.argmax([r['poh_opt'] for r in results])\n",
        "\n",
        "print(f\"Best PoH Accuracy: {results[best_acc_idx]['size']} \"\n",
        "      f\"({results[best_acc_idx]['poh_acc']:.1f}% @ {results[best_acc_idx]['poh_params']/1e6:.1f}M params)\")\n",
        "print(f\"Best PoH Optimality: {results[best_opt_idx]['size']} \"\n",
        "      f\"({results[best_opt_idx]['poh_opt']:.1f}% @ {results[best_opt_idx]['poh_params']/1e6:.1f}M params)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: HUGE (~500M) Benchmark\n",
        "\n",
        "Run a much larger model using `experiments/huge_500m_benchmark.py`.\n",
        "Parameter parity is enforced by reducing PoH depth to â‰¤10% overhead vs baseline.\n",
        "\n",
        "Notes:\n",
        "- Start with fewer epochs (e.g., 5â€“10)\n",
        "- Lower batch size if you hit OOM\n",
        "- Runtime is significantly higher than Large/XL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example HUGE run (adjust epochs/batch-size to your GPU)\n",
        "!python experiments/huge_500m_benchmark.py \\\n",
        "  --maze-size {MAZE_SIZE} \\\n",
        "  --train 2000 \\\n",
        "  --test 200 \\\n",
        "  --epochs 10 \\\n",
        "  --batch-size 8 \\\n",
        "  --R 4 --T 4 \\\n",
        "  --output experiments/results/huge_500m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
