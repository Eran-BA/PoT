{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PoH + TRM Latent Recursion vs Official TRM on Maze 30×30 (Colab)\n",
        "\n",
        "This notebook prepares the Maze-Hard dataset, and runs either:\n",
        "- Official TRM recipe from TinyRecursiveModels (single-GPU A100)\n",
        "- Your PoH + TRM-style latent recursion runner in this repo\n",
        "\n",
        "Reference: TinyRecursiveModels [repo](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#@title Install base dependencies\n",
        "%pip -q install --upgrade pip wheel setuptools\n",
        "%pip -q install tqdm huggingface_hub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#@title Option A: Official TRM (TinyRecursiveModels)\n",
        "%%bash\n",
        "set -e\n",
        "# Clone and install\n",
        "rm -rf /content/TinyRecursiveModels\n",
        "git clone https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git /content/TinyRecursiveModels\n",
        "cd /content/TinyRecursiveModels\n",
        "pip -q install --pre --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126\n",
        "pip -q install -r requirements.txt\n",
        "pip -q install --no-cache-dir --no-build-isolation adam-atan2\n",
        "\n",
        "# Build Maze-Hard dataset (writes to data/maze-30x30-hard-1k)\n",
        "python dataset/build_maze_dataset.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#@title Run TRM Maze-Hard (single A100)\n",
        "%%bash\n",
        "set -e\n",
        "cd /content/TinyRecursiveModels\n",
        "run_name=\"pretrain_att_maze30x30_a100_1gpu\"\n",
        "python pretrain.py \\\n",
        "  arch=trm \\\n",
        "  data_paths=\"[data/maze-30x30-hard-1k]\" \\\n",
        "  evaluators=\"[]\" \\\n",
        "  epochs=50000 eval_interval=5000 \\\n",
        "  lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 \\\n",
        "  arch.L_layers=2 \\\n",
        "  arch.H_cycles=3 arch.L_cycles=4 \\\n",
        "  +run_name=${run_name} ema=True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#@title Option B: PoH + TRM-latent (this repo)\n",
        "%%bash\n",
        "set -e\n",
        "rm -rf /content/PoT\n",
        "git clone https://github.com/Eran-BA/PoT.git /content/PoT\n",
        "cd /content/PoT\n",
        "pip -q install -r requirements.txt\n",
        "pip -q install tqdm huggingface_hub\n",
        "\n",
        "python - << 'PY'\n",
        "import os, csv, json, numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm import tqdm\n",
        "output_dir = '/content/PoT/vendor/hrm/data/maze-30x30-hard-1k'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "CHARSET = \"# SGo\"\n",
        "repo = \"sapientinc/maze-30x30-hard-1k\"\n",
        "for split in ['train','test']:\n",
        "    split_dir = os.path.join(output_dir, split)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    csv_file = hf_hub_download(repo, f\"{split}.csv\", repo_type=\"dataset\")\n",
        "    inputs, labels = [], []\n",
        "    with open(csv_file, 'r') as f:\n",
        "        reader = csv.reader(f); next(reader)\n",
        "        for source, q, a, rating in tqdm(reader):\n",
        "            n = int(len(q)**0.5)\n",
        "            inputs.append(np.frombuffer(q.encode(), dtype=np.uint8).reshape(n,n))\n",
        "            labels.append(np.frombuffer(a.encode(), dtype=np.uint8).reshape(n,n))\n",
        "    char2id = np.zeros(256, np.uint8)\n",
        "    for i,c in enumerate(\"\\0\" + CHARSET):\n",
        "        char2id[ord(c)] = i\n",
        "    X = np.vstack([char2id[x.reshape(-1)] for x in inputs])\n",
        "    Y = np.vstack([char2id[y.reshape(-1)] for y in labels])\n",
        "    np.save(os.path.join(split_dir,'all__inputs.npy'), X)\n",
        "    np.save(os.path.join(split_dir,'all__labels.npy'), Y)\n",
        "    with open(os.path.join(split_dir,'dataset.json'),'w') as f:\n",
        "        json.dump({'seq_len':900,'vocab_size':6}, f)\n",
        "print('✓ Dataset ready at', output_dir)\n",
        "PY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#@title Run PoH + TRM-latent (~35M params)\n",
        "%%bash\n",
        "set -e\n",
        "cd /content/PoT\n",
        "python -u experiments/maze_grid2grid_hrm.py \\\n",
        "  --data-dir vendor/hrm/data/maze-30x30-hard-1k \\\n",
        "  --model poh \\\n",
        "  --d-model 512 --n-heads 8 --n-layers 6 --d-ff 2048 \\\n",
        "  --T 4 --batch-size 12 \\\n",
        "  --max-epochs 5000 --patience 50 \\\n",
        "  --lr 1e-4 --puzzle-emb-lr 5e-4 \\\n",
        "  --weight-decay 1.0 \\\n",
        "  --num-puzzles 1000 --puzzle-emb-dim 256 \\\n",
        "  --max-halting-steps 16 \\\n",
        "  --latent-len 16 --latent-k 3 \\\n",
        "  --output experiments/results/poh_trm_latent_35m --seed 123\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- TRM reference run mirrors the official recipe but on a single GPU (A100). For multi-GPU, replace the python call with torchrun (nproc-per-node=4).\n",
        "- Our PoH runner uses adaptive halting and latent recursion. Start with puzzle-emb weight_decay=0.0; ablate 0.1 and 1.0 if you want strict TRM regularization.\n",
        "- Source: TinyRecursiveModels README (Maze-Hard): https://github.com/SamsungSAILMontreal/TinyRecursiveModels\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
